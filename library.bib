Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{dong2020exploring,
author = {Dong, Jiayun and Rudin, Cynthia},
file = {:Users/hikaruasano/Documents/mendeley/Dong, Rudin{\_}2020{\_}Exploring the cloud of variable importance for the set of all good models.pdf:pdf;:Users/hikaruasano/Documents/mendeley/Dong, Rudin{\_}2020{\_}Exploring the cloud of variable importance for the set of all good models.pdf:pdf},
journal = {Nature Machine Intelligence},
number = {12},
pages = {810--824},
publisher = {Nature Publishing Group},
title = {{Exploring the cloud of variable importance for the set of all good models}},
volume = {2},
year = {2020}
}
@inproceedings{Jaques2019SocialIA,
author = {Jaques, Natasha and Lazaridou, Angeliki and Hughes, Edward and G{\"{u}}l{\c{c}}ehre, {\c{C}}aglar and Ortega, Pedro A and Strouse, D J and Leibo, Joel Z and de Freitas, Nando},
booktitle = {ICML},
file = {:Users/hikaruasano/Documents/mendeley/Jaques et al.{\_}2019{\_}Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning.pdf:pdf},
title = {{Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning}},
year = {2019}
}
@inproceedings{kim2021communication,
author = {Kim, Woojun and Park, Jongeui and Sung, Youngchul},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Kim, Park, Sung{\_}2021{\_}Communication in Multi-Agent Reinforcement Learning Intention Sharing.pdf:pdf},
title = {{Communication in Multi-Agent Reinforcement Learning: Intention Sharing}},
url = {https://openreview.net/forum?id=qpsl2dR9twy},
year = {2021}
}
@inproceedings{10.1145/3447548.3467420,
abstract = {Centralized Training with Decentralized Execution (CTDE) has been a popular paradigm in cooperative Multi-Agent Reinforcement Learning (MARL) settings and is widely used in many real applications. One of the major challenges in the training process is credit assignment, which aims to deduce the contributions of each agent according to the global rewards. Existing credit assignment methods focus on either decomposing the joint value function into individual value functions or measuring the impact of local observations and actions on the global value function. These approaches lack a thorough consideration of the complicated interactions among multiple agents, leading to an unsuitable assignment of credit and subsequently mediocre results on MARL. We propose Shapley Counterfactual Credit Assignment, a novel method for explicit credit assignment which accounts for the coalition of agents. Specifically, Shapley Value and its desired properties are leveraged in deep MARL to credit any combinations of agents, which grants us the capability to estimate the individual credit for each agent. Despite this capability, the main technical difficulty lies in the computational complexity of Shapley Value who grows factorially as the number of agents. We instead utilize an approximation method via Monte Carlo sampling, which reduces the sample complexity while maintaining its effectiveness. We evaluate our method on StarCraft II benchmarks across different scenarios. Our method outperforms existing cooperative MARL algorithms significantly and achieves the state-of-the-art, with especially large margins on tasks with more severe difficulties.},
address = {New York, NY, USA},
author = {Li, Jiahui and Kuang, Kun and Wang, Baoxiang and Liu, Furui and Chen, Long and Wu, Fei and Xiao, Jun},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery {\&} Data Mining},
doi = {10.1145/3447548.3467420},
file = {:Users/hikaruasano/Documents/mendeley/Li et al.{\_}2021{\_}Shapley Counterfactual Credits for Multi-Agent Reinforcement Learning.pdf:pdf},
isbn = {9781450383325},
keywords = {counterfactual thinking,credit assignment,multi-agent systems,reinforcement learning,shapley value},
pages = {934--942},
publisher = {Association for Computing Machinery},
series = {KDD '21},
title = {{Shapley Counterfactual Credits for Multi-Agent Reinforcement Learning}},
url = {https://doi.org/10.1145/3447548.3467420},
year = {2021}
}
@article{Chaslot2008,
abstract = {Classic approaches to game AI require either a high quality of domain knowledge, or a long time to generate effective AI behaviour. These two characteristics hamper the goal of establishing challenging game AI. In this paper, we put forward Monte-Carlo Tree Search as a novel, uniﬁed framework to game AI. In the framework, randomized explorations of the search space are used to predict the most promising game actions. We will demonstrate that Monte-Carlo Tree Search can be applied effectively to (1) classic board-games, (2) modern board-games, and (3) video games.},
author = {Chaslot, Guillaume and Bakkes, Sander and Szitai, Istvan and Spronck, Pieter},
file = {:Users/hikaruasano/Documents/mendeley/Chaslot et al.{\_}2008{\_}Monte-carlo tree search A New Framework for Game AI1.pdf:pdf},
issn = {15687805},
journal = {Belgian/Netherlands Artificial Intelligence Conference},
keywords = {Demonstration Papers},
pages = {389--390},
title = {{Monte-carlo tree search: A New Framework for Game AI1}},
year = {2008}
}
@article{2020,
author = {斎藤, 幸平},
isbn = {9784087211351},
number = {1035A},
publisher = {集英社},
title = {人新世の「資本論」},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2003528061},
year = {2020}
}
@inproceedings{hernandez2016identifying,
author = {Hernandez-Leal, Pablo and Taylor, Matthew E and Rosman, Benjamin and Sucar, L Enrique and {De Cote}, Enrique Munoz},
booktitle = {Workshops at the Thirtieth AAAI Conference on Artificial Intelligence},
file = {:Users/hikaruasano/Documents/mendeley/Hernandez-Leal et al.{\_}2016{\_}Identifying and tracking switching, non-stationary opponents A Bayesian approach.pdf:pdf},
title = {{Identifying and tracking switching, non-stationary opponents: A Bayesian approach}},
year = {2016}
}
@misc{環境省平成２２年14:online,
annote = {(Accessed on 07/13/2020)},
author = {環境省},
howpublished = {https://www.env.go.jp/earth/report/h23-03/},
title = {平成２２年度 再生可能エネルギー導入ポテンシャル調査調査報告書},
year = {2011}
}
@article{Mnih2016AsynchronousMF,
author = {Mnih, V and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, A and Lillicrap, T and Harley, Tim and Silver, D and Kavukcuoglu, K},
journal = {ArXiv},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
volume = {abs/1602.0},
year = {2016}
}
@inproceedings{perolat2017multi,
author = {Perolat, Julien and Leibo, Joel Z and Zambaldi, Vinicius and Beattie, Charles and Tuyls, Karl and Graepel, Thore},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/hikaruasano/Documents/mendeley/Perolat et al.{\_}2017{\_}A multi-agent reinforcement learning model of common-pool resource appropriation(2).pdf:pdf},
pages = {3643--3652},
title = {{A multi-agent reinforcement learning model of common-pool resource appropriation}},
year = {2017}
}
@misc{Apple、2033:online,
annote = {(Accessed on 01/12/2021)},
author = {Apple},
howpublished = {https://www.apple.com/jp/newsroom/2020/07/apple-commits-to-be-100-percent-carbon-neutral-for-its-supply-chain-and-products-by-2030/},
title = {{Apple、2030年までにサプライチェーンの 100％カーボンニュートラル達成を約束 - Apple (日本)}},
year = {2020}
}
@inproceedings{pmlr-v80-kearns18a,
abstract = {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning — which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.},
author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
editor = {Dy, Jennifer and Krause, Andreas},
file = {:Users/hikaruasano/Documents/mendeley/Kearns et al.{\_}2018{\_}Preventing Fairness Gerrymandering Auditing and Learning for Subgroup Fairness.pdf:pdf},
pages = {2564--2572},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness}},
url = {https://proceedings.mlr.press/v80/kearns18a.html},
volume = {80},
year = {2018}
}
@article{bisk2020experience,
author = {Bisk, Yonatan and Holtzman, Ari and Thomason, Jesse and Andreas, Jacob and Bengio, Yoshua and Chai, Joyce and Lapata, Mirella and Lazaridou, Angeliki and May, Jonathan and Nisnevich, Aleksandr and Others},
file = {:Users/hikaruasano/Documents/mendeley/Bisk et al.{\_}2020{\_}Experience grounds language.pdf:pdf},
journal = {arXiv preprint arXiv:2004.10151},
title = {{Experience grounds language}},
year = {2020}
}
@article{li2011knows,
author = {Li, Lihong and Littman, Michael L and Walsh, Thomas J and Strehl, Alexander L},
journal = {Machine learning},
number = {3},
pages = {399--443},
publisher = {Springer},
title = {{Knows what it knows: a framework for self-aware learning}},
volume = {82},
year = {2011}
}
@article{9491972,
author = {Li, Jiachen and Ma, Hengbo and Zhang, Zhihao and Li, Jinning and Tomizuka, Masayoshi},
doi = {10.1109/TITS.2021.3094821},
file = {:Users/hikaruasano/Documents/mendeley/Li et al.{\_}2021{\_}Spatio-Temporal Graph Dual-Attention Network for Multi-Agent Prediction and Tracking.pdf:pdf},
journal = {IEEE Transactions on Intelligent Transportation Systems},
pages = {1--14},
title = {{Spatio-Temporal Graph Dual-Attention Network for Multi-Agent Prediction and Tracking}},
year = {2021}
}
@inproceedings{10.5555/3306127.3331816,
abstract = {We study the offline Multi-Agent Pickup-and-Delivery (MAPD) problem, where a team of agents has to execute a batch of tasks with release times in a known environment. To execute a task, an agent has to move first from its current location to the pickup location of the task and then to the delivery location of the task. The MAPD problem is to assign tasks to agents and plan collision-free paths for them to execute their tasks. Online MAPD algorithms can be applied to the offline MAPD problem, but do not utilize all of the available information and may thus not be effective. Therefore, we present two novel offline MAPD algorithms that improve a state-of-the-art online MAPD algorithm with respect to task planning, path planning, and deadlock avoidance for the offline MAPD problem. Our MAPD algorithms first compute one task sequence for each agent by solving a special traveling salesman problem and then plan paths according to these task sequences. We also introduce an effective deadlock avoidance method, called "reserving dummy paths.'' Theoretically, our MAPD algorithms are complete for well-formed MAPD instances, a realistic subclass of all MAPD instances. Experimentally, they produce solutions of smaller makespans and scale better than the online MAPD algorithm in simulated warehouses with hundreds of robots and thousands of tasks.},
address = {Richland, SC},
author = {Liu, Minghua and Ma, Hang and Li, Jiaoyang and Koenig, Sven},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
isbn = {9781450363099},
keywords = {agent coordination,multi-agent path finding,path planning,pickup and delivery task,task assignment,traveling salesman problem},
pages = {1152--1160},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
series = {AAMAS '19},
title = {{Task and Path Planning for Multi-Agent Pickup and Delivery}},
year = {2019}
}
@book{BC04780001,
author = {陽一郎, 三宅},
pages = {p.32},
publisher = {PLANETS/第二次惑星開発委員会},
title = {人工知能が「生命」になるとき},
url = {https://ci.nii.ac.jp/ncid/BC04780001},
year = {2020}
}
@misc{monthlyR92:online,
annote = {(Accessed on 06/28/2020)},
author = {墨田区窓口課},
howpublished = {$\backslash$url{\{}https://www.city.sumida.lg.jp/kuseijoho/sumida{\_}info/population/monthly/ta301000R0204.files/monthlyR2nendo06.pdf{\}}},
title = {墨田区世帯人口現状}
}
@incollection{wolpert2002optimal,
author = {Wolpert, David H and Tumer, Kagan},
booktitle = {Modeling complexity in economic and social systems},
file = {:Users/hikaruasano/Documents/mendeley/Wolpert, Tumer{\_}2002{\_}Optimal payoff functions for members of collectives.pdf:pdf},
pages = {355--369},
publisher = {World Scientific},
title = {{Optimal payoff functions for members of collectives}},
year = {2002}
}
@article{Arrieta2020ExplainableAI,
author = {Arrieta, A and D'iaz-Rodr'iguez, Natalia and Ser, J and Bennetot, Adrien and Tabik, S and Barbado, A and Garc'ia, Salvador and Gil-L'opez, Sergio and Molina, D and Benjamins, Richard and Chatila, R and Herrera, F},
file = {:Users/hikaruasano/Documents/mendeley/Arrieta et al.{\_}2020{\_}Explainable Artificial Intelligence (XAI) Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.pdf:pdf},
journal = {ArXiv},
title = {{Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI}},
volume = {abs/1910.1},
year = {2020}
}
@book{1020000782078828673,
author = {俊樹, 佐藤},
publisher = {その歴史と構造、ミネルヴァ書房},
title = {社会学の方法},
url = {https://cir.nii.ac.jp/crid/1020000782078828673},
year = {2011}
}
@book{epic337530,
address = {Geneva, Switzerland},
author = {Pachauri, R K and Allen, M R and Barros, V R and Broome, J and Cramer, W and Christ, R and Church, J A and Clarke, L and Dahe, Q and Dasgupta, P and Dubash, N K and Edenhofer, O and Elgizouli, I and Field, C B and Forster, P and Friedlingstein, P and Fuglestvedt, J and Gomez-Echeverri, L and Hallegatte, S and Hegerl, G and Howden, M and Jiang, K and Cisneroz, B Jimenez and Kattsov, V and Lee, H and Mach, K J and Marotzke, J and Mastrandrea, M D and Meyer, L and Minx, J and Mulugetta, Y and O'Brien, K and Oppenheimer, M and Pereira, J J and Pichs-Madruga, R and Plattner, G.-K. and P{\"{o}}rtner, Hans-Otto and Power, S B and Preston, B and Ravindranath, N H and Reisinger, A and Riahi, K and Rusticucci, M and Scholes, R and Seyboth, K and Sokona, Y and Stavins, R and Stocker, T F and Tschakert, P and van Vuuren, D and van Ypserle, J.-P.},
editor = {Pachauri, R K and Meyer, L},
pages = {p.17},
publisher = {IPCC},
title = {{Climate Change 2014: Synthesis Report. Contribution of Working Groups I, II and III to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change}},
year = {2014}
}
@book{BB28214872,
author = {哲郎, 森村 and 講談社サイエンティフィク},
pages = {p.1},
publisher = {講談社},
series = {MLP機械学習プロフェッショナルシリーズ},
title = {強化学習},
url = {https://ci.nii.ac.jp/ncid/BB28214872},
year = {2019}
}
@inproceedings{cui2022when,
author = {Cui, Qiwen and Du, Simon Shaolei},
booktitle = {ICLR 2022 Workshop on Gamification and Multiagent Solutions},
title = {{When is Offline Two-Player Zero-Sum Markov Game Solvable?}},
url = {https://openreview.net/forum?id=BMuvi91Tec},
year = {2022}
}
@misc{「ポスト真実」が32:online,
annote = {(Accessed on 07/09/2021)},
author = {BBCニュース},
howpublished = {https://www.bbc.com/japanese/38009790（2021年7月9日）},
title = {{「ポスト真実」が今年の言葉 英オックスフォード辞書 - BBCニュース}},
year = {2016}
}
@book{santner2003design,
author = {Santner, Thomas J and Williams, Brian J and Notz, William I and Williams, Brain J},
publisher = {Springer},
title = {{The design and analysis of computer experiments}},
volume = {1},
year = {2003}
}
@inproceedings{Kleinegesse2020BayesianED,
author = {Kleinegesse, S and Gutmann, M},
booktitle = {ICML},
file = {:Users/hikaruasano/Documents/mendeley/Kleinegesse, Gutmann{\_}2020{\_}Bayesian Experimental Design for Implicit Models by Mutual Information Neural Estimation.pdf:pdf},
title = {{Bayesian Experimental Design for Implicit Models by Mutual Information Neural Estimation}},
year = {2020}
}
@inproceedings{10.5555/3091125.3091194,
abstract = {Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades.
However, they necessarily treat the choice to cooperate or defect as an atomic action.
In real-world social dilemmas these choices are temporally extended. Cooperativeness
is a property that applies to policies, not elementary actions. We introduce sequential
social dilemmas that share the mixed incentive structure of matrix game social dilemmas
but also require agents to learn policies that implement their strategic intentions.
We analyze the dynamics of policies learned by multiple self-interested independent
learning agents, each using its own deep Q-network, on two Markov games we introduce
here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how
learned behavior in each domain changes as a function of environmental factors including
resource abundance. Our experiments show how conflict can emerge from competition
over shared resources and shed light on how the sequential nature of real world social
dilemmas affects cooperation.},
address = {Richland, SC},
author = {Leibo, Joel Z and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
keywords = {agent-based social simulation,cooperation,markov games,non-cooperative games,social dilemmas},
pages = {464--473},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
series = {AAMAS '17},
title = {{Multi-Agent Reinforcement Learning in Sequential Social Dilemmas}},
year = {2017}
}
@inproceedings{kim2022influencing,
author = {Kim, Dong-Ki and Riemer, Matthew D and Liu, Miao and Foerster, Jakob Nicolaus and Everett, Michael and Sun, Chuangchuang and Tesauro, Gerald and HOW, JONATHAN P},
booktitle = {ICLR 2022 Workshop on Gamification and Multiagent Solutions},
file = {:Users/hikaruasano/Documents/mendeley/Kim et al.{\_}2022{\_}Influencing Long-Term Behavior in Multiagent Reinforcement Learning.pdf:pdf},
title = {{Influencing Long-Term Behavior in Multiagent Reinforcement Learning}},
url = {https://openreview.net/forum?id=SWzdiq1Tlc},
year = {2022}
}
@article{Norman1994,
abstract = {We define the relevant information in a signal {\$}x\backslashin X{\$} as being the information that this signal provides about another signal {\$}y\backslashin \backslashY{\$}. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal {\$}x{\$} requires more than just predicting {\$}y{\$}, it also requires specifying which features of {\$}\backslashX{\$} play a role in the prediction. We formalize this problem as that of finding a short code for {\$}\backslashX{\$} that preserves the maximum information about {\$}\backslashY{\$}. That is, we squeeze the information that {\$}\backslashX{\$} provides about {\$}\backslashY{\$} through a `bottleneck' formed by a limited set of codewords {\$}\backslashtX{\$}. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure {\$}d(x,\backslashx){\$} emerges from the joint statistics of {\$}\backslashX{\$} and {\$}\backslashY{\$}. This approach yields an exact set of self consistent equations for the coding rules {\$}X \backslashto \backslashtX{\$} and {\$}\backslashtX \backslashto \backslashY{\$}. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
author = {Norman, Sandy},
doi = {10.1108/eb040537},
file = {:Users/hikaruasano/Documents/mendeley/Norman{\_}1994{\_}The information bottleneck — electronic copyright issues.pdf:pdf},
issn = {03055728},
journal = {Vine},
number = {3},
pages = {3--6},
title = {{The information bottleneck — electronic copyright issues}},
volume = {24},
year = {1994}
}
@inproceedings{hausknecht2015deep,
author = {Hausknecht, Matthew and Stone, Peter},
booktitle = {2015 AAAI Fall Symposium Series},
file = {:Users/hikaruasano/Documents/mendeley/Hausknecht, Stone{\_}2015{\_}Deep recurrent q-learning for partially observable mdps.pdf:pdf},
title = {{Deep recurrent q-learning for partially observable mdps}},
year = {2015}
}
@article{mcfarland2016sociology,
author = {McFarland, Daniel A and Lewis, Kevin and Goldberg, Amir},
file = {:Users/hikaruasano/Documents/mendeley/McFarland, Lewis, Goldberg{\_}2016{\_}Sociology in the era of big data The ascent of forensic social science(2).pdf:pdf},
journal = {The American Sociologist},
number = {1},
pages = {12--35},
publisher = {Springer},
title = {{Sociology in the era of big data: The ascent of forensic social science}},
volume = {47},
year = {2016}
}
@article{borgatti2006identifying,
author = {Borgatti, Stephen P},
file = {:Users/hikaruasano/Documents/mendeley/Borgatti{\_}2006{\_}Identifying sets of key players in a social network.pdf:pdf},
journal = {Computational {\&} Mathematical Organization Theory},
number = {1},
pages = {21--34},
publisher = {Springer},
title = {{Identifying sets of key players in a social network}},
volume = {12},
year = {2006}
}
@article{1050001338864366976,
author = {田上, 大輔 and 佐々木, 啓},
file = {:Users/hikaruasano/Documents/mendeley/田上, 佐々木{\_}2015{\_}規範理論と秩序問題 社会学における規範的問いと経験的問いに関する一考察.pdf:pdf},
issn = {1349-2276},
journal = {東洋大学人間科学総合研究所紀要},
pages = {75--90},
publisher = {東洋大学人間科学総合研究所},
title = {規範理論と秩序問題 : 社会学における規範的問いと経験的問いに関する一考察},
url = {https://cir.nii.ac.jp/crid/1050001338864366976},
volume = {17},
year = {2015}
}
@article{beck2020multilevel,
author = {Beck, Joakim and {Mansour Dia}, Ben and Espath, Luis and Tempone, Ra{\'{u}}l},
journal = {International Journal for Numerical Methods in Engineering},
number = {15},
pages = {3482--3503},
publisher = {Wiley Online Library},
title = {{Multilevel double loop Monte Carlo and stochastic collocation methods with importance sampling for Bayesian optimal experimental design}},
volume = {121},
year = {2020}
}
@inproceedings{Shababo2013BayesianIA,
author = {Shababo, B and Paige, Brooks and Pakman, Ari and Paninski, L},
booktitle = {NIPS},
title = {{Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits}},
year = {2013}
}
@book{Unknownb,
author = {講義資料},
pages = {53},
title = {第一回，53p}
}
@inproceedings{Saha2022,
author = {Saha, Avishkar and Mendez, Oscar and Russell, Chris and Bowden, Richard},
booktitle = {ICRA 2022},
file = {:Users/hikaruasano/Documents/mendeley/Saha et al.{\_}2022{\_}Translating images into maps.pdf:pdf},
title = {{Translating images into maps}},
url = {https://www.amazon.science/publications/translating-images-into-maps},
year = {2022}
}
@article{Hu2020SampleCO,
author = {Hu, Y and Chen, Xin and He, Niao},
file = {:Users/hikaruasano/Documents/mendeley/Hu, Chen, He{\_}2020{\_}Sample Complexity of Sample Average Approximation for Conditional Stochastic Optimization.pdf:pdf},
journal = {SIAM J. Optim.},
pages = {2103--2133},
title = {{Sample Complexity of Sample Average Approximation for Conditional Stochastic Optimization}},
volume = {30},
year = {2020}
}
@article{florensa2017stochastic,
author = {Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
file = {:Users/hikaruasano/Documents/mendeley/Florensa, Duan, Abbeel{\_}2017{\_}Stochastic neural networks for hierarchical reinforcement learning.pdf:pdf},
journal = {arXiv preprint arXiv:1704.03012},
title = {{Stochastic neural networks for hierarchical reinforcement learning}},
year = {2017}
}
@inproceedings{0001WSZ21,
author = {Zhang, Weinan and Wang, Xihuai and Shen, Jian and Zhou, Ming},
booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2021/466},
editor = {Zhou, Zhi-Hua},
file = {:Users/hikaruasano/Documents/mendeley/Zhang et al.{\_}2021{\_}Model-based Multi-agent Policy Optimization with Adaptive Opponent-wise Rollouts.pdf:pdf},
pages = {3384--3391},
publisher = {ijcai.org},
title = {{Model-based Multi-agent Policy Optimization with Adaptive Opponent-wise Rollouts}},
url = {https://doi.org/10.24963/ijcai.2021/466},
year = {2021}
}
@article{hoekstra2017creating,
author = {Hoekstra, Auke and Steinbuch, Maarten and Verbong, Geert},
journal = {Complexity},
publisher = {Hindawi},
title = {{Creating agent-based energy transition management models that can uncover profitable pathways to climate change mitigation}},
volume = {2017},
year = {2017}
}
@article{yu2015emotional,
author = {Yu, Chao and Zhang, Minjie and Ren, Fenghui and Tan, Guozhen},
file = {:Users/hikaruasano/Documents/mendeley/Yu et al.{\_}2015{\_}Emotional multiagent reinforcement learning in spatial social dilemmas.pdf:pdf},
journal = {IEEE transactions on neural networks and learning systems},
number = {12},
pages = {3083--3096},
publisher = {IEEE},
title = {{Emotional multiagent reinforcement learning in spatial social dilemmas}},
volume = {26},
year = {2015}
}
@book{king2021designing,
author = {King, Gary and Keohane, Robert O and Verba, Sidney},
publisher = {Princeton university press},
title = {{Designing social inquiry: Scientific inference in qualitative research}},
year = {1994}
}
@article{保坂征宏2008温暖化シミュレーションにみる雪氷と気候,
author = {保坂征宏},
journal = {天気},
number = {7},
pages = {560--564},
publisher = {日本気象学会},
title = {温暖化シミュレーションにみる雪氷と気候},
volume = {55},
year = {2008}
}
@inproceedings{46570,
address = {Brisbane, Australia},
author = {Faust, Aleksandra and Ramirez, Oscar and Fiser, Marek and Oslund, Ken and Francis, Anthony and Davidson, James and Tapia, Lydia},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {:Users/hikaruasano/Documents/mendeley/Faust et al.{\_}2018{\_}PRM-RL Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning.pdf:pdf},
pages = {5113--5120},
title = {{PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning}},
url = {https://arxiv.org/abs/1710.03937},
year = {2018}
}
@inproceedings{devlin2011theoretical,
author = {Devlin, Sam and Kudenko, Daniel},
booktitle = {The 10th International Conference on Autonomous Agents and Multiagent Systems},
file = {:Users/hikaruasano/Documents/mendeley/Devlin, Kudenko{\_}2011{\_}Theoretical considerations of potential-based reward shaping for multi-agent systems.pdf:pdf},
organization = {ACM},
pages = {225--232},
title = {{Theoretical considerations of potential-based reward shaping for multi-agent systems}},
year = {2011}
}
@inproceedings{10.1145/3278721.3278722,
abstract = {We introduce a novel technique to achieve non-discrimination in machine learning without sacrificing convexity and probabilistic interpretation. We also propose a new notion of fairness for machine learning called the weighted proportional fairness and show that our technique satisfies this subjective fairness criterion.},
address = {New York, NY, USA},
author = {Goel, Naman and Yaghini, Mohammad and Faltings, Boi},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
doi = {10.1145/3278721.3278722},
file = {:Users/hikaruasano/Documents/mendeley/Goel, Yaghini, Faltings{\_}2018{\_}Non-Discriminatory Machine Learning through Convex Fairness Criteria.pdf:pdf},
isbn = {9781450360128},
keywords = {machine learning,non-discrimination,proportional fairness},
pages = {116},
publisher = {Association for Computing Machinery},
series = {AIES '18},
title = {{Non-Discriminatory Machine Learning through Convex Fairness Criteria}},
url = {https://doi.org/10.1145/3278721.3278722},
year = {2018}
}
@misc{平成30年住宅・92:online,
annote = {(Accessed on 06/28/2020)},
author = {総務省統計局},
howpublished = {https://www.stat.go.jp/data/jyutaku/2018/pdf/yougo.pdf},
title = {平成30年住宅・土地統計調査 用語の解説},
year = {2019}
}
@article{Silver2018,
abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
doi = {10.1126/science.aar6404},
file = {:Users/hikaruasano/Documents/mendeley/Silver et al.{\_}2018{\_}A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.pdf:pdf},
issn = {10959203},
journal = {Science},
number = {6419},
pages = {1140--1144},
title = {{A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play}},
volume = {362},
year = {2018}
}
@inproceedings{silver2013concurrent,
author = {Silver, David and Newnham, Leonard and Barker, David and Weller, Suzanne and McFall, Jason},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Silver et al.{\_}2013{\_}Concurrent reinforcement learning from customer interactions.pdf:pdf},
pages = {924--932},
title = {{Concurrent reinforcement learning from customer interactions}},
year = {2013}
}
@article{tang2018hierarchical,
author = {Tang, Hongyao and Hao, Jianye and Lv, Tangjie and Chen, Yingfeng and Zhang, Zongzhang and Jia, Hangtian and Ren, Chunxu and Zheng, Yan and Meng, Zhaopeng and Fan, Changjie and Others},
file = {:Users/hikaruasano/Documents/mendeley//Tang et al.{\_}2018{\_}Hierarchical deep multiagent reinforcement learning with temporal abstraction.pdf:pdf},
journal = {arXiv preprint arXiv:1809.09332},
title = {{Hierarchical deep multiagent reinforcement learning with temporal abstraction}},
year = {2018}
}
@article{Wolfson1998,
abstract = {Sleep and waking behaviors change significantly during the adolescent years. The objective of this study was to describe the relation between adolescents' sleep/wake habits, characteristics of students (age, sex, school), and daytime functioning (mood, school performance, and behavior). A Sleep Habits Survey was administered in homeroom classes to 3,120 high school students at 4 public high schools from 3 Rhode Island school districts. Self-reported total sleep times (school and weekend nights) decreased by 40-50 min across ages 13-19, ps {\textless} .001. The sleep loss was due to increasingly later bedtimes, whereas rise times were more consistent across ages. Students who described themselves as struggling or failing school (C's, D's/F's) reported that on school nights they obtain about 25 min less sleep and go to bed an average of 40 min later than A and B students, ps {\textless} .001. In addition, students with worse grades reported greater weekend delays of sleep schedule than did those with better grades. Furthermore, this study examined a priori defined adequate sleep habit groups versus less than adequate sleep habit groups on their daytime functioning. Students in the short school-night total sleep group ({\textless}6 hr 45 min) and/or large weekend bedtime delay group ({\textgreater}120 min) reported increased daytime sleepiness, depressive mood, and sleep/wake behavior problems, ps {\textless} .05, versus those sleeping longer than 8 hr 15 min with less than 60 min weekend delay. Altogether, most of the adolescents' surveyed do not get enough sleep, and their sleep loss interferes with daytime functioning.},
author = {Wolfson, Amy R. and Carskadon, Mary A.},
doi = {10.1111/j.1467-8624.1998.tb06149.x},
issn = {00093920},
journal = {Child Development},
number = {4},
pages = {875--887},
pmid = {9768476},
publisher = {Blackwell Publishing Inc.},
title = {{Sleep Schedules and Daytime Functioning in Adolescents}},
volume = {69},
year = {1998}
}
@inproceedings{chandak2019learning,
author = {Chandak, Yash and Theocharous, Georgios and Kostas, James and Jordan, Scott and Thomas, Philip},
booktitle = {International conference on machine learning},
file = {:Users/hikaruasano/Documents/mendeley/Chandak et al.{\_}2019{\_}Learning action representations for reinforcement learning.pdf:pdf},
organization = {PMLR},
pages = {941--950},
title = {{Learning action representations for reinforcement learning}},
year = {2019}
}
@inproceedings{cen2022regret,
author = {Cen, Sarah H and Shah, Devavrat},
booktitle = {International Conference on Artificial Intelligence and Statistics},
file = {:Users/hikaruasano/Documents/mendeley/Cen, Shah{\_}2022{\_}Regret, stability {\&} fairness in matching markets with bandit learners.pdf:pdf},
organization = {PMLR},
pages = {8938--8968},
title = {{Regret, stability {\&} fairness in matching markets with bandit learners}},
year = {2022}
}
@article{levine2020offline,
author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
journal = {arXiv preprint arXiv:2005.01643},
title = {{Offline reinforcement learning: Tutorial, review, and perspectives on open problems}},
year = {2020}
}
@article{2007a,
author = {川俣 and 雅弘},
issn = {00266760},
journal = {三田学会雑誌},
keywords = {パレート,パレートパラドックス,完全分配定理,序数主義,新厚生経済学},
number = {4},
pages = {657--679},
publisher = {慶応義塾経済学会},
title = {パレートの『経済学提要』と20世紀ミクロ経済学の展開 (小特集 ヴィレフレード・パレート『経済学提要』刊行100年)},
url = {http://ci.nii.ac.jp/naid/40015405977/ja/},
volume = {99},
year = {2007}
}
@article{9743365,
author = {Li, Jian-Yu and Zhan, Zhi-Hui and Tan, Kay Chen and Zhang, Jun},
doi = {10.1109/TCYB.2022.3158391},
file = {:Users/hikaruasano/Documents/mendeley/Li et al.{\_}2022{\_}Dual Differential Grouping A More General Decomposition Method for Large-Scale Optimization.pdf:pdf},
journal = {IEEE Transactions on Cybernetics},
pages = {1--15},
title = {{Dual Differential Grouping: A More General Decomposition Method for Large-Scale Optimization}},
year = {2022}
}
@misc{AboutGid40:online,
annote = {(Accessed on 07/15/2020)},
howpublished = {$\backslash$url{\{}https://www.gidas.org/en/about-gidas/ueberblick-ueber-gidas/{\}}},
title = {{About Gidas | GIDAS}}
}
@misc{foster2021deep,
archivePrefix = {arXiv},
arxivId = {stat.ML/2103.02438},
author = {Foster, Adam and Ivanova, Desi R and Malik, Ilyas and Rainforth, Tom},
eprint = {2103.02438},
file = {:Users/hikaruasano/Documents/mendeley/Foster et al.{\_}2021{\_}Deep Adaptive Design Amortizing Sequential Bayesian Experimental Design.pdf:pdf},
primaryClass = {stat.ML},
title = {{Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design}},
year = {2021}
}
@article{hofman2021integrating,
author = {Hofman, Jake M and Watts, Duncan J and Athey, Susan and Garip, Filiz and Griffiths, Thomas L and Kleinberg, Jon and Margetts, Helen and Mullainathan, Sendhil and Salganik, Matthew J and Vazire, Simine and Others},
file = {:Users/hikaruasano/Documents/mendeley/Hofman et al.{\_}2021{\_}Integrating explanation and prediction in computational social science(2).pdf:pdf},
journal = {Nature},
number = {7866},
pages = {181--188},
publisher = {Nature Publishing Group},
title = {{Integrating explanation and prediction in computational social science}},
volume = {595},
year = {2021}
}
@techreport{2006a,
abstract = {因果ネットワークは，社会的事象の因果関係を系統的かつ視覚的に把握するためのツールとして，さま ざまな分野で用いられている．しかし，多くの場合において因果ネットワークの作成は分析者による多大 な解釈的作業を必要とし，その構築には多くの時間を要する．本研究では，Web 上にある膨大な文書デー タを利用して因果ネットワークを自動的に構築するツールを開発し，その活用方法を具体的事例と共に示 した．本手法を活用することにより，日常の政策論議の信頼性を，既存の因果的言説を通して検証するこ とが可能になる． キーワード：因果ネットワーク 推論システム 自然言語処理 政策論議 1. 本研究の背景・目的 社会問題の解決には様々な事象間の因果関係を明らか にすることがしばしば重要となる．しかし多くの事象が 複雑に絡み合って起きる社会現象の全体像を把握するこ とはきわめて難しい．各事象間の因果関係を実証するこ とはもちろん，どのような要素的事象が着目している社 会問題に影響を及ぼしていそうかさえ容易に判断できな いことも多い． 因果ネットワーク(Causal Networks)は， 諸事象間の因果 関係を有向グラフとして表すことによってその全体像を 可視化する手法である．因果ネットワークによって，あ る主題に関する様々な因果関係を系統的に把握すること ができる．また，社会的に受容できない事象を防ぐため にはどのような事象を制御すればよいのかといった，問 題解決型のアプローチを採用する際にも因果ネットワー クは有用である． しかし因果ネットワークの作成は多くの場合において 分析者の膨大な解釈的作業が必要となる．また，一般に ひとつの事象を取り巻く因果関係は非常に複雑であり， 因果ネットワークを形成する要素事象は膨大な数に上る． そのため，日々生じる種々の社会問題に関する因果ネッ トワークを手作業で短期間に構築することは非常に困難 である． 本研究では，以上の問題を解決するために web 上にあ る膨大な文書データを活用して因果ネットワークを自動 的に構築するツールを開発した．このツールは，文書の 記述から因果関係のデータを自動的に抽出する因果関係 自動抽出器と， そのデータを基にネットワークを構築し， それを可視化するとともに，様々な事象の生起を数値的 に表す因果ネットワークのツールによって構成される． 2, 3 章でそのアルゴリズムについて述べ， 4 章ではこの ツールの活用法を実装例と共に示す．最後に 5 章で結論 を述べる． 2. 因果関係自動抽出器 因果関係の推論の蓋然性を評価するためには，判断材 料となる知識が必要となる．因果ネットワークにおいて その知識は，原因から結果に至る連鎖の間に存在する， 中間事象として表現できる． 本研究では，佐藤ら 1) の手法を基に，web上にある文書 データから，所与の原因と結果の間に位置する中間事象 を自動的に抽出するシステムを開発した． 2.1. アルゴリズム 因果関係自動抽出器のアルゴリズムは以下のとおり である． 66},
author = {岳文, 佐藤 and 昌英, ・堀田},
file = {:Users/hikaruasano/Documents/mendeley/岳文, 昌英{\_}2006{\_}ASSESSING THE PLAUSIBILITY OF INFERENCE BASED ON AUTOMATED CONSTRUCTION OF CAUSAL NETWORKS USING WEB-MINING.pdf:pdf},
pages = {66--74},
title = {{ASSESSING THE PLAUSIBILITY OF INFERENCE BASED ON AUTOMATED CONSTRUCTION OF CAUSAL NETWORKS USING WEB-MINING}},
volume = {4},
year = {2006}
}
@inproceedings{littman2001friend,
author = {Littman, Michael L},
booktitle = {ICML},
pages = {322--328},
title = {{Friend-or-foe Q-learning in general-sum games}},
volume = {1},
year = {2001}
}
@inproceedings{10.5555/3398761.3398926,
abstract = {This work exploits action equivariance for representation learning in reinforcement learning. Equivariance under actions states that transitions in the input space are mirrored by equivalent transitions in latent space, while the map and transition functions should also commute. We introduce a contrastive loss function that enforces action equivariance on the learned representations. We prove that when our loss is zero, we have a homomorphism of a deterministic Markov Decision Process (MDP). Learning equivariant maps leads to structured latent spaces, allowing us to build a model on which we plan through value iteration. We show experimentally that for deterministic MDPs, the optimal policy in the abstract MDP can be successfully lifted to the original MDP. Moreover, the approach easily adapts to changes in the goal states. Empirically, we show that in such MDPs, we obtain better representations in fewer epochs compared to representation learning approaches using reconstructions, while generalizing better to new goals than model-free approaches.},
address = {Richland, SC},
author = {van der Pol, Elise and Kipf, Thomas and Oliehoek, Frans A and Welling, Max},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
file = {:Users/hikaruasano/Documents/mendeley/van der Pol et al.{\_}2020{\_}Plannable Approximations to MDP Homomorphisms Equivariance under Actions.pdf:pdf},
isbn = {9781450375184},
keywords = {equivariance,mdp homomorphisms,mdps,planning},
pages = {1431--1439},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
series = {AAMAS '20},
title = {{Plannable Approximations to MDP Homomorphisms: Equivariance under Actions}},
year = {2020}
}
@article{Mnih2015HumanlevelCT,
author = {Mnih, V and Kavukcuoglu, K and Silver, D and Rusu, Andrei A and Veness, J and Bellemare, Marc G and Graves, A and Riedmiller, Martin A and Fidjeland, A and Ostrovski, Georg and Petersen, Stig and Beattie, Charlie and Sadik, A and Antonoglou, Ioannis and King, Helen and Kumaran, D and Wierstra, Daan and Legg, S and Hassabis, D},
journal = {Nature},
pages = {529--533},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}
@inproceedings{Buolamwini2018GenderSI,
author = {Buolamwini, Joy and Gebru, Timnit},
booktitle = {FAT},
title = {{Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification}},
year = {2018}
}
@article{head2015extent,
author = {Head, Megan L and Holman, Luke and Lanfear, Rob and Kahn, Andrew T and Jennions, Michael D},
journal = {PLoS biology},
number = {3},
pages = {e1002106},
publisher = {Public Library of Science},
title = {{The extent and consequences of p-hacking in science}},
volume = {13},
year = {2015}
}
@book{BA79120757p53,
author = {学, 赤川},
pages = {p.53},
publisher = {勁草書房},
title = {構築主義を再構築する},
url = {https://ci.nii.ac.jp/ncid/BA79120757},
year = {2006}
}
@article{Giles2015MultilevelMC,
author = {Giles, M},
journal = {Acta Numerica},
pages = {259--328},
title = {{Multilevel Monte Carlo methods}},
volume = {24},
year = {2015}
}
@inproceedings{Hashimoto2018FairnessWD,
author = {Hashimoto, T and Srivastava, Megha and Namkoong, Hongseok and Liang, Percy},
booktitle = {ICML},
file = {:Users/hikaruasano/Documents/mendeley/Hashimoto et al.{\_}2018{\_}Fairness Without Demographics in Repeated Loss Minimization.pdf:pdf},
title = {{Fairness Without Demographics in Repeated Loss Minimization}},
year = {2018}
}
@article{doucet2009tutorial,
author = {Doucet, Arnaud and Johansen, Adam M},
file = {:Users/hikaruasano/Documents/mendeley/Doucet, Johansen{\_}2009{\_}A tutorial on particle filtering and smoothing Fifteen years later.pdf:pdf},
journal = {Handbook of nonlinear filtering},
number = {656-704},
pages = {3},
title = {{A tutorial on particle filtering and smoothing: Fifteen years later}},
volume = {12},
year = {2009}
}
@article{dawson2022safe,
author = {Dawson, Charles and Gao, Sicun and Fan, Chuchu},
file = {:Users/hikaruasano/Documents/mendeley/Dawson, Gao, Fan{\_}2022{\_}Safe Control with Learned Certificates A Survey of Neural Lyapunov, Barrier, and Contraction methods.pdf:pdf},
journal = {arXiv preprint arXiv:2202.11762},
title = {{Safe Control with Learned Certificates: A Survey of Neural Lyapunov, Barrier, and Contraction methods}},
year = {2022}
}
@article{Hagiwara2019,
abstract = {This study focuses on category formation for individual agents and the dynamics of symbol emergence in a multi-agent system through semiotic communication. In this study, the semiotic communication refers to exchanging signs composed of the signifier (i.e., words) and the signified (i.e., categories). We define the generation and interpretation of signs associated with the categories formed through the agent's own sensory experience or by exchanging signs with other agents as basic functions of the semiotic communication. From the viewpoint of language evolution and symbol emergence, organization of a symbol system in a multi-agent system (i.e., agent society) is considered as a bottom-up and dynamic process, where individual agents share the meaning of signs and categorize sensory experience. A constructive computational model can explain the mutual dependency of the two processes and has mathematical support that guarantees a symbol system's emergence and sharing within the multi-agent system. In this paper, we describe a new computational model that represents symbol emergence in a two-agent system based on a probabilistic generative model for multimodal categorization. It models semiotic communication via a probabilistic rejection based on the receiver's own belief. We have found that the dynamics by which cognitively independent agents create a symbol system through their semiotic communication can be regarded as the inference process of a hidden variable in an interpersonal multimodal categorizer, i.e., the complete system can be regarded as a single agent performing multimodal categorization using the sensors of all agents, if we define the rejection probability based on the Metropolis-Hastings algorithm. The validity of the proposed model and algorithm for symbol emergence, i.e., forming and sharing signs and categories, is also verified in an experiment with two agents observing daily objects in the real-world environment. In the experiment, we compared three communication algorithms: no communication, no rejection, and the proposed algorithm. The experimental results demonstrate that our model reproduces the phenomena of symbol emergence, which does not require a teacher who would know a pre-existing symbol system. Instead, the multi-agent system can form and use a symbol system without having pre-existing categories.},
author = {Hagiwara, Yoshinobu and Kobayashi, Hiroyoshi and Taniguchi, Akira and Taniguchi, Tadahiro},
doi = {10.3389/frobt.2019.00134},
file = {:Users/hikaruasano/Documents/mendeley/Hagiwara et al.{\_}2019{\_}Symbol Emergence as an Interpersonal Multimodal Categorization.pdf:pdf},
issn = {2296-9144},
journal = {Frontiers in Robotics and AI},
keywords = {language evolution,multiagent system,multimodal categorization,probabilistic generative model,semiotic communication,symbol emergence in robotics,symbol system},
month = {dec},
pages = {134},
publisher = {Frontiers Media S.A.},
title = {{Symbol Emergence as an Interpersonal Multimodal Categorization}},
url = {https://www.frontiersin.org/article/10.3389/frobt.2019.00134/full},
volume = {6},
year = {2019}
}
@inproceedings{rana2020multiplicative,
author = {Rana, Krishan and Dasagi, Vibhavari and Talbot, Ben and Milford, Michael and S{\"{u}}nderhauf, Niko},
booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
file = {:Users/hikaruasano/Documents/mendeley/Rana et al.{\_}2020{\_}Multiplicative controller fusion Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim.pdf:pdf},
organization = {IEEE},
pages = {6069--6076},
title = {{Multiplicative controller fusion: Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim-to-real transfer}},
year = {2020}
}
@inproceedings{pmlr-v80-liu18c,
abstract = {Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.},
author = {Liu, Lydia T and Dean, Sarah and Rolf, Esther and Simchowitz, Max and Hardt, Moritz},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
editor = {Dy, Jennifer and Krause, Andreas},
file = {:Users/hikaruasano/Documents/mendeley/Liu et al.{\_}2018{\_}Delayed Impact of Fair Machine Learning.pdf:pdf},
pages = {3150--3158},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Delayed Impact of Fair Machine Learning}},
url = {https://proceedings.mlr.press/v80/liu18c.html},
volume = {80},
year = {2018}
}
@inproceedings{10.5555/3016100.3016191,
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain
conditions. It was not previously known whether, in practice, such overestimations
are common, whether they harm performance, and whether they can generally be prevented.
In this paper, we answer all these questions affirmatively. In particular, we first
show that the recent DQN algorithm, which combines Q-learning with a deep neural network,
suffers from substantial overestimations in some games in the Atari 2600 domain. We
then show that the idea behind the Double Q-learning algorithm, which was introduced
in a tabular setting, can be generalized to work with large-scale function approximation.
We propose a specific adaptation to the DQN algorithm and show that the resulting
algorithm not only reduces the observed overestimations, as hypothesized, but that
this also leads to much better performance on several games.},
author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {2094--2100},
publisher = {AAAI Press},
series = {AAAI'16},
title = {{Deep Reinforcement Learning with Double Q-Learning}},
year = {2016}
}
@article{Kaindl1991,
abstract = {The focus is especially on trees with an ordering similar to that we have actually found in game playing practice. We also compare the algorithms using two different distributions of the static values, the uniform distribution, and a distribution estimated from practical data. Moreover, this is the first systematic comparison of using aspiration windows for all of the usual minimax algorithms. We analyze the effects of aspiration windows of varying size and position. Increasing the ordering of moves to near the optimum results in unexpectedly high savings. Algorithms with linear space complexity especially benefit most. Although the ordering of the first move is of predominant importance, that of the remainder has only second-order effects. The use of an aspiration window not only makes alpha-beta (AB) search competitive, but there also exists previously unpublished dependencies of its effects on certain properties of the trees. In general, the more recently developed algorithms with exponential space complexity are not to be recommended for game-playing practice since their advantage in having to visit fewer nodes is more than outweighed under practical conditions by their enormous space requirements. Finally, we propose a method for an analytic determination of estimates of the optimal window size, presupposing evidence about some characteristic properties of the domain of application. In summary, we discovered and found empirical evidence for several effects unpredicted by theoretical studies. This paper is based on investigations of several algorithms for computing exact minimax values of game trees (uti-lizing backward pruning). {\textcopyright} 1991 IEEE},
author = {Kaindl, Hermann and Shams, Reza and Horacek, Helmut},
doi = {10.1109/34.106996},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Alpha-beta search,aspiration windows,minimal,minimax search algorithms,ordered game trees,simulation studies,state-space search,window search},
number = {12},
pages = {1225--1235},
title = {{Minimax Search Algorithms with and without Aspiration Windows}},
volume = {13},
year = {1991}
}
@book{10.5555/1206386,
address = {Hoboken, NJ, USA},
author = {Montgomery, Douglas C},
isbn = {0470088109},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Design and Analysis of Experiments}},
year = {2006}
}
@inproceedings{rahman2021towards,
author = {Rahman, Muhammad A and Hopner, Niklas and Christianos, Filippos and Albrecht, Stefano V},
booktitle = {International Conference on Machine Learning},
organization = {PMLR},
pages = {8776--8786},
title = {{Towards open ad hoc teamwork using graph-based policy learning}},
year = {2021}
}
@article{倉橋貴彦2016有限要素解析に用いる三角形メッシュの自動生成法,
author = {倉橋貴彦 and 鋤柄あかね and 山際健吾 and 丸岡宏太郎 and 井山徹郎},
journal = {長岡工業高等専門学校研究紀要},
title = {有限要素解析に用いる三角形メッシュの自動生成法},
volume = {52},
year = {2016}
}
@techreport{Greenwald2002,
abstract = {Bowling named two desiderata for multiagent learning algorithms: rationality and convergence. This paper introduces co{\~{}}elated-Q learning, a natural generalization of Nash-Q and FF-Q that satisfies these criteria. NashoQ satisfies rationality, but in general it does not converge. FF-Q satisfies convergence, but in general it is not rational. Correlated-Q satisfies rationality by construction. This papers demonstrates the empirical convergence of correlated-Q on a standard testbed of general-sum Markov games.},
author = {Greenwald, Amy and Hall, Keith},
file = {:Users/hikaruasano/Documents/mendeley/Greenwald, Hall{\_}2002{\_}Correlated-Q Learning.pdf:pdf},
title = {{Correlated-Q Learning}},
url = {www.aaai.org},
year = {2002}
}
@misc{Unknownc,
author = {紗季, 河野 and 美裕, 佐々木},
title = {シミュレーションを用いた感染者隔離と自粛の効果},
url = {http://www.st.nanzan-u.ac.jp/info/gr-thesis/2020/sasakim/pdf/17ss037.pdf}
}
@article{10.1214/ss/1009213726,
author = {Breiman, Leo},
doi = {10.1214/ss/1009213726},
file = {:Users/hikaruasano/Documents/mendeley/Breiman{\_}2001{\_}Statistical Modeling The Two Cultures (with comments and a rejoinder by the author).pdf:pdf},
journal = {Statistical Science},
number = {3},
pages = {199--231},
publisher = {Institute of Mathematical Statistics},
title = {{Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)}},
url = {https://doi.org/10.1214/ss/1009213726},
volume = {16},
year = {2001}
}
@article{Kim2014AHA,
author = {Kim, Woojae and Pitt, M and Lu, Zhong-Lin and Steyvers, M and Myung, Jay I},
file = {:Users/hikaruasano/Documents/mendeley/Kim et al.{\_}2014{\_}A Hierarchical Adaptive Approach to Optimal Experimental Design.pdf:pdf},
journal = {Neural Computation},
pages = {2465--2492},
title = {{A Hierarchical Adaptive Approach to Optimal Experimental Design}},
volume = {26},
year = {2014}
}
@article{doi:10.1126/science.abi7668,
abstract = {Deep neural networks highlight features of human behavior Imagine a choice between two gambles: getting {\$}100 with a probability of 20{\%} or getting {\$}50 with a probability of 80{\%}. In 1979, Kahneman and Tversky published prospect theory (1), a mathematically specified descriptive theory of how people make risky choices such as these. They explained numerous documented violations of expected utility theory, the dominant theory at the time, by using nonlinear psychophysical functions for perceiving underlying probabilities and evaluating resulting payoffs. Prospect theory revolutionized the study of choice behavior, showing that researchers could build formal models of decision-making based on realistic psychological principles (2). But in the ensuing decades, as dozens of competing theories have been proposed (3), there has been theoretical fragmentation, redundancy, and stagnation. There is little consensus on the best decision theory or model. On page 1209 of this issue, Peterson et al. (4) demonstrate the power of a more recent approach: Instead of relying on the intuitions and (potentially limited) intellect of human researchers, the task of theory generation can be outsourced to powerful machine-learning algorithms.},
author = {Bhatia, Sudeep and He, Lisheng},
doi = {10.1126/science.abi7668},
journal = {Science},
number = {6547},
pages = {1150--1151},
title = {{Machine-generated theories of human decision-making}},
url = {https://www.science.org/doi/abs/10.1126/science.abi7668},
volume = {372},
year = {2021}
}
@article{;2017,
author = {{東京大学工学教程編纂委員会; 青山}, 和浩; 山西, 健司},
number = {. システム工学 . 知識システム = Knowledge systems ; 1},
publisher = {東京大学},
title = {知識の表現と学習},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2003372308},
year = {2017}
}
@inproceedings{pmlr-v164-dawson22a,
abstract = {Safety and stability are common requirements for robotic control systems; however, designing safe, stable controllers remains difficult for nonlinear and uncertain models. We develop a model-based learning approach to synthesize robust feedback controllers with safety and stability guarantees. We take inspiration from robust convex optimization and Lyapunov theory to define robust control Lyapunov barrier functions that generalize despite model uncertainty. We demonstrate our approach in simulation on problems including car trajectory tracking, nonlinear control with obstacle avoidance, satellite rendezvous with safety constraints, and flight control with a learned ground effect model. Simulation results show that our approach yields controllers that match or exceed the capabilities of robust MPC while reducing computational costs by an order of magnitude. We provide source code at github.com/dawsonc/neural{\_}clbf/.},
author = {Dawson, Charles and Qin, Zengyi and Gao, Sicun and Fan, Chuchu},
booktitle = {Proceedings of the 5th Conference on Robot Learning},
editor = {Faust, Aleksandra and Hsu, David and Neumann, Gerhard},
file = {:Users/hikaruasano/Documents/mendeley/Dawson et al.{\_}2022{\_}Safe Nonlinear Control Using Robust Neural Lyapunov-Barrier Functions.pdf:pdf},
pages = {1724--1735},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Safe Nonlinear Control Using Robust Neural Lyapunov-Barrier Functions}},
url = {https://proceedings.mlr.press/v164/dawson22a.html},
volume = {164},
year = {2022}
}
@techreport{Yu,
abstract = {Reinforcement learning agents are prone to unde-sired behaviors due to reward mis-specification. Finding a set of reward functions to properly guide agent behaviors is particularly challenging in multi-agent scenarios. Inverse reinforcement learning provides a framework to automatically acquire suitable reward functions from expert demonstrations. Its extension to multi-agent settings, however, is difficult due to the more complex notions of rational behaviors. In this paper, we propose MA-AIRL, a new framework for multi-agent inverse reinforcement learning, which is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics. We derive our algorithm based on a new solution concept and maximum pseu-dolikelihood estimation within an adversarial reward learning framework. In the experiments, we demonstrate that MA-AIRL can recover reward functions that are highly correlated with ground truth ones, and significantly outperforms prior methods in terms of policy imitation.},
archivePrefix = {arXiv},
arxivId = {1907.13220v1},
author = {Yu, Lantao and Song, Jiaming and Ermon, Stefano},
eprint = {1907.13220v1},
file = {:Users/hikaruasano/Documents/mendeley/Yu, Song, Ermon{\_}Unknown{\_}Multi-Agent Adversarial Inverse Reinforcement Learning.pdf:pdf},
isbn = {1907.13220v1},
title = {{Multi-Agent Adversarial Inverse Reinforcement Learning}}
}
@article{duan2021distributional,
author = {Duan, Jingliang and Guan, Yang and Li, Shengbo Eben and Ren, Yangang and Sun, Qi and Cheng, Bo},
file = {:Users/hikaruasano/Documents/mendeley/Duan et al.{\_}2021{\_}Distributional soft actor-critic Off-policy reinforcement learning for addressing value estimation errors.pdf:pdf},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
publisher = {IEEE},
title = {{Distributional soft actor-critic: Off-policy reinforcement learning for addressing value estimation errors}},
year = {2021}
}
@article{Saffidine2012,
abstract = {Alpha-Beta pruning is one of the most powerful and fundamental MiniMax search improvements. It was designed for sequential two-player zero-sum perfect information games. In this paper we introduce an Alpha-Beta-like sound pruning method for the more general class of ``stacked matrix games'' that allow for simultaneous moves by both players. This is accomplished by maintaining upper and lower bounds for achievable payoffs in states with simultaneous actions and dominated action pruning based on the feasibility of certain linear programs. Empirical data shows considerable savings in terms of expanded nodes and computation time compared to naive depth-first move computation without pruning.$\backslash$n},
author = {Saffidine, Abdallah and Finnsson, Hilmar and Buro, Michael},
file = {:Users/hikaruasano/Documents/mendeley/Saffidine, Finnsson, Buro{\_}2012{\_}Alpha-beta pruning for games with simultaneous moves.pdf:pdf},
isbn = {9781577355687},
journal = {Proceedings of the National Conference on Artificial Intelligence},
keywords = {Constraints, Satisfiability, and Search (Main Trac},
pages = {556--562},
title = {{Alpha-beta pruning for games with simultaneous moves}},
volume = {1},
year = {2012}
}
@book{2016研究を売れ,
author = {哲, 夏目 and 眞理雄, 所},
isbn = {9784863452756},
publisher = {丸善プラネット},
title = {研究を売れ!: ソニーコンピュータサイエンス研究所のしたたかな技術経営},
url = {https://books.google.co.jp/books?id=i9wpjwEACAAJ},
year = {2016}
}
@inproceedings{pmlr-v162-sessa22a,
abstract = {We consider model-based multi-agent reinforcement learning, where the environment transition model is unknown and can only be learned via expensive interactions with the environment. We propose H-MARL (Hallucinated Multi-Agent Reinforcement Learning), a novel sample-efficient algorithm that can efficiently balance exploration, i.e., learning about the environment, and exploitation, i.e., achieve good equilibrium performance in the underlying general-sum Markov game. H-MARL builds high-probability confidence intervals around the unknown transition model and sequentially updates them based on newly observed data. Using these, it constructs an optimistic hallucinated game for the agents for which equilibrium policies are computed at each round. We consider general statistical models (e.g., Gaussian processes, deep ensembles, etc.) and policy classes (e.g., deep neural networks), and theoretically analyze our approach by bounding the agents' dynamic regret. Moreover, we provide a convergence rate to the equilibria of the underlying Markov game. We demonstrate our approach experimentally on an autonomous driving simulation benchmark. H-MARL learns successful equilibrium policies after a few interactions with the environment and can significantly improve the performance compared to non-optimistic exploration methods.},
author = {Sessa, Pier Giuseppe and Kamgarpour, Maryam and Krause, Andreas},
booktitle = {Proceedings of the 39th International Conference on Machine Learning},
editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
file = {:Users/hikaruasano/Documents/mendeley/Sessa, Kamgarpour, Krause{\_}2022{\_}Efficient Model-based Multi-agent Reinforcement Learning via Optimistic Equilibrium Computation.pdf:pdf},
pages = {19580--19597},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Efficient Model-based Multi-agent Reinforcement Learning via Optimistic Equilibrium Computation}},
url = {https://proceedings.mlr.press/v162/sessa22a.html},
volume = {162},
year = {2022}
}
@misc{r201810a74:online,
annote = {(Accessed on 06/28/2020)},
author = {千葉市都市局},
howpublished = {https://www.city.chiba.jp/toshi/kenchiku/jutakuseisaku/documents/r201810{\_}akiyatoutaisakukeikaku{\_}honpen.pdf},
title = {千葉市空家等対策計画},
year = {2019}
}
@inproceedings{abel2016reinforcement,
author = {Abel, David and MacGlashan, James and Littman, Michael L},
booktitle = {Workshops at the thirtieth AAAI conference on artificial intelligence},
file = {:Users/hikaruasano/Documents/mendeley/Abel, MacGlashan, Littman{\_}2016{\_}Reinforcement learning as a framework for ethical decision making.pdf:pdf},
title = {{Reinforcement learning as a framework for ethical decision making}},
year = {2016}
}
@inproceedings{chu2020multiagent,
author = {Chu, Tianshu and Chinchali, Sandeep and Katti, Sachin},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley//Chu, Chinchali, Katti{\_}2020{\_}Multi-agent Reinforcement Learning for Networked System Control.pdf:pdf},
title = {{Multi-agent Reinforcement Learning for Networked System Control}},
url = {https://openreview.net/forum?id=Syx7A3NFvH},
year = {2020}
}
@inproceedings{foerster2019bayesian,
author = {Foerster, Jakob and Song, Francis and Hughes, Edward and Burch, Neil and Dunning, Iain and Whiteson, Shimon and Botvinick, Matthew and Bowling, Michael},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Foerster et al.{\_}2019{\_}Bayesian action decoder for deep multi-agent reinforcement learning.pdf:pdf},
pages = {1942--1951},
title = {{Bayesian action decoder for deep multi-agent reinforcement learning}},
year = {2019}
}
@article{Pronzato2002,
author = {Pronzato, Luc and Thierry, {\'{E}}ric},
doi = {10.1007/BF02509828},
journal = {Statistical Methods and Applications},
pages = {277--292},
title = {{Sequential experimental design and response optimisation}},
volume = {11},
year = {2002}
}
@inproceedings{lu2022modelfree,
author = {Lu, Chris and Willi, Timon and de Witt, Christian Schroeder and Foerster, Jakob Nicolaus},
booktitle = {ICLR 2022 Workshop on Gamification and Multiagent Solutions},
file = {:Users/hikaruasano/Documents/mendeley/Lu et al.{\_}2022{\_}Model-Free Opponent Shaping.pdf:pdf},
title = {{Model-Free Opponent Shaping}},
url = {https://openreview.net/forum?id=Bfg{\_}sqypl5},
year = {2022}
}
@inproceedings{dai2017learning,
author = {Dai, Bo and He, Niao and Pan, Yunpeng and Boots, Byron and Song, Le},
booktitle = {Artificial Intelligence and Statistics},
file = {:Users/hikaruasano/Documents/mendeley/Dai et al.{\_}2017{\_}Learning from conditional distributions via dual embeddings.pdf:pdf},
pages = {1458--1467},
title = {{Learning from conditional distributions via dual embeddings}},
year = {2017}
}
@inproceedings{NIPS2017_17d8da81,
author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Guyon, I and Luxburg, U Von and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
file = {:Users/hikaruasano/Documents/mendeley/Dann, Lattimore, Brunskill{\_}2017{\_}Unifying PAC and Regret Uniform PAC Bounds for Episodic Reinforcement Learning(2).pdf:pdf},
publisher = {Curran Associates, Inc.},
title = {{Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning}},
url = {https://proceedings.neurips.cc/paper/2017/file/17d8da815fa21c57af9829fb0a869602-Paper.pdf},
volume = {30},
year = {2017}
}
@misc{2020060184:online,
annote = {(Accessed on 06/28/2020)},
author = {江東区区民部区民課},
howpublished = {https://www.city.koto.lg.jp/060305/kuse/profile/shokai/documents/20200601{\_}1.pdf},
title = {江東区の世帯と人口}
}
@article{Nakata,
author = {Nakata, Y and TJSAI, S Arai - and 2020, Undefined},
file = {:Users/hikaruasano/Documents/mendeley/Nakata, Arai{\_}2020{\_}Bayesian Inverse Reinforcement Learning for Demonstrations of an Expert in Multiple Dynamics.pdf:pdf},
journal = {ui.adsabs.harvard.edu},
title = {{Bayesian Inverse Reinforcement Learning for Demonstrations of an Expert in Multiple Dynamics}},
url = {https://gateway.itc.u-tokyo.ac.jp/abs/2020TJSAI..35J..73N/abstract,DanaInfo=ui.adsabs.harvard.edu}
}
@misc{IEAwebst68:online,
annote = {(Accessed on 07/22/2020)},
author = {IEA},
howpublished = {https://webstore.iea.org/co2-emissions-from-fuel-combustion-2018-highlights},
title = {{CO2 Emissions from Fuel Combustion 2018 Highlights}},
year = {2018}
}
@article{CorbettDavies2018TheMA,
author = {Corbett-Davies, S and Goel, S},
journal = {ArXiv},
title = {{The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning}},
volume = {abs/1808.0},
year = {2018}
}
@book{salganik2019bit,
author = {Salganik, Matthew J},
publisher = {Princeton University Press},
title = {{Bit by bit: Social research in the digital age}},
year = {2019}
}
@book{BB09141636,
author = {理一郎, 溝口 and 人工知能学会},
publisher = {オーム社},
series = {知の科学},
title = {オントロジー工学の理論と実践},
url = {https://ci.nii.ac.jp/ncid/BB09141636},
year = {2012}
}
@inproceedings{yi2022learning,
author = {Yi, Yuxuan and Li, Ge and Wang, Yaowei and Lu, Zongqing},
booktitle = {ICLR 2022 Workshop on Gamification and Multiagent Solutions},
file = {:Users/hikaruasano/Documents/mendeley/Yi et al.{\_}2022{\_}Learning to Share in Multi-Agent Reinforcement Learning.pdf:pdf},
title = {{Learning to Share in Multi-Agent Reinforcement Learning}},
url = {https://openreview.net/forum?id=Sqxvjqy6xc},
year = {2022}
}
@article{1988,
author = {吉沢 and 昌恭},
issn = {03871436},
journal = {広島経済大学経済研究論集},
number = {1},
pages = {p89--102},
publisher = {広島経済大学経済学会},
title = {限界効用理論の含意},
url = {http://ci.nii.ac.jp/naid/120005378497/ja/},
volume = {11},
year = {1988}
}
@misc{｢広告宣伝費｣が31:online,
annote = {(Accessed on 01/21/2021)},
author = {東洋経済オンライン},
howpublished = {https://toyokeizai.net/articles/-/187757?page=2},
title = {｢広告宣伝費｣が多いトップ300社ランキング | 企業ランキング},
year = {2017}
}
@article{kumar2014solving,
author = {Kumar, V Sivaram and Thansekhar, M R and Saravanan, R and Amali, S Miruna Joe},
journal = {Procedia Engineering},
pages = {2176--2185},
publisher = {Elsevier},
title = {{Solving multi-objective vehicle routing problem with time windows by FAGA}},
volume = {97},
year = {2014}
}
@article{ryan2016review,
author = {Ryan, Elizabeth G and Drovandi, Christopher C and McGree, James M and Pettitt, Anthony N},
file = {:Users/hikaruasano/Documents/mendeley/Ryan et al.{\_}2016{\_}A review of modern computational algorithms for Bayesian optimal design.pdf:pdf},
journal = {International Statistical Review},
number = {1},
pages = {128--154},
publisher = {Wiley Online Library},
title = {{A review of modern computational algorithms for Bayesian optimal design}},
volume = {84},
year = {2016}
}
@book{1987,
abstract = {執筆:石塚満ほか 稅込価格: 2500円 各章末:参考文献},
author = {情報処理学会},
isbn = {427407353X},
publisher = {オーム社},
title = {知識工学},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2000082596},
year = {1987}
}
@inproceedings{pmlr-v32-silver14,
abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces.},
address = {Bejing, China},
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
booktitle = {Proceedings of the 31st International Conference on Machine Learning},
editor = {Xing, Eric P and Jebara, Tony},
number = {1},
pages = {387--395},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Deterministic Policy Gradient Algorithms}},
url = {https://proceedings.mlr.press/v32/silver14.html},
volume = {32},
year = {2014}
}
@article{Broadie2011EfficientRE,
author = {Broadie, M and Du, Yiping and Moallemi, C},
file = {:Users/hikaruasano/Documents/mendeley/Broadie, Du, Moallemi{\_}2011{\_}Efficient Risk Estimation via Nested Sequential Simulation.pdf:pdf},
journal = {Manag. Sci.},
pages = {1172--1194},
title = {{Efficient Risk Estimation via Nested Sequential Simulation}},
volume = {57},
year = {2011}
}
@inproceedings{miryoosefi2022simple,
author = {Miryoosefi, Sobhan and Jin, Chi},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Dann, Lattimore, Brunskill{\_}2017{\_}Unifying PAC and Regret Uniform PAC Bounds for Episodic Reinforcement Learning(2).pdf:pdf},
organization = {PMLR},
pages = {15666--15698},
title = {{A simple reward-free approach to constrained reinforcement learning}},
year = {2022}
}
@article{ryu2018multi,
author = {Ryu, Heechang and Shin, Hayong and Park, Jinkyoo},
file = {:Users/hikaruasano/Documents/mendeley/Ryu, Shin, Park{\_}2018{\_}Multi-agent actor-critic with generative cooperative policy network.pdf:pdf},
journal = {arXiv preprint arXiv:1810.09206},
title = {{Multi-agent actor-critic with generative cooperative policy network}},
year = {2018}
}
@article{110009882538,
author = {雄太, 福士 and 英之, 臼井},
issn = {09196072},
journal = {情報処理学会研究報告. ICS, [知能と複雑系]},
month = {feb},
number = {13},
pages = {1--7},
publisher = {一般社団法人情報処理学会},
title = {マルチエージェントを用いた並列パンデミックシミュレーション},
url = {https://ci.nii.ac.jp/naid/110009882538/},
volume = {2015-ICS-1},
year = {2015}
}
@book{BC02891850p162-164,
author = {McIntyre, Lee C and 匠, 居村 and 智史, 大﨑 and 卓也, 西橋 and 完太郎, 大橋},
pages = {pp.162--164},
publisher = {人文書院},
title = {ポストトゥルース},
url = {https://ci.nii.ac.jp/ncid/BC02891850},
year = {2020}
}
@article{1050845763880837888,
author = {小田切, 祐詞},
file = {:Users/hikaruasano/Documents/mendeley/小田切{\_}2013{\_}批判社会学とその規範性の問題 構築主義以後の社会学におけ.pdf:pdf},
issn = {0912-456X},
journal = {慶応義塾大学大学院社会学研究科紀要 : 社会学心理学教育学 : 人間と社会の探究},
pages = {43--59},
publisher = {慶應義塾大学大学院社会学研究科},
title = {批判社会学とその規範性の問題 : 構築主義以後の社会学における規範性への関心の高まりを考えるために},
url = {https://cir.nii.ac.jp/crid/1050845763880837888},
volume = {76},
year = {2013}
}
@techreport{GeraldTesauro1995,
author = {{Gerald Tesauro}, By and Keith, Tom},
booktitle = {Communications of the ACM},
file = {:Users/hikaruasano/Documents/mendeley/Gerald Tesauro, Keith{\_}1995{\_}Temporal Difference Learning and TD-Gammon.pdf:pdf},
number = {3},
title = {{Temporal Difference Learning and TD-Gammon}},
volume = {38},
year = {1995}
}
@article{Myung2013ATO,
author = {Myung, Jay I and Cavagnaro, Daniel R and Pitt, M},
journal = {Journal of mathematical psychology},
pages = {53--67},
title = {{A Tutorial on Adaptive Design Optimization.}},
volume = {57 3-4},
year = {2013}
}
@misc{第1部第1章はじ57:online,
annote = {(Accessed on 07/13/2020)},
author = {資源エネルギー庁},
howpublished = {https://www.enecho.meti.go.jp/about/whitepaper/2020html/},
title = {{令和元年度エネルギーに関する年次報告（エネルギー白書2020） HTML版}},
year = {2020}
}
@article{papoudakis2019dealing,
author = {Papoudakis, Georgios and Christianos, Filippos and Rahman, Arrasy and Albrecht, Stefano V},
file = {:Users/hikaruasano/Documents/mendeley/Papoudakis et al.{\_}2019{\_}Dealing with non-stationarity in multi-agent deep reinforcement learning.pdf:pdf},
journal = {arXiv preprint arXiv:1906.04737},
title = {{Dealing with non-stationarity in multi-agent deep reinforcement learning}},
year = {2019}
}
@book{BA79120757p52,
author = {学, 赤川},
pages = {p.52},
publisher = {勁草書房},
title = {構築主義を再構築する},
url = {https://ci.nii.ac.jp/ncid/BA79120757},
year = {2006}
}
@article{gao2021evaluation,
author = {Gao, Yuxiang and Huang, Chien-Ming},
file = {:Users/hikaruasano/Documents/mendeley/Gao, Huang{\_}2021{\_}Evaluation of socially-aware robot navigation.pdf:pdf},
journal = {Frontiers in Robotics and AI},
pages = {420},
publisher = {Frontiers},
title = {{Evaluation of socially-aware robot navigation}},
year = {2021}
}
@inproceedings{10.1145/3194770.3194776,
abstract = {Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
address = {New York, NY, USA},
author = {Verma, Sahil and Rubin, Julia},
booktitle = {Proceedings of the International Workshop on Software Fairness},
doi = {10.1145/3194770.3194776},
file = {:Users/hikaruasano/Documents/mendeley/Verma, Rubin{\_}2018{\_}Fairness Definitions Explained.pdf:pdf},
isbn = {9781450357463},
pages = {1--7},
publisher = {Association for Computing Machinery},
series = {FairWare '18},
title = {{Fairness Definitions Explained}},
url = {https://doi.org/10.1145/3194770.3194776},
year = {2018}
}
@article{app9204198,
abstract = {Compared with the single robot system, a multi-robot system has higher efficiency and fault tolerance. The multi-robot system has great potential in some application scenarios, such as the robot search, rescue and escort tasks, and so on. Deep reinforcement learning provides a potential framework for multi-robot formation and collaborative navigation. This paper mainly studies the collaborative formation and navigation of multi-robots by using the deep reinforcement learning algorithm. The proposed method improves the classical Deep Deterministic Policy Gradient (DDPG) to address the single robot mapless navigation task. We also extend the single-robot Deep Deterministic Policy Gradient algorithm to the multi-robot system, and obtain the Parallel Deep Deterministic Policy Gradient (PDDPG). By utilizing the 2D lidar sensor, the group of robots can accomplish the formation construction task and the collaborative formation navigation task. The experiment results in a Gazebo simulation platform illustrates that our method is capable of guiding mobile robots to construct the formation and keep the formation during group navigation, directly through raw lidar data inputs.},
author = {Chen, Wenzhou and Zhou, Shizheng and Pan, Zaisheng and Zheng, Huixian and Liu, Yong},
doi = {10.3390/app9204198},
file = {:Users/hikaruasano/Documents/mendeley/Chen et al.{\_}2019{\_}Mapless Collaborative Navigation for a Multi-Robot System Based on the Deep Reinforcement Learning.pdf:pdf;:Users/hikaruasano/Documents/mendeley/Chen et al.{\_}2019{\_}Mapless Collaborative Navigation for a Multi-Robot System Based on the Deep Reinforcement Learning(2).pdf:pdf},
issn = {2076-3417},
journal = {Applied Sciences},
number = {20},
title = {{Mapless Collaborative Navigation for a Multi-Robot System Based on the Deep Reinforcement Learning}},
url = {https://www.mdpi.com/2076-3417/9/20/4198},
volume = {9},
year = {2019}
}
@inproceedings{dobrevski2020adaptive,
author = {Dobrevski, Matej and Sko{\v{c}}aj, Danijel},
booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
file = {:Users/hikaruasano/Documents/mendeley/Dobrevski, Sko{\v{c}}aj{\_}2020{\_}Adaptive dynamic window approach for local navigation.pdf:pdf},
organization = {IEEE},
pages = {6930--6936},
title = {{Adaptive dynamic window approach for local navigation}},
year = {2020}
}
@article{1390001206100363136,
author = {永學, 韓},
doi = {10.24460/mscom.85.0_123},
file = {:Users/hikaruasano/Documents/mendeley/永學{\_}2014{\_}知る権利と国家機密 特定秘密保護法を題材に.pdf:pdf},
issn = {1341-1306},
journal = {マス・コミュニケーション研究},
number = {0},
pages = {123--141},
publisher = {日本マス・コミュニケーション学会},
title = {知る権利と国家機密: 特定秘密保護法を題材に},
url = {https://cir.nii.ac.jp/crid/1390001206100363136},
volume = {85},
year = {2014}
}
@inproceedings{as2022constrained,
author = {As, Yarden and Usmanova, Ilnura and Curi, Sebastian and Krause, Andreas},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/As et al.{\_}2022{\_}Constrained Policy Optimization via Bayesian World Models.pdf:pdf},
title = {{Constrained Policy Optimization via Bayesian World Models}},
url = {https://openreview.net/forum?id=PRZoSmCinhf},
year = {2022}
}
@misc{江東5区大規模水66:online,
annote = {(Accessed on 06/28/2020)},
author = {江東5区広域避難推進協議会},
howpublished = {https://www.city.koto.lg.jp/057101/bosai/bosai-top/topics/20180822.html},
title = {江東5区大規模水害ハザードマップ・江東5区大規模水害広域避難計画について}
}
@article{gilles2021gohome,
author = {Gilles, Thomas and Sabatini, Stefano and Tsishkou, Dzmitry and Stanciulescu, Bogdan and Moutarde, Fabien},
file = {:Users/hikaruasano/Documents/mendeley/Gilles et al.{\_}2021{\_}Gohome Graph-oriented heatmap output for future motion estimation.pdf:pdf},
journal = {arXiv preprint arXiv:2109.01827},
title = {{Gohome: Graph-oriented heatmap output for future motion estimation}},
year = {2021}
}
@inproceedings{naesseth2018variational,
author = {Naesseth, Christian and Linderman, Scott and Ranganath, Rajesh and Blei, David},
booktitle = {International Conference on Artificial Intelligence and Statistics},
file = {:Users/hikaruasano/Documents/mendeley/Naesseth et al.{\_}2018{\_}Variational sequential monte carlo.pdf:pdf},
pages = {968--977},
title = {{Variational sequential monte carlo}},
year = {2018}
}
@article{Wang2017SampleEA,
author = {Wang, Ziyun and Bapst, Victor and Heess, Nicolas Manfred Otto and Mnih, Volodymyr and Munos, R{\'{e}}mi and Kavukcuoglu, Koray and de Freitas, Nando},
journal = {ArXiv},
title = {{Sample Efficient Actor-Critic with Experience Replay}},
volume = {abs/1611.0},
year = {2017}
}
@inproceedings{Nachum2017BridgingTG,
author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
booktitle = {NIPS},
file = {:Users/hikaruasano/Documents/mendeley/Nachum et al.{\_}2017{\_}Bridging the Gap Between Value and Policy Based Reinforcement Learning.pdf:pdf},
title = {{Bridging the Gap Between Value and Policy Based Reinforcement Learning}},
year = {2017}
}
@inproceedings{cotter2019training,
author = {Cotter, Andrew and Gupta, Maya and Jiang, Heinrich and Srebro, Nathan and Sridharan, Karthik and Wang, Serena and Woodworth, Blake and You, Seungil},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Cotter et al.{\_}2019{\_}Training well-generalizing classifiers for fairness metrics and other data-dependent constraints.pdf:pdf},
organization = {PMLR},
pages = {1397--1405},
title = {{Training well-generalizing classifiers for fairness metrics and other data-dependent constraints}},
year = {2019}
}
@article{curran2014well,
author = {Curran, Scott J and Wagner, Robert M and Graves, Ronald L and Keller, Martin and {Green Jr}, Johney B},
journal = {Energy},
pages = {194--203},
publisher = {Elsevier},
title = {{Well-to-wheel analysis of direct and indirect use of natural gas in passenger vehicles}},
volume = {75},
year = {2014}
}
@inproceedings{10.5555/3463952.3464010,
abstract = {Communication improves the efficiency and convergence of multi-agent learning. Existing study of agent communication has been limited on predefined fixed connections. While an attention mechanism exists and is useful for scheduling the communication between agents, it, however, largely ignores the dynamical nature of communication and thus the correlation between agents' connections. In this work, we adopt a normalizing flow to encode correlation between agents interactions. The dynamical communication topology is directly learned by maximizing the agent rewards. In our end-to-end formulation, the communication structure is learned by considering it as a hidden dynamical variable. We realize centralized training of critics and graph reasoning policy, and decentralized execution from local observation and message that are received through the learned dynamical communication topology. Experiments on cooperative navigation in the particle world and adaptive traffic control tasks demonstrate the effectiveness of our method.},
address = {Richland, SC},
author = {Du, Yali and Liu, Bo and Moens, Vincent and Liu, Ziqi and Ren, Zhicheng and Wang, Jun and Chen, Xu and Zhang, Haifeng},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
file = {:Users/hikaruasano/Documents/mendeley/Du et al.{\_}2021{\_}Learning Correlated Communication Topology in Multi-Agent Reinforcement Learning.pdf:pdf},
isbn = {9781450383073},
keywords = {communication topology,multi-agent systems,reinforcement learning},
pages = {456--464},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
series = {AAMAS '21},
title = {{Learning Correlated Communication Topology in Multi-Agent Reinforcement Learning}},
year = {2021}
}
@misc{JovenelM17:online,
annote = {(Accessed on 07/08/2021)},
author = {{BBC NEWS}},
howpublished = {https://www.bbc.com/news/world-latin-america-57758864（2020年1月31日）},
title = {{Jovenel Mo{\"{i}}se: Police kill four after Haiti's president assassinated}},
year = {2021}
}
@article{9803860,
author = {Emara, Salma and Wang, Fei and Li, Baochun and Zeyl, Timothy},
doi = {10.1109/TNSE.2022.3185253},
file = {:Users/hikaruasano/Documents/mendeley/Emara et al.{\_}2022{\_}Pareto Fair Congestion Control with Online Reinforcement Learning.pdf:pdf},
journal = {IEEE Transactions on Network Science and Engineering},
pages = {1--18},
title = {{Pareto: Fair Congestion Control with Online Reinforcement Learning}},
year = {2022}
}
@article{RePEc:spr:bioerq:v:2:y:2017:i:2:d:10.1007_s41247-017-0021-4,
abstract = { The potential for decoupling of energy and resources from economic growth should enable economic development while improving environmental sustainability indicators. Relative decoupling of energy has been a characteristic of developed nations as a consequence of efficiency gains and productivity growth. The trend has strengthened in recent decades as economies have advanced further into the service economy phase. The next phase of development (the so-called ‘Infotronics' phase) is being enabled by the rapid growth of information and communications technology (ICT), and artificial intelligence. The question explored in this commentary is whether the Infotronics phase will shift energy consumption in absolute rather than relative terms (so-called ‘strong' vs. ‘weak' decoupling), using Australia as a case study. In this context, weak decoupling is defined as a relative reduction in energy consumption per unit of GDP, whereas strong decoupling is also an absolute reduction in national energy consumption. Historic data on Australian primary energy consumption, gross domestic product, GDP deflator, and industrial sectors have been assembled for the period 1900–2014. A time-series linear regression between energy and real GDP was undertaken to explore the historic relationship between changes in the changing structure of the Australian economy and energy consumption. Despite a significant shift towards a service economy, primary energy consumption has remained strongly connected to GDP, but overlaid with distinct long-run trends in energy intensity. An explanation for the long-run connection is two-fold. The evolution towards greater social and industrial complexity has been underpinned by the ready availability of cheap fuels. The deepening of the service economy towards the infotronics phase should be seen partly as a consequence of available energy supply and productive primary and secondary sectors. Several specific examples of ICT are explored to test the hypothesis,},
author = {Palmer, Graham},
doi = {10.1007/s41247-017-0021-4},
journal = {Biophysical Economics and Resource Quality},
keywords = {Energy Intensity; Primary Energy Consumption; Serv},
month = {jun},
number = {2},
pages = {1--9},
title = {{Energetic Implications of a Post-industrial Information Economy: The Case Study of Australia}},
url = {https://ideas.repec.org/a/spr/bioerq/v2y2017i2d10.1007{\_}s41247-017-0021-4.html},
volume = {2},
year = {2017}
}
@inproceedings{10.1145/3531146.3533232,
abstract = {It is almost always easier to find an accurate-but-complex model than an accurate-yet-simple model. Finding optimal, sparse, accurate models of various forms (linear models with integer coefficients, decision sets, rule lists, decision trees) is generally NP-hard. We often do not know whether the search for a simpler model will be worthwhile, and thus we do not go to the trouble of searching for one. In this work, we ask an important practical question: can accurate-yet-simple models be proven to exist, or shown likely to exist, before explicitly searching for them? We hypothesize that there is an important reason that simple-yet-accurate models often do exist. This hypothesis is that the size of the Rashomon set is often large, where the Rashomon set is the set of almost-equally-accurate models from a function class. If the Rashomon set is large, it contains numerous accurate models, and perhaps at least one of them is the simple model we desire. In this work, we formally present the Rashomon ratio as a new gauge of simplicity for a learning problem, depending on a function class and a data set. The Rashomon ratio is the ratio of the volume of the set of accurate models to the volume of the hypothesis space, and it is different from standard complexity measures from statistical learning theory. Insight from studying the Rashomon ratio provides an easy way to check whether a simpler model might exist for a problem before finding it, namely whether several different machine learning methods achieve similar performance on the data. In that sense, the Rashomon ratio is a powerful tool for understanding why and when an accurate-yet-simple model might exist. If, as we hypothesize in this work, many real-world data sets admit large Rashomon sets, the implications are vast: it means that simple or interpretable models may often be used for high-stakes decisions without losing accuracy.},
address = {New York, NY, USA},
author = {Semenova, Lesia and Rudin, Cynthia and Parr, Ronald},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
doi = {10.1145/3531146.3533232},
file = {:Users/hikaruasano/Documents/mendeley/Semenova, Rudin, Parr{\_}2022{\_}On the Existence of Simpler Machine Learning Models.pdf:pdf},
isbn = {9781450393522},
keywords = {Generalization,Interpretable Machine Learning,Model Multiplicity,Rashomon Set,Simplicity},
pages = {1827--1858},
publisher = {Association for Computing Machinery},
series = {FAccT '22},
title = {{On the Existence of Simpler Machine Learning Models}},
url = {https://doi.org/10.1145/3531146.3533232},
year = {2022}
}
@inproceedings{Yang2020CM3:,
author = {Yang, Jiachen and Nakhaei, Alireza and Isele, David and Fujimura, Kikuo and Zha, Hongyuan},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Yang et al.{\_}2020{\_}CM3 Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning.pdf:pdf},
title = {{CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning}},
url = {https://openreview.net/forum?id=S1lEX04tPr},
year = {2020}
}
@inproceedings{hughes2018inequity,
author = {Hughes, Edward and Leibo, Joel Z and Phillips, Matthew and Tuyls, Karl and Due{\~{n}}ez-Guzman, Edgar and Casta{\~{n}}eda, Antonio Garc$\backslash$'$\backslash$ia and Dunning, Iain and Zhu, Tina and McKee, Kevin and Koster, Raphael and Others},
booktitle = {Advances in neural information processing systems},
file = {:Users/hikaruasano/Documents/mendeley/Hughes et al.{\_}2018{\_}Inequity aversion improves cooperation in intertemporal social dilemmas(2).pdf:pdf},
pages = {3326--3336},
title = {{Inequity aversion improves cooperation in intertemporal social dilemmas}},
year = {2018}
}
@misc{Carsplan74:online,
annote = {(Accessed on 06/15/2021)},
author = {{Our World in Data}},
howpublished = {https://ourworldindata.org/co2-emissions-from-transport},
title = {{Cars, planes, trains: where do CO2 emissions from transport come from? - Our World in Data}},
year = {2020}
}
@inproceedings{pmlr-v84-narasimhan18a,
abstract = {We develop a general approach for solving constrained classification problems, where the loss and constraints are defined in terms of a general function of the confusion matrix. We are able to handle complex, non-linear loss functions such as the F-measure, G-mean or H-mean, and constraints ranging from budget limits, to constraints for fairness, to bounds on complex evaluation metrics. Our approach builds on the framework of Narasimhan et al. (2015) for unconstrained classification with complex losses, and reduces the constrained learning problem to a sequence of cost-sensitive learning tasks. We provide algorithms for two broad families of problems, involving convex and fractional-convex losses, subject to convex constraints. Our algorithms are statistically consistent, generalize an existing approach for fair classification, and readily apply to multiclass problems. Experiments on a variety of tasks demonstrate the efficacy of our methods.},
author = {Narasimhan, Harikrishna},
booktitle = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
editor = {Storkey, Amos and Perez-Cruz, Fernando},
file = {:Users/hikaruasano/Documents/mendeley/Narasimhan{\_}2018{\_}Learning with Complex Loss Functions and Constraints.pdf:pdf},
pages = {1646--1654},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Learning with Complex Loss Functions and Constraints}},
url = {https://proceedings.mlr.press/v84/narasimhan18a.html},
volume = {84},
year = {2018}
}
@inproceedings{10.5555/2936924.2936998,
address = {Richland, SC},
author = {Odom, Phillip and Natarajan, Sriraam},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents {\&} Multiagent Systems},
file = {:Users/hikaruasano/Desktop/mendeley/2936924.2936998 (1).pdf:pdf},
isbn = {9781450342391},
keywords = {active advice seeking,advice-based learning,inverse reinforcement learning},
pages = {512--520},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
series = {AAMAS '16},
title = {{Active Advice Seeking for Inverse Reinforcement Learning}},
year = {2016}
}
@article{2020a,
author = {裕子, 藤垣 and 喜幸, 廣野},
isbn = {9784130032094},
publisher = {東京大学出版会},
title = {科学コミュニケーション論},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2003540240},
year = {2020}
}
@inproceedings{XuIROS22,
author = {Xu, Qinghong and Li, Jiaoyang and Koenig, Sven and Ma, Hang},
booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
title = {{Multi-Goal Multi-Agent Pickup and Delivery}},
year = {2022}
}
@misc{環境省平成２７年58:online,
annote = {(Accessed on 07/15/2020)},
author = {環境省},
howpublished = {https://www.env.go.jp/earth/report/h29-02/},
title = {平成２７年度低炭素社会の実現に向けた中長期的再生可能エネルギー導入拡大方策検討調査委託業務報告書},
year = {2015}
}
@article{zhang2019decentralized,
author = {Zhang, Kaiqing and Yang, Zhuoran and Ba$\backslash$csar, Tamer},
file = {:Users/hikaruasano/Documents/mendeley/Zhang, Yang, Bacsar{\_}2019{\_}Decentralized Multi-Agent Reinforcement Learning with Networked Agents Recent Advances.pdf:pdf},
journal = {arXiv preprint arXiv:1912.03821},
title = {{Decentralized Multi-Agent Reinforcement Learning with Networked Agents: Recent Advances}},
year = {2019}
}
@inproceedings{Foster2019VariationalBO,
author = {Foster, Adam and Jankowiak, M and Bingham, Eli and Horsfall, Paul and Teh, Y and Rainforth, Tom and Goodman, Noah D},
booktitle = {NeurIPS},
file = {:Users/hikaruasano/Documents/mendeley/Foster et al.{\_}2019{\_}Variational Bayesian Optimal Experimental Design.pdf:pdf},
title = {{Variational Bayesian Optimal Experimental Design}},
year = {2019}
}
@article{Chouldechova2017FairPW,
author = {Chouldechova, A},
file = {:Users/hikaruasano/Documents/mendeley/Chouldechova{\_}2017{\_}Fair prediction with disparate impact A study of bias in recidivism prediction instruments.pdf:pdf},
journal = {Big data},
pages = {153--163},
title = {{Fair prediction with disparate impact: A study of bias in recidivism prediction instruments}},
volume = {5 2},
year = {2017}
}
@misc{「日本の財政を考30:online,
annote = {(Accessed on 07/20/2020)},
author = {財務省},
howpublished = {https://www.mof.go.jp/zaisei/index.htm},
title = {日本の財政を考える},
year = {2019}
}
@article{9434397,
author = {Xu, Minrui and Peng, Jialiang and Gupta, B B and Kang, Jiawen and Xiong, Zehui and Li, Zhenni and El-Latif, Ahmed A Abd},
doi = {10.1109/JIOT.2021.3081626},
file = {:Users/hikaruasano/Documents/mendeley/Xu et al.{\_}2021{\_}Multi-Agent Federated Reinforcement Learning for Secure Incentive Mechanism in Intelligent Cyber-Physical Systems.pdf:pdf},
journal = {IEEE Internet of Things Journal},
pages = {1},
title = {{Multi-Agent Federated Reinforcement Learning for Secure Incentive Mechanism in Intelligent Cyber-Physical Systems}},
year = {2021}
}
@book{BB10483692p167.169,
author = {千鶴子, 上野},
edition = {新版},
pages = {pp.167--169},
publisher = {岩波書店},
series = {岩波現代文庫},
title = {ナショナリズムとジェンダー},
url = {https://ci.nii.ac.jp/ncid/BB10483692},
year = {2012}
}
@article{Shorthouse2018,
author = {Shorthouse, David},
file = {:Users/hikaruasano/Documents/mendeley/Shorthouse{\_}2018{\_}Guardtime Whitepaper on VOLTA - its KSI blockchain-based solution for GDPR.pdf:pdf},
keywords = {a path,blockchain,gdpr,guardtime,ksi,really stands out in,s marketplace,s volta product presents,to gdpr certification that,today,volta},
pages = {1--8},
title = {{Guardtime Whitepaper on VOLTA - its KSI blockchain-based solution for GDPR}},
url = {https://m.guardtime.com/files/guardtime-whitepaper-volta-v2.pdf},
year = {2018}
}
@techreport{Nishimura,
abstract = {This work presents a deep reinforcement learning framework for interactive navigation in a crowded place. Our proposed Learning to Balance (L2B) framework enables mobile robot agents to steer safely towards their destinations by avoiding collisions with a crowd, while actively clearing a path by asking nearby pedestrians to make room, if necessary, to keep their travel efficient. We observe that the safety and efficiency requirements in crowd-aware navigation have a trade-off in the presence of social dilemmas between the agent and the crowd. On the one hand, intervening in pedestrian paths too much to achieve instant efficiency will result in collapsing a natural crowd flow and may eventually put everyone, including the self, at risk of collisions. On the other hand, keeping in silence to avoid every single collision will lead to the agent's inefficient travel. With this observation, our L2B framework augments the reward function used in learning an interactive navigation policy to penalize frequent active path clearing and passive collision avoidance, which substantially improves the balance of the safety-efficiency trade-off. We evaluate our L2B framework in a challenging crowd simulation and demonstrate its superiority, in terms of both navigation success and collision rate, over a state-of-the-art navigation approach.},
archivePrefix = {arXiv},
arxivId = {2003.09207v1},
author = {Nishimura, Mai and Yonetani, Ryo},
eprint = {2003.09207v1},
file = {:Users/hikaruasano/Documents/mendeley/Nishimura, Yonetani{\_}Unknown{\_}L2B Learning to Balance the Safety-Efficiency Trade-off in Interactive Crowd-aware Robot Navigation.pdf:pdf},
title = {{L2B: Learning to Balance the Safety-Efficiency Trade-off in Interactive Crowd-aware Robot Navigation}}
}
@article{baker20161,
author = {Baker, Monya},
journal = {Nature},
number = {7604},
pages = {452--454},
title = {1,500 scientists lift the lid on reproducibility},
volume = {533},
year = {2016}
}
@inproceedings{hadfield2016cooperative,
author = {Hadfield-Menell, Dylan and Russell, Stuart J and Abbeel, Pieter and Dragan, Anca},
booktitle = {Advances in neural information processing systems},
file = {:Users/hikaruasano/Documents/mendeley/Hadfield-Menell et al.{\_}2016{\_}Cooperative inverse reinforcement learning.pdf:pdf},
pages = {3909--3917},
title = {{Cooperative inverse reinforcement learning}},
year = {2016}
}
@book{BA62319162,
author = {裕子, 藤垣},
pages = {pp.13--30},
publisher = {東京大学出版会},
title = {専門知と公共性 : 科学技術社会論の構築へ向けて},
url = {https://ci.nii.ac.jp/ncid/BA62319162},
year = {2003}
}
@article{andrychowicz2020matters,
author = {Andrychowicz, Marcin and Raichuk, Anton and Sta{\'{n}}czyk, Piotr and Orsini, Manu and Girgin, Sertan and Marinier, Raphael and Hussenot, L{\'{e}}onard and Geist, Matthieu and Pietquin, Olivier and Michalski, Marcin and Others},
file = {:Users/hikaruasano/Documents/mendeley/Andrychowicz et al.{\_}2020{\_}What Matters In On-Policy Reinforcement Learning A Large-Scale Empirical Study.pdf:pdf},
journal = {arXiv preprint arXiv:2006.05990},
title = {{What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study}},
year = {2020}
}
@inproceedings{wang2021influencing,
author = {Wang, Woodrow Zhouyuan and Shih, Andy and Xie, Annie and Sadigh, Dorsa},
booktitle = {5th Annual Conference on Robot Learning},
file = {:Users/hikaruasano/Documents/mendeley/Wang et al.{\_}2021{\_}Influencing Towards Stable Multi-Agent Interactions.pdf:pdf},
title = {{Influencing Towards Stable Multi-Agent Interactions}},
url = {https://openreview.net/forum?id=n6xYib0irVR},
year = {2021}
}
@article{su2020counterfactual,
author = {Su, Jianyu and Adams, Stephen and Beling, Peter A},
file = {:Users/hikaruasano/Documents/mendeley/Su, Adams, Beling{\_}2020{\_}Counterfactual multi-agent reinforcement learning with graph convolution communication.pdf:pdf},
journal = {arXiv preprint arXiv:2004.00470},
title = {{Counterfactual multi-agent reinforcement learning with graph convolution communication}},
year = {2020}
}
@techreport{Inui2004,
abstract = {1 2 causeeffectprecond(ition)means 4 4 causeprecondmeans 80{\%} 95{\%}effect 30{\%} 90{\%} 1 27,000 This paper reports the present results of our approach to the automatic knowledge acquisition of causal relations. We created a new typology of the causal relation-cause, effect, precond(ition) and means-mainly based on volitionality. From our experiments using a Japanese resultative connective tame, we achieved 80{\%} recall with over 95{\%} precision for the cause, precond and means relations, and 30{\%} recall with 90{\%} precision for the effect relation. The results indicate that over 27,000 instances of causal relations can be acquired from one year of Japanese newspaper articles.},
author = {Inui, Takashi and Inui, Kentaro and Matsumoto, Yuji},
file = {:Users/hikaruasano/Documents/mendeley/Inui, Inui, Matsumoto{\_}2004{\_}Acquiring Causal Knowledge from Text Using the Connective Marker tame WWW.pdf:pdf},
title = {{Acquiring Causal Knowledge from Text Using the Connective Marker tame WWW}},
year = {2004}
}
@article{Papadopoulos2012,
abstract = {This paper discusses the design and implementation of a highly efficient MiniMax algorithm for the game Abalone. For perfect information games with relatively low branching factor for their decision tree (such as Chess, Checkers etc.) and a highly accurate evaluation function, Alpha-Beta search proved to be far more efficient than Monte Carlo Tree Search. In recent years many new techniques have been developed to improve the efficiency of the Alpha-Beta tree, applied to a variety of scientific fields. This paper explores several techniques for increasing the efficiency of Alpha-Beta Search on the board game of Abalone while introducing some new innovative techniques that proved to be very effective. The main idea behind them is the incorporation of probabilistic features to the otherwise deterministic Alpha-Beta search. {\textcopyright} 2012 IEEE.},
author = {Papadopoulos, Athanasios and Toumpas, Konstantinos and Chrysopoulos, Antonios and Mitkas, Pericles A.},
doi = {10.1109/CIG.2012.6374139},
file = {:Users/hikaruasano/Documents/mendeley/Papadopoulos et al.{\_}2012{\_}Exploring optimization strategies in board game Abalone for Alpha-Beta search.pdf:pdf},
isbn = {9781467311922},
journal = {2012 IEEE Conference on Computational Intelligence and Games, CIG 2012},
pages = {63--70},
title = {{Exploring optimization strategies in board game Abalone for Alpha-Beta search}},
year = {2012}
}
@inproceedings{pmlr-v37-schulman15,
abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
address = {Lille, France},
author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
editor = {Bach, Francis and Blei, David},
pages = {1889--1897},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Trust Region Policy Optimization}},
url = {https://proceedings.mlr.press/v37/schulman15.html},
volume = {37},
year = {2015}
}
@techreport{Ziebart,
abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.},
author = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
file = {:Users/hikaruasano/Documents/mendeley/Ziebart et al.{\_}Unknown{\_}Maximum Entropy Inverse Reinforcement Learning(2).pdf:pdf},
keywords = {Special Track on Physically Grounded Artificial In},
title = {{Maximum Entropy Inverse Reinforcement Learning}},
url = {www.aaai.org}
}
@inproceedings{mohseni2019interaction,
author = {Mohseni-Kabir, Anahita and Isele, David and Fujimura, Kikuo},
booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
file = {:Users/hikaruasano/Documents/mendeley/Mohseni-Kabir, Isele, Fujimura{\_}2019{\_}Interaction-aware multi-agent reinforcement learning for mobile agents with individual goals.pdf:pdf},
organization = {IEEE},
pages = {3370--3376},
title = {{Interaction-aware multi-agent reinforcement learning for mobile agents with individual goals}},
year = {2019}
}
@book{2012ナショナリズムとジェンダー,
author = {千鶴子, 上野},
isbn = {9784006002718},
pages = {p.172},
publisher = {岩波書店},
series = {岩波現代文庫. 学術},
title = {ナショナリズムとジェンダー},
url = {https://books.google.co.jp/books?id=81QfNAEACAAJ},
year = {2012}
}
@article{doi:10.1177/0038038507080443,
abstract = { This ar ticle argues that in an age of knowing capitalism, sociologists have not adequately thought about the challenges posed to their expertise by the proliferation of `social' transactional data which are now routinely collected, processed and analysed by a wide variety of private and public institutions. Drawing on British examples, we argue that whereas over the past 40 years sociologists championed innovative methodological resources, notably the sample survey and the in-depth interviews, which reasonably allowed them to claim distinctive expertise to access the `social' in powerful ways, such claims are now much less secure. We argue that both the sample survey and the in-depth interview are increasingly dated research methods, which are unlikely to provide a robust base for the jurisdiction of empirical sociologists in coming decades. We conclude by speculating how sociology might respond to this coming crisis through taking up new interests in the `politics of method'. },
author = {Savage, Mike and Burrows, Roger},
doi = {10.1177/0038038507080443},
journal = {Sociology},
number = {5},
pages = {885--899},
title = {{The Coming Crisis of Empirical Sociology}},
url = {https://doi.org/10.1177/0038038507080443},
volume = {41},
year = {2007}
}
@article{Vanlier2012ABA,
author = {Vanlier, J and Tiemann, C A and Hilbers, P and Riel, N},
journal = {Bioinformatics},
pages = {1136--1142},
title = {{A Bayesian approach to targeted experiment design}},
volume = {28},
year = {2012}
}
@book{Unknowna,
author = {講義資料},
pages = {51},
title = {第一回，51p}
}
@inproceedings{evtimova2018emergent,
author = {Evtimova, Katrina and Drozdov, Andrew and Kiela, Douwe and Cho, Kyunghyun},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Evtimova et al.{\_}2018{\_}Emergent Communication in a Multi-Modal, Multi-Step Referential Game.pdf:pdf},
title = {{Emergent Communication in a Multi-Modal, Multi-Step Referential Game}},
url = {https://openreview.net/forum?id=rJGZq6g0-},
year = {2018}
}
@inproceedings{Gleave2020Adversarial,
author = {Gleave, Adam and Dennis, Michael and Wild, Cody and Kant, Neel and Levine, Sergey and Russell, Stuart},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Gleave et al.{\_}2020{\_}Adversarial Policies Attacking Deep Reinforcement Learning.pdf:pdf},
title = {{Adversarial Policies: Attacking Deep Reinforcement Learning}},
url = {https://openreview.net/forum?id=HJgEMpVFwB},
year = {2020}
}
@inproceedings{duan2017one,
author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly and Ho, OpenAI Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
booktitle = {Advances in neural information processing systems},
file = {:Users/hikaruasano/Documents/mendeley/Duan et al.{\_}2017{\_}One-shot imitation learning.pdf:pdf},
pages = {1087--1098},
title = {{One-shot imitation learning}},
year = {2017}
}
@misc{Thenumbe80:online,
annote = {(Accessed on 07/22/2020)},
author = {Forum, World Economic},
howpublished = {https://www.weforum.org/agenda/2016/04/the-number-of-cars-worldwide-is-set-to-double-by-2040},
title = {{The number of cars worldwide is set to double by 2040}},
year = {2016}
}
@article{peng2021learning,
author = {Peng, Zhenghao and Hui, Ka Ming and Liu, Chunxiao and Zhou, Bolei},
file = {:Users/hikaruasano/Documents/mendeley/Peng et al.{\_}2021{\_}Learning to Simulate Self-Driven Particles System with Coordinated Policy Optimization.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Learning to Simulate Self-Driven Particles System with Coordinated Policy Optimization}},
volume = {34},
year = {2021}
}
@inproceedings{inproceedings,
author = {Karras, Tero and Laine, Samuli and Aila, Timo},
doi = {10.1109/CVPR.2019.00453},
pages = {4396--4405},
title = {{A Style-Based Generator Architecture for Generative Adversarial Networks}},
year = {2019}
}
@article{zhang2021decentralized,
author = {Zhang, Kaiqing and Yang, Zhuoran and Ba$\backslash$csar, Tamer},
file = {:Users/hikaruasano/Documents/mendeley/Zhang, Yang, Bacsar{\_}2021{\_}Decentralized multi-agent reinforcement learning with networked agents Recent advances.pdf:pdf},
journal = {Frontiers of Information Technology $\backslash${\&} Electronic Engineering},
number = {6},
pages = {802--814},
publisher = {Springer},
title = {{Decentralized multi-agent reinforcement learning with networked agents: Recent advances}},
volume = {22},
year = {2021}
}
@inproceedings{10.5555/3367243.3367363,
abstract = {Deep Reinforcement Learning (DRL) has been applied to address a variety of cooperative multi-agent problems with either discrete action spaces or continuous action spaces. However, to the best of our knowledge, no previous work has ever succeeded in applying DRL to multi-agent problems with discrete-continuous hybrid (or parameterized) action spaces which is very common in practice. Our work fills this gap by proposing two novel algorithms: Deep Multi-Agent Parameterized Q-Networks (Deep MAPQN) and Deep Multi-Agent Hierarchical Hybrid Q-Networks (Deep MAHHQN). We follow the centralized training but decentralized execution paradigm: different levels of communication between different agents are used to facilitate the training process, while each agent executes its policy independently based on local observations during execution. Our empirical results on several challenging tasks (simulated RoboCup Soccer and game Ghost Story) show that both Deep MAPQN and Deep MAHHQN are effective and significantly outperform existing independent deep parameterized Q-learning method.},
author = {Fu, Haotian and Tang, Hongyao and Hao, Jianye and Lei, Zihan and Chen, Yingfeng and Fan, Changjie},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
isbn = {9780999241141},
pages = {2329--2335},
publisher = {AAAI Press},
series = {IJCAI'19},
title = {{Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid Action Spaces}},
year = {2019}
}
@article{article,
author = {Ryu, Heechang and Shin, Hayong and Park, Jinkyoo},
doi = {10.1609/aaai.v34i05.6214},
file = {:Users/hikaruasano/Desktop/mendeley/Ryu, Shin, Park{\_}2020{\_}Multi-Agent Actor-Critic with Hierarchical Graph Attention Network.pdf:pdf},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
pages = {7236--7243},
title = {{Multi-Agent Actor-Critic with Hierarchical Graph Attention Network}},
volume = {34},
year = {2020}
}
@inproceedings{achiam2017constrained,
author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
booktitle = {International conference on machine learning},
file = {:Users/hikaruasano/Documents/mendeley/Achiam et al.{\_}2017{\_}Constrained policy optimization.pdf:pdf},
organization = {PMLR},
pages = {22--31},
title = {{Constrained policy optimization}},
year = {2017}
}
@inproceedings{pmlr-v81-dwork18a,
abstract = {When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the naive application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups. We provide a simple and efficient decoupling technique, that can be added on top of any black-box machine learning algorithm, to learn different classifiers for different groups. Transfer learning is used to mitigate the problem of having too little data on any one group.},
author = {Dwork, Cynthia and Immorlica, Nicole and Kalai, Adam Tauman and Leiserson, Max},
booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
editor = {Friedler, Sorelle A and Wilson, Christo},
file = {:Users/hikaruasano/Documents/mendeley/Dwork et al.{\_}2018{\_}Decoupled Classifiers for Group-Fair and Efficient Machine Learning.pdf:pdf},
pages = {119--133},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Decoupled Classifiers for Group-Fair and Efficient Machine Learning}},
url = {https://proceedings.mlr.press/v81/dwork18a.html},
volume = {81},
year = {2018}
}
@article{Li2017,
abstract = {This work handles the inverse reinforcement learning (IRL) problem where only a small number of demonstrations are available from a demonstrator for each high-dimensional task, insufficient to estimate an accurate reward function. Observing that each demonstrator has an inherent reward for each state and the task-specific behaviors mainly depend on a small number of key states, we propose a meta IRL algorithm that first models the reward function for each task as a distribution conditioned on a baseline reward function shared by all tasks and dependent only on the demonstrator, and then finds the most likely reward function in the distribution that explains the task-specific behaviors. We test the method in a simulated environment on path planning tasks with limited demonstrations, and show that the accuracy of the learned reward function is significantly improved. We also apply the method to analyze the motion of a patient under rehabilitation.},
archivePrefix = {arXiv},
arxivId = {1710.03592},
author = {Li, Kun and Burdick, Joel W.},
eprint = {1710.03592},
file = {:Users/hikaruasano/Documents/mendeley/Li, Burdick{\_}2017{\_}Meta Inverse Reinforcement Learning via Maximum Reward Sharing for Human Motion Analysis.pdf:pdf},
month = {oct},
title = {{Meta Inverse Reinforcement Learning via Maximum Reward Sharing for Human Motion Analysis}},
url = {http://arxiv.org/abs/1710.03592},
year = {2017}
}
@misc{グレタ・トゥーン61:online,
annote = {(Accessed on 01/12/2021)},
author = {HUFFPOST},
howpublished = {https://www.huffingtonpost.jp/entry/story{\_}jp{\_}5ff40afec5b6e7974fd5e1f8},
title = {グレタ・トゥーンベリさんら若者が、三菱商事などに抗議。公開質問状も | ハフポスト},
year = {2021}
}
@article{lau2022multi,
author = {Lau, Tim Tsz-Kit and Sengupta, Biswa},
file = {:Users/hikaruasano/Documents/mendeley/Lau, Sengupta{\_}2022{\_}The Multi-Agent Pickup and Delivery Problem MAPF, MARL and Its Warehouse Applications.pdf:pdf},
journal = {arXiv preprint arXiv:2203.07092},
title = {{The Multi-Agent Pickup and Delivery Problem: MAPF, MARL and Its Warehouse Applications}},
year = {2022}
}
@article{huan2016sequential,
author = {Huan, Xun and Marzouk, Youssef M},
file = {:Users/hikaruasano/Documents/mendeley/Huan, Marzouk{\_}2016{\_}Sequential Bayesian optimal experimental design via approximate dynamic programming.pdf:pdf},
journal = {arXiv preprint arXiv:1604.08320},
title = {{Sequential Bayesian optimal experimental design via approximate dynamic programming}},
year = {2016}
}
@inproceedings{dean2012large,
author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Others},
booktitle = {Advances in neural information processing systems},
file = {:Users/hikaruasano/Documents/mendeley/Dean et al.{\_}2012{\_}Large scale distributed deep networks.pdf:pdf},
pages = {1223--1231},
title = {{Large scale distributed deep networks}},
year = {2012}
}
@article{Giles2019DecisionmakingUU,
author = {Giles, M and Goda, T},
file = {:Users/hikaruasano/Documents/mendeley/Giles, Goda{\_}2019{\_}Decision-making under uncertainty using MLMC for efficient estimation of EVPPI.pdf:pdf},
journal = {Statistics and Computing},
pages = {739--751},
title = {{Decision-making under uncertainty: using MLMC for efficient estimation of EVPPI}},
volume = {29},
year = {2019}
}
@inproceedings{Whitney2020Dynamics-Aware,
author = {Whitney, William and Agarwal, Rajat and Cho, Kyunghyun and Gupta, Abhinav},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Whitney et al.{\_}2020{\_}Dynamics-Aware Embeddings.pdf:pdf},
title = {{Dynamics-Aware Embeddings}},
url = {https://openreview.net/forum?id=BJgZGeHFPH},
year = {2020}
}
@misc{Energyin78:online,
annote = {(Accessed on 07/22/2020)},
author = {Agency, Swedish Energy},
howpublished = {file:///Users/hikaruasano/Downloads/Energy{\%}20in{\%}20Sweden{\%}20An{\%}20overview.pdf},
title = {{Energy in Sweden 2018 – An overview}},
year = {2019}
}
@book{BN00482290,
author = {Foucault, Michel},
publisher = {新潮社},
title = {性の歴史},
url = {https://ci.nii.ac.jp/ncid/BN00482290},
year = {1986}
}
@inproceedings{koh2018reinforcement,
author = {Koh, Song Sang and Zhou, Bo and Yang, Po and Yang, Zaili and Fang, Hui and Feng, Jianxin},
booktitle = {2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
organization = {IEEE},
pages = {1468--1473},
title = {{Reinforcement Learning for Vehicle Route Optimization in SUMO}},
year = {2018}
}
@inproceedings{Liu,
author = {Liu, Zuxin and Guo, Zijian and Zhao, Ding},
booktitle = {ICML 2022},
file = {:Users/hikaruasano/Documents/mendeley/Liu, Guo, Zhao{\_}2022{\_}Constrained Model-based Reinforcement Learning via Robust Planning.pdf:pdf},
title = {{Constrained Model-based Reinforcement Learning via Robust Planning}},
year = {2022}
}
@techreport{Matsubara1997,
abstract = {In game programming research there are four interesting and related domains: chess, xiang qi (Chinese chess), shogi (Japanese chess) and go. In this article we will compare chess with shogi, both comparing the rules and the computational aspects of both games. We will see that chess and shogi are very similar, but that there are some important dierences that complicate game programming for shogi. Most important dierence is the game tree complexity, which is considerably higher than the game tree complexity of chess. We will then argue that these similarities and dierences make shogi a good choice for further research in game programming. Chess will soon no longer be competitively interesting. Xiang qi has a game tree complexity similar to chess, suggesting that the same AI techniques will also be successful in this domain. Go is too risky as a next research target because little is known about the cognitive aspects of the game, which in our view hold the key to developing new techniques. Also in this article, a short history of computer shogi with the results of the latest CSA computer shogi tournament is given. In the appendix a short introduction to the rules of the game is included.},
author = {Matsubara, Hitoshi and Iida, Hiroyuki and Grimbergen, Reijer},
file = {:Users/hikaruasano/Documents/mendeley/Matsubara, Iida, Grimbergen{\_}1997{\_}Chess, Shogi, Go, natural developments in game research.pdf:pdf},
title = {{Chess, Shogi, Go, natural developments in game research}},
year = {1997}
}
@article{doi:10.1080/01621459.1995.10476636,
author = {M{\"{u}}ller, Peter and Parmigiani, Giovanni},
doi = {10.1080/01621459.1995.10476636},
journal = {Journal of the American Statistical Association},
number = {432},
pages = {1322--1330},
publisher = {Taylor {\&} Francis},
title = {{Optimal Design via Curve Fitting of Monte Carlo Experiments}},
url = {https://doi.org/10.1080/01621459.1995.10476636},
volume = {90},
year = {1995}
}
@article{ma2021learning,
author = {Ma, Ziyuan and Luo, Yudong and Pan, Jia},
file = {:Users/hikaruasano/Documents/mendeley/Ma, Luo, Pan{\_}2021{\_}Learning Selective Communication for Multi-Agent Path Finding.pdf:pdf;:Users/hikaruasano/Documents/mendeley/Ma, Luo, Pan{\_}2021{\_}Learning Selective Communication for Multi-Agent Path Finding(2).pdf:pdf},
journal = {IEEE Robotics and Automation Letters},
publisher = {IEEE},
title = {{Learning Selective Communication for Multi-Agent Path Finding}},
year = {2021}
}
@article{Bosansky2013,
abstract = {We focus on solving two-player zero-sum extensive-form games with perfect information and simultaneous moves. In these games, both players fully observe the current state of the game where they simultaneously make a move determining the next state of the game. We solve these games by a novel algorithm that relies on two components: (1) it iteratively solves the games that correspond to a single simultaneous move using a double-oracle method, and (2) it prunes the states of the game using bounds on the sub-game values obtained by the classical Alpha-Beta search on a serialized variant of the game. We experimentally evaluate our algorithm on the Goofspiel card game, a pursuitevasion game, and randomly generated games. The results show that our novel algorithm typically provides significant running-time improvements and reduction in the number of evaluated nodes compared to the full search algorithm.},
author = {Bo{\v{s}}ansk{\'{y}}, Branislav and Lis{\'{y}}, Viliam and {\v{C}}erm{\'{a}}k, Jiř{\'{i}} and V{\'{i}}tek, Roman and Pe{\v{c}}hou{\v{c}}ek, Michal},
file = {:Users/hikaruasano/Documents/mendeley/Bo{\v{s}}ansk{\'{y}} et al.{\_}2013{\_}Using double-oracle method and serialized Alpha-Beta search for pruning in simultaneous move games.pdf:pdf},
isbn = {9781577356332},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {48--54},
title = {{Using double-oracle method and serialized Alpha-Beta search for pruning in simultaneous move games}},
year = {2013}
}
@article{BarthMaron2018DistributedDD,
author = {Barth-Maron, Gabriel and Hoffman, Matthew W and Budden, D and Dabney, Will and Horgan, Dan and Dhruva, T B and Muldal, Alistair and Heess, N and Lillicrap, T},
file = {:Users/hikaruasano/Documents/mendeley/Barth-Maron et al.{\_}2018{\_}Distributed Distributional Deterministic Policy Gradients.pdf:pdf},
journal = {ArXiv},
title = {{Distributed Distributional Deterministic Policy Gradients}},
volume = {abs/1804.0},
year = {2018}
}
@inproceedings{qiu2021reward,
author = {Qiu, Shuang and Ye, Jieping and Wang, Zhaoran and Yang, Zhuoran},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Qiu et al.{\_}2021{\_}On reward-free rl with kernel and neural function approximations Single-agent mdp and markov game.pdf:pdf},
organization = {PMLR},
pages = {8737--8747},
title = {{On reward-free rl with kernel and neural function approximations: Single-agent mdp and markov game}},
year = {2021}
}
@article{130008081659,
author = {五十樹, 野田 and 竣希, 鷹見 and 正輝, 大西},
doi = {10.11517/jsaisigtwo.2020.SAI-039_13},
journal = {人工知能学会第二種研究会資料},
number = {SAI-039},
pages = {13},
publisher = {一般社団法人 人工知能学会},
title = {マルチエージェント 感染シミュレーショ ンにおける人口分布偏在 影響の反映},
url = {https://ci.nii.ac.jp/naid/130008081659/},
volume = {2020},
year = {2020}
}
@book{BC02891850p162164,
author = {McIntyre, Lee C and 匠, 居村 and 智史, 大﨑 and 卓也, 西橋 and 完太郎, 大橋},
pages = {pp.162--164},
publisher = {人文書院},
title = {ポストトゥルース},
url = {https://ci.nii.ac.jp/ncid/BC02891850},
year = {2020}
}
@article{Cherry2011,
abstract = {In this paper we present an intelligent Othello game player that combines game-specific heuristics with machine learning techniques for move selection. Five game specific heuristics have been proposed; some of which can be generalized to fit other games. For machine learning techniques, the normal Minimax algorithm along with a custom variation is used as a base. Genetic algorithms and neural networks are applied to learn the static evaluation function. The game specific techniques (or a subset of) are to be executed first and if no move is found, Minimax is performed. All techniques, and several subsets of them, have been tested against three deterministic agents, one non-deterministic agent, and three human players of varying skill levels. The results show that the combined Othello player performs better in general. We present the study results on the basis of performance (percentage of games won), speed, predictability of opponent, and usage situation.},
author = {Cherry, Kevin and Chen, Jianhua},
file = {:Users/hikaruasano/Documents/mendeley/Cherry, Chen{\_}2011{\_}An intelligent othello player combining machine learning and game specific heuristics.pdf:pdf},
isbn = {9781601321855},
journal = {Proceedings of the 2011 International Conference on Artificial Intelligence, ICAI 2011},
keywords = {Expected min,Genetic algorithm,Influence map,Minimax,Neural network,Othello},
pages = {372--379},
title = {{An intelligent othello player combining machine learning and game specific heuristics}},
volume = {1},
year = {2011}
}
@article{rudin2019stop,
author = {Rudin, Cynthia},
file = {:Users/hikaruasano/Documents/mendeley/Rudin{\_}2019{\_}Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.pdf:pdf},
journal = {Nature Machine Intelligence},
number = {5},
pages = {206--215},
publisher = {Nature Publishing Group},
title = {{Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead}},
volume = {1},
year = {2019}
}
@article{Guez2018,
abstract = {Planning problems are among the most important and well-studied problems in artificial intel-ligence. They are most typically solved by tree search algorithms that simulate ahead into the fu-ture, evaluate future states, and back-up those evaluations to the root of a search tree. Among these algorithms, Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely used. A typical implementation of MCTS uses cleverly designed rules, optimised to the particular characteristics of the domain. These rules control where the simulation traverses, what to evaluate in the states that are reached, and how to back-up those evaluations. In this paper we instead learn where, what and how to search. Our architecture, which we call an MCTSnet, incorporates simulation-based search inside a neural network, by expanding, evaluating and backing-up a vector embedding. The parameters of the network are trained end-to-end using gradient-based optimisation. When applied to small searches in the well-known planning problem Sokoban, the learned search algorithm significantly outperformed MCTS baselines.},
archivePrefix = {arXiv},
arxivId = {1802.04697},
author = {Guez, Arthur and Weber, Theophane and Antonoglou, Ioannis and Simonyan, Karen and Vinyals, Oriol and Wierstra, Daan and Munos, Remi and Silver, David},
eprint = {1802.04697},
file = {:Users/hikaruasano/Documents/mendeley/Guez et al.{\_}2018{\_}Learning to search with MCTSnets.pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
pages = {2920--2931},
title = {{Learning to search with MCTSnets}},
volume = {4},
year = {2018}
}
@article{nosek2018preregistration,
author = {Nosek, Brian A and Ebersole, Charles R and DeHaven, Alexander C and Mellor, David T},
journal = {Proceedings of the National Academy of Sciences},
number = {11},
pages = {2600--2606},
publisher = {National Acad Sciences},
title = {{The preregistration revolution}},
volume = {115},
year = {2018}
}
@misc{Passenge67:online,
annote = {(Accessed on 01/31/2022)},
author = {(EIA), U.S. Energy Information Administration},
howpublished = {https://www.eia.gov/todayinenergy/detail.php?id=23832},
title = {{Passenger travel accounts for most of world transportation energy use}},
year = {2015}
}
@article{Xu_Wang_Wang_Jia_Lu_2021,
author = {Xu, Bingyu and Wang, Yaowei and Wang, Zhaozhi and Jia, Huizhu and Lu, Zongqing},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
month = {may},
number = {1},
pages = {669--677},
title = {{Hierarchically and Cooperatively Learning Traffic Signal Control}},
url = {https://ojs.aaai.org/index.php/AAAI/article/view/16147},
volume = {35},
year = {2021}
}
@article{1390001204467060352,
author = {忠久, 浜田 and 千代子, 小川 and 美都江, 小野田},
doi = {10.20704/rmsj.71.0_3},
file = {:Users/hikaruasano/Documents/mendeley/忠久, 千代子, 美都江{\_}2016{\_}社会的状況の国際比較による情報公開、公文書管理、秘密保護に関わる考察.pdf:pdf},
issn = {0915-4787},
journal = {レコード・マネジメント},
number = {0},
pages = {3--23},
publisher = {記録管理学会},
title = {社会的状況の国際比較による情報公開、公文書管理、秘密保護に関わる考察},
url = {https://cir.nii.ac.jp/crid/1390001204467060352},
volume = {71},
year = {2016}
}
@article{Milewski2014,
abstract = {Background: Much attention has been given to the relationship between various training factors and athletic injuries, but no study has examined the impact of sleep deprivation on injury rates in young athletes. Information about sleep practices was gathered as part of a study designed to correlate various training practices with the risk of injury in adolescent athletes. Methods: Informed consent for participation in an online survey of training practices and a review of injury records was obtained from 160 student athletes at a combined middle/high school (grades 7 to 12) and from their parents. Online surveys were completed by 112 adolescent athletes (70{\%} completion rate), including 54 male and 58 female athletes with a mean age of 15 years (SD=1.5; range, 12 to 18y). The students' responses were then correlated with data obtained from a retrospective review of injury records maintained by the school's athletic department. Results: Multivariate analysis showed that hours of sleep per night and the grade in school were the best independent predictors of injury. Athletes who slept on average {\textless}8 hours per night were 1.7 times (95{\%} confidence interval, 1.0-3.0; P =0.04) more likely to have had an injury compared with athletes who slept for ≥8 hours. For each additional grade in school, the athletes were 1.4 times more likely to have had an injury (95{\%} confidence interval, 1.2-1.6; P {\textless} 0.001). Conclusion: Sleep deprivation and increasing grade in school appear to be associated with injuries in an adolescent athletic population. Encouraging young athletes to get optimal amounts of sleep may help protect them against athletic injuries. Level of Evidence: Level III. Copyright {\textcopyright} 2014 by Lippincott Williams {\&} Wilkins.},
author = {Milewski, Matthew D. and Skaggs, David L. and Bishop, Gregory A. and Pace, J. Lee and Ibrahim, David A. and Wren, Tishya A.L. and Barzdukas, Audrius},
doi = {10.1097/BPO.0000000000000151},
issn = {15392570},
journal = {Journal of Pediatric Orthopaedics},
keywords = {Adolescents,Injuries,Sleep,Sports},
number = {2},
pages = {129--133},
pmid = {25028798},
publisher = {Lippincott Williams and Wilkins},
title = {{Chronic lack of sleep is associated with increased sports injuries in adolescent athletes}},
volume = {34},
year = {2014}
}
@inproceedings{pmlr-v80-agarwal18a,
abstract = {We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.},
author = {Agarwal, Alekh and Beygelzimer, Alina and Dudik, Miroslav and Langford, John and Wallach, Hanna},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
editor = {Dy, Jennifer and Krause, Andreas},
file = {:Users/hikaruasano/Documents/mendeley/Agarwal et al.{\_}2018{\_}A Reductions Approach to Fair Classification.pdf:pdf},
pages = {60--69},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{A Reductions Approach to Fair Classification}},
url = {https://proceedings.mlr.press/v80/agarwal18a.html},
volume = {80},
year = {2018}
}
@misc{江東5区広域避難13:online,
annote = {(Accessed on 06/28/2020)},
howpublished = {https://www.city.edogawa.tokyo.jp/e007/bosaianzen/bosai/kojo/koto5{\_}kyougikai.html},
title = {江東5区広域避難推進協議会 江戸川区ホームページ}
}
@article{8676306,
author = {Wang, Yuandou and Liu, Hang and Zheng, Wanbo and Xia, Yunni and Li, Yawen and Chen, Peng and Guo, Kunyin and Xie, Hong},
doi = {10.1109/ACCESS.2019.2902846},
file = {:Users/hikaruasano/Documents/mendeley/Wang et al.{\_}2019{\_}Multi-Objective Workflow Scheduling With Deep-Q-Network-Based Multi-Agent Reinforcement Learning.pdf:pdf},
journal = {IEEE Access},
pages = {39974--39982},
title = {{Multi-Objective Workflow Scheduling With Deep-Q-Network-Based Multi-Agent Reinforcement Learning}},
volume = {7},
year = {2019}
}
@article{white1982multi,
author = {White, D J},
file = {:Users/hikaruasano/Documents/mendeley/White{\_}1982{\_}Multi-objective infinite-horizon discounted Markov decision processes.pdf:pdf},
journal = {Journal of mathematical analysis and applications},
number = {2},
pages = {639--647},
publisher = {Academic Press},
title = {{Multi-objective infinite-horizon discounted Markov decision processes}},
volume = {89},
year = {1982}
}
@article{2005,
author = {川俣 and 雅弘},
issn = {1880-3164},
journal = {経済学史研究},
number = {2},
pages = {108--124},
publisher = {経済学史学会},
title = {20世紀の経済学における序数主義の興隆と衰退},
url = {http://ci.nii.ac.jp/naid/130004246464/ja/},
volume = {47},
year = {2005}
}
@misc{Worldbank,
author = {Worldbank},
title = {{CO2 emissions}},
url = {https://data.worldbank.org/indicator/EN.ATM.CO2E.PC},
urldate = {2020-06-12}
}
@article{10016060503,
author = {NISHINO, Makoto},
issn = {1341688X},
journal = {Bulletin of the Iron and Steel Institute of Japan},
month = {jan},
number = {1},
pages = {23--26},
publisher = {日本鉄鋼協会},
title = {{Theoretical Evaluation of CO{\_}2 Emission by Integrated Steelmakers}},
url = {https://ci.nii.ac.jp/naid/10016060503/en/},
volume = {3},
year = {1998}
}
@inbook{Nowé2012,
abstract = {Reinforcement Learning was originally developed for Markov Decision Processes (MDPs). It allows a single agent to learn a policy that maximizes a possibly delayed reward signal in a stochastic stationary environment. It guarantees convergence to the optimal policy, provided that the agent can sufficiently experiment and the environment in which it is operating is Markovian. However, when multiple agents apply reinforcement learning in a shared environment, this might be beyond the MDP model. In such systems, the optimal policy of an agent depends not only on the environment, but on the policies of the other agents as well. These situations arise naturally in a variety of domains, such as: robotics, telecommunications, economics, distributed control, auctions, traffic light control, etc. In these domains multi-agent learning is used, either because of the complexity of the domain or because control is inherently decentralized. In such systems it is important that agents are capable of discovering good solutions to the problem at hand either by coordinating with other learners or by competing with them. This chapter focuses on the application reinforcement learning techniques in multi-agent systems. We describe a basic learning framework based on the economic research into game theory, and illustrate the additional complexity that arises in such systems. We also described a representative selection of algorithms for the different areas of multi-agent reinforcement learning research.},
address = {Berlin, Heidelberg},
author = {Now{\'{e}}, Ann and Vrancx, Peter and {De Hauwere}, Yann-Micha{\"{e}}l},
booktitle = {Reinforcement Learning: State-of-the-Art},
doi = {10.1007/978-3-642-27645-3_14},
editor = {Wiering, Marco and van Otterlo, Martijn},
file = {:Users/hikaruasano/Documents/mendeley/Now{\'{e}}, Vrancx, De Hauwere{\_}2012{\_}Game Theory and Multi-agent Reinforcement Learning.pdf:pdf},
isbn = {978-3-642-27645-3},
pages = {441--470},
publisher = {Springer Berlin Heidelberg},
title = {{Game Theory and Multi-agent Reinforcement Learning}},
url = {https://doi.org/10.1007/978-3-642-27645-3{\_}14},
year = {2012}
}
@misc{検索の仕組み検索30:online,
annote = {(Accessed on 01/21/2021)},
author = {Google},
howpublished = {https://www.google.com/intl/ja/search/howsearchworks/algorithms/},
title = {検索の仕組み | 検索アルゴリズム}
}
@inproceedings{papoudakis2021benchmarking,
author = {Papoudakis, Georgios and Christianos, Filippos and Sch{\"{a}}fer, Lukas and Albrecht, Stefano V},
booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
file = {:Users/hikaruasano/Documents/mendeley/Papoudakis et al.{\_}2021{\_}Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks.pdf:pdf},
title = {{Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks}},
url = {https://openreview.net/forum?id=cIrPX-Sn5n},
year = {2021}
}
@article{doi:10.1073/pnas.1303102110,
abstract = {Population heterogeneity is ubiquitous in social science. The very objective of social science research is not to discover abstract and universal laws but to understand population heterogeneity. Due to population heterogeneity, causal inference with observational data in social science is impossible without strong assumptions. Researchers have long been concerned with two potential sources of bias. The first is bias in unobserved pretreatment factors affecting the outcome even in the absence of treatment. The second is bias due to heterogeneity in treatment effects. In this article, I show how “composition bias” due to population heterogeneity evolves over time when treatment propensity is systematically associated with heterogeneous treatment effects. A form of selection bias, composition bias, arises dynamically at the aggregate level even when the classic assumption of ignorability holds true at the microlevel.},
author = {Xie, Yu},
doi = {10.1073/pnas.1303102110},
file = {:Users/hikaruasano/Documents/mendeley/Xie{\_}2013{\_}Population heterogeneity and causal inference.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
number = {16},
pages = {6262--6268},
title = {{Population heterogeneity and causal inference}},
url = {https://www.pnas.org/doi/abs/10.1073/pnas.1303102110},
volume = {110},
year = {2013}
}
@article{10.1145/3070861,
abstract = {Multi-agent reinforcement learning (MARL) is a widely researched technique for decentralised control in complex large-scale autonomous systems. Such systems often operate in environments that are continuously evolving and where agents' actions are non-deterministic, so called inherently non-stationary environments. When there are inconsistent results for agents acting on such an environment, learning and adapting is challenging. In this article, we propose P-MARL, an approach that integrates prediction and pattern change detection abilities into MARL and thus minimises the effect of non-stationarity in the environment. The environment is modelled as a time-series, with future estimates provided using prediction techniques. Learning is based on the predicted environment behaviour, with agents employing this knowledge to improve their performance in realtime. We illustrate P-MARL's performance in a real-world smart grid scenario, where the environment is heavily influenced by non-stationary power demand patterns from residential consumers. We evaluate P-MARL in three different situations, where agents' action decisions are independent, simultaneous, and sequential. Results show that all methods outperform traditional MARL, with sequential P-MARL achieving best results.},
address = {New York, NY, USA},
author = {Marinescu, Andrei and Dusparic, Ivana and Clarke, Siobh{\'{a}}n},
doi = {10.1145/3070861},
file = {:Users/hikaruasano/Documents/mendeley/Marinescu, Dusparic, Clarke{\_}2017{\_}Prediction-Based Multi-Agent Reinforcement Learning in Inherently Non-Stationary Environments.pdf:pdf},
issn = {1556-4665},
journal = {ACM Trans. Auton. Adapt. Syst.},
keywords = {Multi-agent systems,environment prediction,reinforcement learning,smart grids},
month = {may},
number = {2},
publisher = {Association for Computing Machinery},
title = {{Prediction-Based Multi-Agent Reinforcement Learning in Inherently Non-Stationary Environments}},
url = {https://doi.org/10.1145/3070861},
volume = {12},
year = {2017}
}
@inproceedings{iqbal2019actor,
author = {Iqbal, Shariq and Sha, Fei},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Iqbal, Sha{\_}2019{\_}Actor-attention-critic for multi-agent reinforcement learning.pdf:pdf},
pages = {2961--2970},
title = {{Actor-attention-critic for multi-agent reinforcement learning}},
year = {2019}
}
@inproceedings{ames2019control,
author = {Ames, Aaron D and Coogan, Samuel and Egerstedt, Magnus and Notomista, Gennaro and Sreenath, Koushil and Tabuada, Paulo},
booktitle = {2019 18th European control conference (ECC)},
file = {:Users/hikaruasano/Documents/mendeley/Ames et al.{\_}2019{\_}Control barrier functions Theory and applications.pdf:pdf},
organization = {IEEE},
pages = {3420--3431},
title = {{Control barrier functions: Theory and applications}},
year = {2019}
}
@article{ribes2019sts,
author = {Ribes, David},
journal = {Science, Technology, {\&} Human Values},
number = {3},
pages = {514--539},
publisher = {SAGE Publications Sage CA: Los Angeles, CA},
title = {{STS, meet data science, once again}},
volume = {44},
year = {2019}
}
@misc{空き家施策神奈川14:online,
annote = {(Accessed on 06/28/2020)},
author = {神奈川県県土整備局建築住宅部},
howpublished = {https://www.pref.kanagawa.jp/docs/zm4/akiya/index.html{\#}jokyo},
title = {空き家施策},
year = {2020}
}
@inproceedings{Konda1999ActorCriticA,
author = {Konda, V and Tsitsiklis, J},
booktitle = {NIPS},
title = {{Actor-Critic Algorithms}},
year = {1999}
}
@misc{焦点：アマゾンが64:online,
annote = {(Accessed on 01/31/2021)},
author = {ロイター},
howpublished = {https://jp.reuters.com/article/amazon-jobs-ai-analysis-idJPKCN1ML0DN，2021年1月30日},
title = {{焦点：アマゾンがＡＩ採用打ち切り、「女性差別」の欠陥露呈で}},
year = {2018}
}
@book{;;;2017,
author = {純子, 竹内 and 剛, 伊藤 and 浩, 岡本 and 直樹, 戸田},
isbn = {9784532321703},
publisher = {日本経済新聞出版社},
title = {{エネルギー産業の2050年Utility3.0へのゲームチェンジ}},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2003445773},
year = {2017}
}
@article{zhang2019decentralized,
author = {Zhang, Kaiqing and Yang, Zhuoran and Ba$\backslash$csar, Tamer},
file = {:Users/hikaruasano/Documents/mendeley/Zhang, Yang, Bacsar{\_}2019{\_}Decentralized Multi-Agent Reinforcement Learning with Networked Agents Recent Advances.pdf:pdf},
journal = {arXiv preprint arXiv:1912.03821},
title = {{Decentralized Multi-Agent Reinforcement Learning with Networked Agents: Recent Advances}},
year = {2019}
}
@article{de2021constrained,
author = {{De Nijs}, Frits and Walraven, Erwin and {De Weerdt}, Mathijs and Spaan, Matthijs},
file = {:Users/hikaruasano/Documents/mendeley/De Nijs et al.{\_}2021{\_}Constrained multiagent Markov decision processes A taxonomy of problems and algorithms.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {955--1001},
title = {{Constrained multiagent Markov decision processes: A taxonomy of problems and algorithms}},
volume = {70},
year = {2021}
}
@article{saltelli2020sociology,
author = {Saltelli, Andrea and {Di Fiore}, Monica},
journal = {Humanities and Social Sciences Communications},
number = {1},
pages = {1--8},
publisher = {Palgrave},
title = {{From sociology of quantification to ethics of quantification}},
volume = {7},
year = {2020}
}
@incollection{NIPS2011_4420,
author = {Levine, Sergey and Popovic, Zoran and Koltun, Vladlen},
booktitle = {Advances in Neural Information Processing Systems 24},
editor = {Shawe-Taylor, J and Zemel, R S and Bartlett, P L and Pereira, F and Weinberger, K Q},
file = {:Users/hikaruasano/Documents/mendeley/Levine, Popovic, Koltun{\_}2011{\_}Nonlinear Inverse Reinforcement Learning with Gaussian Processes.pdf:pdf},
pages = {19--27},
publisher = {Curran Associates, Inc.},
title = {{Nonlinear Inverse Reinforcement Learning with Gaussian Processes}},
url = {http://papers.nips.cc/paper/4420-nonlinear-inverse-reinforcement-learning-with-gaussian-processes.pdf},
year = {2011}
}
@inproceedings{conf/iclr/PeysakhovichL18,
author = {Peysakhovich, Alexander and Lerer, Adam},
booktitle = {ICLR (Poster)},
keywords = {dblp},
publisher = {OpenReview.net},
title = {{Consequentialist conditional cooperation in social dilemmas with imperfect information}},
url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2018.html{\#}PeysakhovichL18},
year = {2018}
}
@misc{0013583953:online,
annote = {(Accessed on 02/10/2022)},
author = {国土交通省},
howpublished = {https://www.mlit.go.jp/common/001358398.pdf},
title = {世界の港湾別コンテナ取扱個数ランキング}
}
@book{BA79120757p59,
author = {学, 赤川},
pages = {p.59},
publisher = {勁草書房},
title = {構築主義を再構築する},
url = {https://ci.nii.ac.jp/ncid/BA79120757},
year = {2006}
}
@techreport{Al-Bassam,
abstract = {Light clients, also known as Simple Payment Verification (SPV) clients, are nodes which only download a small portion of the data in a blockchain, and use indirect means to verify that a given chain is valid. Typically, instead of validating block data, they assume that the chain favoured by the blockchain's consensus algorithm only contains valid blocks, and that the majority of block producers are honest. By allowing such clients to receive fraud proofs generated by fully validating nodes that show that a block violates the protocol rules, and combining this with probabilistic sampling techniques to verify that all of the data in a block actually is available to be downloaded, we can eliminate the honest-majority assumption for block validity, and instead make much weaker assumptions about a minimum number of honest nodes that re-broadcast data. Fraud and data availability proofs are key to enabling on-chain scaling of blockchains (e.g., via sharding or bigger blocks) while maintaining a strong assurance that on-chain data is available and valid. We present, implement, and evaluate a novel fraud and data availability proof system.},
archivePrefix = {arXiv},
arxivId = {1809.09044v5},
author = {Al-Bassam, Mustafa and Sonnino, Alberto and Buterin, Vitalik},
eprint = {1809.09044v5},
file = {:Users/hikaruasano/Documents/mendeley/Al-Bassam, Sonnino, Buterin{\_}Unknown{\_}Fraud and Data Availability Proofs Maximising Light Client Security and Scaling Blockchains with Dis.pdf:pdf},
title = {{Fraud and Data Availability Proofs: Maximising Light Client Security and Scaling Blockchains with Dishonest Majorities}},
url = {https://arxiv.org/pdf/1809.09044.pdf}
}
@book{BC02891850,
author = {McIntyre, Lee C and 匠, 居村 and 智史, 大﨑 and 卓也, 西橋 and 完太郎, 大橋},
publisher = {人文書院},
title = {ポストトゥルース},
url = {https://ci.nii.ac.jp/ncid/BC02891850},
year = {2020}
}
@article{Chaloner1995BayesianED,
author = {Chaloner, Kathryn and Verdinelli, Isabella},
journal = {Statistical Science},
pages = {273--304},
title = {{Bayesian Experimental Design: A Review}},
volume = {10},
year = {1995}
}
@article{shiarlis2016inverse,
author = {Shiarlis, Kyriacos and Messias, Joao and Whiteson, S A},
file = {:Users/hikaruasano/Documents/mendeley/Shiarlis, Messias, Whiteson{\_}2016{\_}Inverse reinforcement learning from failure.pdf:pdf},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
title = {{Inverse reinforcement learning from failure}},
year = {2016}
}
@article{Li_Tinka_Kiesel_Durham_Kumar_Koenig_2021,
author = {Li, Jiaoyang and Tinka, Andrew and Kiesel, Scott and Durham, Joseph W and Kumar, T K Satish and Koenig, Sven},
file = {:Users/hikaruasano/Documents/mendeley/Li et al.{\_}2021{\_}Lifelong Multi-Agent Path Finding in Large-Scale Warehouses.pdf:pdf},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
month = {may},
number = {13},
pages = {11272--11281},
title = {{Lifelong Multi-Agent Path Finding in Large-Scale Warehouses}},
url = {https://ojs.aaai.org/index.php/AAAI/article/view/17344},
volume = {35},
year = {2021}
}
@article{Hashimoto,
author = {Hashimoto, Jun and Hashimoto, Tsuyoshi},
file = {:Users/hikaruasano/Documents/mendeley/Hashimoto, Hashimoto{\_}Unknown{\_}The Use of Killer Heuristics in Computer Shogi.pdf:pdf},
pages = {1--8},
title = {{The Use of Killer Heuristics in Computer Shogi}}
}
@inproceedings{fortunato2018noisy,
author = {Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Hessel, Matteo and Osband, Ian and Graves, Alex and Mnih, Volodymyr and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and Blundell, Charles and Legg, Shane},
booktitle = {International Conference on Learning Representations},
title = {{Noisy Networks For Exploration}},
url = {https://openreview.net/forum?id=rywHCPkAW},
year = {2018}
}
@misc{01107pdf4:online,
annote = {(Accessed on 07/13/2020)},
author = {資源エネルギー庁},
howpublished = {https://www.enecho.meti.go.jp/committee/council/basic{\_}policy{\_}subcommittee/mitoshi},
title = {長期エネルギー需給見通し関連資料},
year = {2015}
}
@inproceedings{Balduzzi2020Smooth,
author = {Balduzzi, David and Czarnecki, Wojciech M and Anthony, Tom and Gemp, Ian and Hughes, Edward and Leibo, Joel and Piliouras, Georgios and Graepel, Thore},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Balduzzi et al.{\_}2020{\_}Smooth markets A basic mechanism for organizing gradient-based learners.pdf:pdf},
title = {{Smooth markets: A basic mechanism for organizing gradient-based learners}},
url = {https://openreview.net/forum?id=B1xMEerYvB},
year = {2020}
}
@inproceedings{10.5555/3524938.3525854,
abstract = {The role concept provides a useful tool to design and understand complex multi-agent systems, which allows agents with a similar role to share similar behaviors. However, existing role-based methods use prior domain knowledge and predefine role structures and behaviors. In contrast, multi-agent reinforcement learning (MARL) provides flexibility and adaptability, but less efficiency in complex tasks. In this paper, we synergize these two paradigms and propose a role-oriented MARL framework (ROMA). In this framework, roles are emergent, and agents with similar roles tend to share their learning and to be specialized on certain sub-tasks. To this end, we construct a stochastic role embedding space by introducing two novel regularizers and conditioning individual policies on roles. Experiments show that our method can learn specialized, dynamic, and identifiable roles, which help our method push forward the state of the art on the StarCraft II micromanagement benchmark.},
author = {Wang, Tonghan and Dong, Heng and Lesser, Victor and Zhang, Chongjie},
booktitle = {nternational Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Wang et al.{\_}2020{\_}ROMA Multi-Agent Reinforcement Learning with Emergent Roles.pdf:pdf},
publisher = {JMLR.org},
series = {ICML'20},
title = {{ROMA: Multi-Agent Reinforcement Learning with Emergent Roles}},
year = {2020}
}
@article{Rusu2015PolicyD,
author = {Rusu, Andrei A and Colmenarejo, Sergio Gomez and G{\"{u}}l{\c{c}}ehre, {\c{C}}aglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
file = {:Users/hikaruasano/Documents/mendeley/Rusu et al.{\_}2015{\_}Policy Distillation.pdf:pdf},
journal = {CoRR},
title = {{Policy Distillation}},
volume = {abs/1511.0},
year = {2015}
}
@article{1572261552145233792,
author = {隆一, 指田},
issn = {1349-0850},
journal = {四天王寺国際仏教大学紀要},
number = {45},
pages = {147--161},
publisher = {四天王寺国際仏教大学},
title = {社会学における理論構成の方法},
url = {https://cir.nii.ac.jp/crid/1572261552145233792},
year = {2007}
}
@article{eccles2019learning,
author = {Eccles, Tom and Hughes, Edward and Kram{\'{a}}r, J{\'{a}}nos and Wheelwright, Steven and Leibo, Joel Z},
file = {:Users/hikaruasano/Documents/mendeley/Eccles et al.{\_}2019{\_}Learning reciprocity in complex sequential social dilemmas.pdf:pdf},
journal = {arXiv preprint arXiv:1903.08082},
title = {{Learning reciprocity in complex sequential social dilemmas}},
year = {2019}
}
@misc{資60:online,
annote = {(Accessed on 07/15/2020)},
author = {東京都住宅政策本部},
title = {空き家の現状と取組【資料集】},
year = {2015}
}
@article{Anthony2017,
abstract = {Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (EXIT), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that EXIT outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MOHEX 1.0, the most recent Olympiad Champion player to be publicly released.},
archivePrefix = {arXiv},
arxivId = {1705.08439},
author = {Anthony, Thomas and Tian, Zheng and Barber, David},
doi = {10.1586/14789450.2016.1116391},
eprint = {1705.08439},
file = {:Users/hikaruasano/Documents/mendeley/Anthony, Tian, Barber{\_}2017{\_}Estados{\_}2012.pdf:pdf},
isbn = {0002-9165 (Print)$\backslash$r0002-9165 (Linking)},
issn = {10495258},
number = {Il},
pages = {1--11},
pmid = {26258699},
title = {{Estados{\_}2012}},
year = {2017}
}
@book{Dathan2015,
address = {Cham},
author = {Dathan, Brahma and Ramnath, Sarnath},
doi = {10.1007/978-3-319-24280-4},
file = {:Users/hikaruasano/Documents/mendeley/Dathan, Ramnath{\_}2015{\_}Object-Oriented Analysis, Design and Implementation.pdf:pdf},
isbn = {978-3-319-24278-1},
publisher = {Springer International Publishing},
series = {Undergraduate Topics in Computer Science},
title = {{Object-Oriented Analysis, Design and Implementation}},
url = {http://link.springer.com/10.1007/978-3-319-24280-4},
year = {2015}
}
@inproceedings{Long*2020Evolutionary,
author = {Long*, Qian and Zhou*, Zihan and Gupta, Abhinav and Fang, Fei and Wu†, Yi and Wang†, Xiaolong},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Long et al.{\_}2020{\_}Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning.pdf:pdf},
title = {{Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning}},
url = {https://openreview.net/forum?id=SJxbHkrKDH},
year = {2020}
}
@inproceedings{shum2019theory,
author = {Shum, Michael and Kleiman-Weiner, Max and Littman, Michael L and Tenenbaum, Joshua B},
booktitle = {Proceedings of the AAAI conference on artificial intelligence},
file = {:Users/hikaruasano/Documents/mendeley/Shum et al.{\_}2019{\_}Theory of minds Understanding behavior in groups through inverse planning.pdf:pdf},
number = {01},
pages = {6163--6170},
title = {{Theory of minds: Understanding behavior in groups through inverse planning}},
volume = {33},
year = {2019}
}
@misc{FastFact56:online,
annote = {(Accessed on 07/22/2020)},
author = {Agency, United States Environmental Protection},
howpublished = {https://www.epa.gov/greenvehicles/fast-facts-transportation-greenhouse-gas-emissions},
title = {{Fast Facts on Transportation Greenhouse Gas Emissions}},
year = {2020}
}
@article{inala2020neurosymbolic,
author = {Inala, Jeevana Priya and Yang, Yichen and Paulos, James and Pu, Yewen and Bastani, Osbert and Kumar, Vijay and Rinard, Martin and Solar-Lezama, Armando},
file = {:Users/hikaruasano/Documents/mendeley/Inala et al.{\_}2020{\_}Neurosymbolic transformers for multi-agent communication.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {13597--13608},
title = {{Neurosymbolic transformers for multi-agent communication}},
volume = {33},
year = {2020}
}
@article{yamauchi2023efficient,
author = {Yamauchi, Tomoki and Miyashita, Yuki and Sugawara, Toshiharu},
file = {:Users/hikaruasano/Documents/mendeley/Yamauchi, Miyashita, Sugawara{\_}2023{\_}Efficient Path and Action Planning Method for Multi-Agent Pickup and Delivery Tasks under Environment.pdf:pdf},
journal = {SN Computer Science},
number = {1},
pages = {1--20},
publisher = {Springer},
title = {{Efficient Path and Action Planning Method for Multi-Agent Pickup and Delivery Tasks under Environmental Constraints}},
volume = {4},
year = {2023}
}
@inproceedings{10.5555/3298483.3298527,
abstract = {We propose an algorithm for enumerating solutions to the Lasso regression problem. In ordinary Lasso regression, one global optimum is obtained and the resulting features are interpreted as task-relevant features. However, this can overlook possibly relevant features not selected by the Lasso. With the proposed method, we can enumerate many possible feature sets for human inspection, thus recording all the important features. We prove that by enumerating solutions, we can recover a true feature set exactly under less restrictive conditions compared with the ordinary Lasso. We confirm our theoretical results also in numerical simulations. Finally, in the gene expression and the text data, we demonstrate that the proposed method can enumerate a wide variety of meaningful feature sets, which are overlooked by the global optima.},
author = {Hara, Satoshi and Maehara, Takanori},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {1985--1991},
publisher = {AAAI Press},
series = {AAAI'17},
title = {{Enumerate Lasso Solutions for Feature Selection}},
year = {2017}
}
@article{doi:10.1177/1749975509105533,
abstract = { This article explores a performative understanding of social science method. First, it draws on STS to consider the plausibility of the claim that research methods generate not only representations of reality, but also the realities those representations depict. Second, it undertakes an archaeology of a major survey — a Eurobarometer investigation of European citizens' attitudes to farm animal welfare — in order to explore the character of its performativity. Finally, it considers some of the implications of the performativity of research tools for the future of methods in social science. },
author = {Law, John},
doi = {10.1177/1749975509105533},
journal = {Cultural Sociology},
number = {2},
pages = {239--256},
title = {{Seeing Like a Survey}},
url = {https://doi.org/10.1177/1749975509105533},
volume = {3},
year = {2009}
}
@article{article,
author = {Kar, Soummya and Moura, Jose and Poor, H Vincent},
doi = {10.1109/TSP.2013.2241057},
journal = {IEEE Transactions on Signal Processing},
title = {{{\$}QD{\$}-Learning: A Collaborative Distributed Strategy for Multi-Agent Reinforcement Learning Through Consensus + Innovations}},
volume = {61},
year = {2012}
}
@misc{平成27年国勢調77:online,
annote = {(Accessed on 06/28/2020)},
author = {東京都総務局統計部},
howpublished = {https://www.toukei.metro.tokyo.lg.jp/kokutyo/2015/kt-15index1.htm},
title = {平成27年国勢調査 人口等基本集計結果概要}
}
@article{Arai2014,
abstract = {This study is intended to encourage appropriate social norms among multiple agents. Effective norms, such as those emerging from sustained individual interactions over time, can make agents act cooperatively to optimize their performance. We introduce a "social learning" model in which agents mutually interact under a framework of the coordination game. Because coordination games have dual equilibria, social norms are necessary to make agents converge to a unique equilibrium. As described in this paper, we present the emergence of a right social norm by inverse reinforcement learning, which is an approach for extracting a reward function from the observation of optimal behaviors. First, we let a mediator agent estimate the reward function by inverse reinforcement learning from the observation of a master's behavior. Secondly, we introduce agents who act according to an estimated reward function in the multiagent world in which most agents, called citizens, have no way to act. Finally, we evaluate the effectiveness of introducing inverse reinforcement learning.},
author = {Arai, Sachiyo and Suzuki, Kanako},
doi = {10.2197/ipsjjip.22.299},
file = {:Users/hikaruasano/Documents/mendeley/Arai, Suzuki{\_}2014{\_}Encouragement of Right Social Norms by Inverse Reinforcement Learning.pdf:pdf},
journal = {Journal of Information Processing},
keywords = {inverse reinforcement learning,social norms},
pages = {299--306},
title = {{Encouragement of Right Social Norms by Inverse Reinforcement Learning}},
url = {https://gateway.itc.u-tokyo.ac.jp/article/ipsjjip/22/2/22{\_}299/{\_}article/-char/ja/,DanaInfo=www.jstage.jst.go.jp},
volume = {22},
year = {2014}
}
@misc{Google2041:online,
annote = {(Accessed on 07/20/2020)},
author = {Google},
howpublished = {https://sustainability.google/reports/environmental-report-2019},
title = {{Google 2019 Environmental Web Report}},
year = {2019}
}
@article{2018,
author = {川俣 and 雅弘},
issn = {0026-6760},
journal = {三田学会雑誌 = Mita journal of economics},
keywords = {Marginal Revolution,price mechanism,principles of rational behavior,theory of utility and scarcity,value and price,価値と価格,価格メカニズム,効用と希少性の理論,合理的行動原理,限界革命},
number = {3},
pages = {325--359},
publisher = {慶應義塾経済学会},
title = {限界革命にかんする再考察},
url = {http://ci.nii.ac.jp/naid/120006726373/ja/},
volume = {111},
year = {2018}
}
@misc{トランプ米大統領5:online,
annote = {(Accessed on 07/09/2021)},
author = {BBCニュース},
howpublished = {https://www.bbc.com/japanese/46354080（2021年7月9日）},
title = {トランプ米大統領、米政府の気候変動報告「信じない」},
year = {2018}
}
@article{giles2013multilevel,
author = {Giles, Michael B},
file = {:Users/hikaruasano/Documents/mendeley/Giles{\_}2013{\_}Multilevel monte carlo methods.pdf:pdf},
journal = {Monte Carlo and Quasi-Monte Carlo Methods 2012},
pages = {83--103},
publisher = {Springer},
title = {{Multilevel monte carlo methods}},
year = {2013}
}
@inproceedings{hara2018approximate,
author = {Hara, Satoshi and Ishihata, Masakazu},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
number = {1},
title = {{Approximate and exact enumeration of rule models}},
volume = {32},
year = {2018}
}
@article{Hasson2019,
abstract = {Evolution is a blind fitting process by which organisms, over generations, adapt to the niches of an ever-changing environment. Does the mammalian brain use similar brute-force fitting processes to learn how to perceive and act upon the world? Recent advances in training deep neural networks has exposed the power of optimizing millions of synaptic weights to map millions of observations along ecologically relevant objective functions. This class of models has dramatically outstripped simpler, more intuitive models, operating robustly in real-life contexts spanning perception, language, and action coordination. These models do not learn an explicit, human-interpretable representation of the underlying structure of the data; rather, they use local computations to interpolate over task-relevant manifolds in a high-dimensional parameter space. Furthermore, counterintuitively, over-parameterized models, similarly to evolutionary processes, can be simple and parsimonious as they provide a versatile, robust solution for learning a diverse set of functions. In contrast to traditional scientific models, where the ultimate goal is interpretability, over-parameterized models eschew interpretability in favor of solving real-life problems or tasks. We contend that over-parameterized blind fitting presents a radical challenge to many of the underlying assumptions and practices in computational neuroscience and cognitive psychology. At the same time, this shift in perspective informs longstanding debates and establishes unexpected links with evolution, ecological psychology, and artificial life.},
author = {Hasson, Uri and Nastase, Samuel and Goldstein, Ariel},
doi = {10.1101/764258},
file = {:Users/hikaruasano/Documents/mendeley/Hasson, Nastase, Goldstein{\_}2019{\_}Direct-fit to nature an evolutionary perspective on biological (and artificial) neural networks.pdf:pdf},
journal = {Neuron},
title = {{Direct-fit to nature: an evolutionary perspective on biological (and artificial) neural networks}},
year = {2019}
}
@book{BC02459350,
author = {公三, 渡辺},
publisher = {講談社},
series = {講談社学術文庫},
title = {レヴィ=ストロース : 構造},
url = {https://ci.nii.ac.jp/ncid/BC02459350},
year = {2020}
}
@inproceedings{han2022variational,
author = {Han, Dongqi and Kozuno, Tadashi and Luo, Xufang and Chen, Zhao-Yun and Doya, Kenji and Yang, Yuqing and Li, Dongsheng},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Han et al.{\_}2022{\_}Variational oracle guiding for reinforcement learning.pdf:pdf},
title = {{Variational oracle guiding for reinforcement learning}},
url = {https://openreview.net/forum?id=pjqqxepwoMy},
year = {2022}
}
@article{199572,
author = {達夫, 正田},
doi = {10.5874/jfsr.2.2_72},
journal = {フードシステム研究},
number = {2},
pages = {72--87},
title = {チーズ産業形成期のマーケティング},
volume = {2},
year = {1995}
}
@inproceedings{10.1145/3442188.3445912,
abstract = {Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.},
address = {New York, NY, USA},
author = {Finocchiaro, Jessie and Maio, Roland and Monachou, Faidra and Patro, Gourab K and Raghavan, Manish and Stoica, Ana-Andreea and Tsirtsis, Stratis},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
doi = {10.1145/3442188.3445912},
file = {:Users/hikaruasano/Documents/mendeley/Finocchiaro et al.{\_}2021{\_}Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness.pdf:pdf},
isbn = {9781450383097},
pages = {489--503},
publisher = {Association for Computing Machinery},
series = {FAccT '21},
title = {{Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness}},
url = {https://doi.org/10.1145/3442188.3445912},
year = {2021}
}
@book{BB21960374,
author = {岳彦, 大黒},
publisher = {勁草書房},
title = {情報社会の「哲学」 : グーグル・ビッグデータ・人工知能},
url = {https://ci.nii.ac.jp/ncid/BB21960374},
year = {2016}
}
@misc{菅首相2030年19:online,
annote = {(Accessed on 06/15/2021)},
author = {NHK},
howpublished = {https://www3.nhk.or.jp/news/html/20210422/k10012991191000.html},
title = {{菅首相 2030年の温室効果ガス目標 2013年度比46％削減を表明 | 環境 | NHKニュース}},
year = {2021}
}
@techreport{Sagehashi2017,
abstract = {We examined the effect of pruning in multiple turnbased strategy game that have multiple unit movement in trurn. Experiences is done with TUBSTAP which is a common platform. Turnbased strategy game has more than 100 million branching factors per turn. It is practically impossible to search a complete two or more turns. Therefore, effective pruning is indispensable also in the Monte Carlo search. In this research, in particular, we studied a method of pruning based on unit attack behavior. For experiments, a player that reduces attack behavior to only representative one was used. As a result, within the range of the experimented conditions, there was no effect of increasing the win rate. From this it was shown that the formation of the posterior unit is more important than the magnitude of the attack effect.},
author = {Sagehashi, Rin and Nishino, Junji},
file = {:Users/hikaruasano/Documents/mendeley/Sagehashi, Nishino{\_}2017{\_}Attack Action Pruning in Unit based UCT for TUBSTAP.pdf:pdf},
title = {{Attack Action Pruning in Unit based UCT for TUBSTAP}},
year = {2017}
}
@inproceedings{henderson2018deep,
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
booktitle = {Proceedings of the AAAI conference on artificial intelligence},
number = {1},
title = {{Deep reinforcement learning that matters}},
volume = {32},
year = {2018}
}
@inproceedings{chen2021decision,
author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Beygelzimer, A and Dauphin, Y and Liang, P and Vaughan, J Wortman},
file = {:Users/hikaruasano/Documents/mendeley/Chen et al.{\_}2021{\_}Decision Transformer Reinforcement Learning via Sequence Modeling.pdf:pdf},
title = {{Decision Transformer: Reinforcement Learning via Sequence Modeling}},
url = {https://openreview.net/forum?id=a7APmM4B9d},
year = {2021}
}
@article{Ryan2015FullyBE,
author = {Ryan, Elizabeth G and Drovandi, C and Pettitt, A},
journal = {Entropy},
pages = {1063--1089},
title = {{Fully Bayesian Experimental Design for Pharmacokinetic Studies}},
volume = {17},
year = {2015}
}
@article{Brennan2007CalculatingPE,
author = {Brennan, A and Kharroubi, S and O'Hagan, A and Chilcott, J},
file = {:Users/hikaruasano/Documents/mendeley/Brennan et al.{\_}2007{\_}Calculating Partial Expected Value of Perfect Information via Monte Carlo Sampling Algorithms.pdf:pdf},
journal = {Medical Decision Making},
pages = {448--470},
title = {{Calculating Partial Expected Value of Perfect Information via Monte Carlo Sampling Algorithms}},
volume = {27},
year = {2007}
}
@techreport{IEA2020,
author = {IEA},
pages = {15},
title = {{World Energy Balances}},
year = {2020}
}
@article{vaisanen2020lifestyle,
author = {V{\"{a}}is{\"{a}}nen, Daniel and Kallings, Lena V and Andersson, Gunnar and Wallin, Peter and Hemmingsson, Erik and Ekblom-Bak, Elin},
journal = {BMC public health},
number = {1},
pages = {1--13},
publisher = {BioMed Central},
title = {{Lifestyle-associated health risk indicators across a wide range of occupational groups: a cross-sectional analysis in 72,855 workers}},
volume = {20},
year = {2020}
}
@article{espeholt2018impala,
author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Others},
file = {:Users/hikaruasano/Documents/mendeley/Espeholt et al.{\_}2018{\_}Impala Scalable distributed deep-rl with importance weighted actor-learner architectures.pdf:pdf},
journal = {arXiv preprint arXiv:1802.01561},
title = {{Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures}},
year = {2018}
}
@article{1390282679270679936,
author = {桜井, 厚},
doi = {10.4057/jsr.53.4_452},
file = {:Users/hikaruasano/Documents/mendeley/桜井{\_}2003{\_}社会調査の困難.pdf:pdf},
issn = {00215414},
journal = {社会学評論},
number = {4},
pages = {452--470},
publisher = {日本社会学会},
title = {社会調査の困難},
url = {https://cir.nii.ac.jp/crid/1390282679270679936},
volume = {53},
year = {2003}
}
@inproceedings{pmlr-v28-zemel13,
abstract = {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a whole), and individual fairness (similar individuals should be treated similarly). We formulate fairness as an optimization problem of finding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group. We show positive results of our algorithm relative to other known techniques, on three datasets. Moreover, we demonstrate several advantages to our approach. First, our intermediate representation can be used for other classification tasks (i.e., transfer learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.},
address = {Atlanta, Georgia, USA},
author = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
editor = {Dasgupta, Sanjoy and McAllester, David},
file = {:Users/hikaruasano/Documents/mendeley/Zemel et al.{\_}2013{\_}Learning Fair Representations.pdf:pdf},
number = {3},
pages = {325--333},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Learning Fair Representations}},
url = {https://proceedings.mlr.press/v28/zemel13.html},
volume = {28},
year = {2013}
}
@inproceedings{Lopes2009,
abstract = {Inverse reinforcement learning addresses the general problem of recovering a reward function from samples of a policy provided by an expert/demonstrator. In this paper, we introduce active learning for inverse reinforcement learning. We propose an algorithm that allows the agent to query the demonstrator for samples at specific states, instead of relying only on samples provided at "arbitrary" states. The purpose of our algorithm is to estimate the reward function with similar accuracy as other methods from the literature while reducing the amount of policy samples required from the expert. We also discuss the use of our algorithm in higher dimensional problems, using both Monte Carlo and gradient methods. We present illustrative results of our algorithm in several simulated examples of different complexities. {\textcopyright} 2009 Springer Berlin Heidelberg.},
author = {Lopes, Manuel and Melo, Francisco and Montesano, Luis},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-04174-7_3},
file = {:Users/hikaruasano/Documents/mendeley/Lopes, Melo, Montesano{\_}2009{\_}Active learning for reward estimation in inverse reinforcement learning.pdf:pdf},
isbn = {3642041736},
issn = {03029743},
number = {PART 2},
pages = {31--46},
title = {{Active learning for reward estimation in inverse reinforcement learning}},
volume = {5782 LNAI},
year = {2009}
}
@article{liu2014multiobjective,
author = {Liu, Chunming and Xu, Xin and Hu, Dewen},
file = {:Users/hikaruasano/Documents/mendeley/Liu, Xu, Hu{\_}2014{\_}Multiobjective reinforcement learning A comprehensive overview.pdf:pdf},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
number = {3},
pages = {385--398},
publisher = {IEEE},
title = {{Multiobjective reinforcement learning: A comprehensive overview}},
volume = {45},
year = {2014}
}
@article{smith2022walk,
author = {Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
file = {:Users/hikaruasano/Documents/mendeley/Smith, Kostrikov, Levine{\_}2022{\_}A Walk in the Park Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning.pdf:pdf},
journal = {arXiv preprint arXiv:2208.07860},
title = {{A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning}},
year = {2022}
}
@article{GuardtimeFederal2017,
abstract = {The use of blockchain technology to provide digital integrity has exploded in financial applications. Widely proliferated blockchain technology consumes significant storage, communications bandwidth and time for each transaction, whereas Guardtime-Federal, LLC provides Keyless Signature Infrastructure (KSI) that is a blockchain designed for security, scalability and speed. Through the properties of verifiable authenticity, identity of the client, and non-global positioning system-based non-spoofable time; KSI provides provenance, integrity and identity associated with digital assets. This implementation consumes far less storage and bandwidth than widely proliferated blockchain technology and can provide the above defined attributes for thousands of files a second scalable to billions. KSI cryptographically links data assets with immutable properties provided by the KSI infrastructure, and implemented in a KSI signature. KSI promises this additional integrity and authenticity in newly created or already existing data whether on the network, in embedded systems or traversing the cloud. Customers who implement KSI can prove their current and future information systems are in a truthful state and meet business and mission needs with increased security. KSI was designed with considerations for security, scalability, and speed to meet the requirements for a host of complex applications.},
author = {{Guardtime Federal}},
file = {:Users/hikaruasano/Documents/mendeley/Guardtime Federal{\_}2017{\_}Keyless Signature Infrastructure (KSI).pdf:pdf},
pages = {1--8},
title = {{Keyless Signature Infrastructure (KSI)}},
url = {http://blockchain.machetemag.com/wp-content/uploads/2017/11/Guardtime{\_}WhitePaper{\_}KSI.pdf},
year = {2017}
}
@article{Lindley1956OnAM,
author = {Lindley, David},
journal = {Annals of Mathematical Statistics},
pages = {986--1005},
title = {{On a Measure of the Information Provided by an Experiment}},
volume = {27},
year = {1956}
}
@article{holland1986statistics,
author = {Holland, Paul W},
journal = {Journal of the American statistical Association},
number = {396},
pages = {945--960},
publisher = {Taylor {\&} Francis},
title = {{Statistics and causal inference}},
volume = {81},
year = {1986}
}
@inproceedings{tan1993multi,
author = {Tan, Ming},
booktitle = {Proceedings of the tenth international conference on machine learning},
file = {:Users/hikaruasano/Documents/mendeley/Tan{\_}1993{\_}Multi-agent reinforcement learning Independent vs. cooperative agents.pdf:pdf},
pages = {330--337},
title = {{Multi-agent reinforcement learning: Independent vs. cooperative agents}},
year = {1993}
}
@inproceedings{ota2020efficient,
author = {Ota, Kei and Sasaki, Yoko and Jha, Devesh K and Yoshiyasu, Yusuke and Kanezaki, Asako},
booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
file = {:Users/hikaruasano/Documents/mendeley/Ota et al.{\_}2020{\_}Efficient exploration in constrained environments with goal-oriented reference path.pdf:pdf},
organization = {IEEE},
pages = {6061--6068},
title = {{Efficient exploration in constrained environments with goal-oriented reference path}},
year = {2020}
}
@article{Dror2008SequentialED,
author = {Dror, Hovav A and Steinberg, D M},
file = {:Users/hikaruasano/Documents/mendeley/Dror, Steinberg{\_}2008{\_}Sequential Experimental Designs for Generalized Linear Models.pdf:pdf},
journal = {Journal of the American Statistical Association},
pages = {288--298},
title = {{Sequential Experimental Designs for Generalized Linear Models}},
volume = {103},
year = {2008}
}
@inproceedings{10.5555/3295222.3295230,
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
address = {Red Hook, NY, USA},
author = {Lundberg, Scott M and Lee, Su-In},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
isbn = {9781510860964},
pages = {4768--4777},
publisher = {Curran Associates Inc.},
series = {NIPS'17},
title = {{A Unified Approach to Interpreting Model Predictions}},
year = {2017}
}
@inproceedings{NEURIPS2021_80fee67c,
author = {Lin, Toru and Huh, Jacob and Stauffer, Christopher and Lim, Ser Nam and Isola, Phillip},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Ranzato, M and Beygelzimer, A and Dauphin, Y and Liang, P S and Vaughan, J Wortman},
file = {:Users/hikaruasano/Documents/mendeley/Lin et al.{\_}2021{\_}Learning to Ground Multi-Agent Communication with Autoencoders.pdf:pdf},
pages = {15230--15242},
publisher = {Curran Associates, Inc.},
title = {{Learning to Ground Multi-Agent Communication with Autoencoders}},
url = {https://proceedings.neurips.cc/paper/2021/file/80fee67c8a4c4989bf8a580b4bbb0cd2-Paper.pdf},
volume = {34},
year = {2021}
}
@book{BB30262266,
author = {Steele, Claude and 朝子, 藤原 and 英哉, 北村},
publisher = {英治出版},
title = {ステレオタイプの科学 : 「社会の刷り込み」は成果にどう影響し、わたしたちは何ができるのか},
url = {https://ci.nii.ac.jp/ncid/BB30262266},
year = {2020}
}
@inproceedings{liu2018competitive,
author = {Liu, Hao and Trott, Alexander and Socher, Richard and Xiong, Caiming},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Liu et al.{\_}2019{\_}Competitive experience replay.pdf:pdf},
title = {{Competitive experience replay}},
url = {https://openreview.net/forum?id=Sklsm20ctX},
year = {2019}
}
@article{barekatain2019multipolar,
author = {Barekatain, Mohammadamin and Yonetani, Ryo and Hamaya, Masashi},
file = {:Users/hikaruasano/Documents/mendeley/Barekatain, Yonetani, Hamaya{\_}2019{\_}MULTIPOLAR Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environ.pdf:pdf},
journal = {arXiv preprint arXiv:1909.13111},
title = {{MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics}},
year = {2019}
}
@article{huang2019mapping,
author = {Huang, Zhiao and Liu, Fangchen and Su, Hao},
file = {:Users/hikaruasano/Documents/mendeley/Huang, Liu, Su{\_}2019{\_}Mapping state space using landmarks for universal goal reaching(2).pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Mapping state space using landmarks for universal goal reaching}},
volume = {32},
year = {2019}
}
@inproceedings{foerster2018learning,
author = {Foerster, Jakob and Chen, Richard Y and Al-Shedivat, Maruan and Whiteson, Shimon and Abbeel, Pieter and Mordatch, Igor},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
file = {:Users/hikaruasano/Documents/mendeley/Foerster et al.{\_}2018{\_}Learning with opponent-learning awareness.pdf:pdf},
organization = {International Foundation for Autonomous Agents and Multiagent Systems},
pages = {122--130},
title = {{Learning with opponent-learning awareness}},
year = {2018}
}
@inproceedings{9341303,
author = {Sun, Chuangchuang and Shen, Macheng and How, Jonathan P},
booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS45743.2020.9341303},
file = {:Users/hikaruasano/Documents/mendeley/Sun, Shen, How{\_}2020{\_}Scaling Up Multiagent Reinforcement Learning for Robotic Systems Learn an Adaptive Sparse Communication Graph.pdf:pdf},
pages = {11755--11762},
title = {{Scaling Up Multiagent Reinforcement Learning for Robotic Systems: Learn an Adaptive Sparse Communication Graph}},
year = {2020}
}
@misc{5461726F75:online,
annote = {(Accessed on 07/15/2020)},
author = {国土交通省},
howpublished = {https://www.mlit.go.jp/singikai/koutusin/rikujou/jidosha/taxi/01/images/05.pdf},
title = {タクシー事業の実態},
year = {2016}
}
@inproceedings{pmlr-v162-li22l,
abstract = {Value decomposition (VD) methods have been widely used in cooperative multi-agent reinforcement learning (MARL), where credit assignment plays an important role in guiding the agents' decentralized execution. In this paper, we investigate VD from a novel perspective of causal inference. We first show that the environment in existing VD methods is an unobserved confounder as the common cause factor of the global state and the joint value function, which leads to the confounding bias on learning credit assignment. We then present our approach, deconfounded value decomposition (DVD), which cuts off the backdoor confounding path from the global state to the joint value function. The cut is implemented by introducing the {\textless}em{\textgreater}trajectory graph{\textless}/em{\textgreater}, which depends only on the local trajectories, as a proxy confounder. DVD is general enough to be applied to various VD methods, and extensive experiments show that DVD can consistently achieve significant performance gains over different state-of-the-art VD methods on StarCraft II and MACO benchmarks.},
author = {Li, Jiahui and Kuang, Kun and Wang, Baoxiang and Liu, Furui and Chen, Long and Fan, Changjie and Wu, Fei and Xiao, Jun},
booktitle = {Proceedings of the 39th International Conference on Machine Learning},
editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
pages = {12843--12856},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Deconfounded Value Decomposition for Multi-Agent Reinforcement Learning}},
url = {https://proceedings.mlr.press/v162/li22l.html},
volume = {162},
year = {2022}
}
@article{weko_48_1,
author = {杉原, 喜代美 and 市江, 和子},
journal = {看護学研究紀要(E-ISSN 2434-7566)},
month = {mar},
number = {1},
pages = {11--20},
title = {核家族で妊娠・育児期にある母親の睡眠・疲労の状況 睡眠日誌の自由記述からの内容分析},
volume = {1},
year = {2013}
}
@inproceedings{kallus2018residual,
author = {Kallus, Nathan and Zhou, Angela},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Kallus, Zhou{\_}2018{\_}Residual unfairness in fair machine learning from prejudiced data.pdf:pdf},
organization = {PMLR},
pages = {2439--2448},
title = {{Residual unfairness in fair machine learning from prejudiced data}},
year = {2018}
}
@article{wang2018evolving,
author = {Wang, Jane X and Hughes, Edward and Fernando, Chrisantha and Czarnecki, Wojciech M and Du{\'{e}}{\~{n}}ez-Guzm{\'{a}}n, Edgar A and Leibo, Joel Z},
file = {:Users/hikaruasano/Documents/mendeley/Wang et al.{\_}2018{\_}Evolving intrinsic motivations for altruistic behavior.pdf:pdf},
journal = {arXiv preprint arXiv:1811.05931},
title = {{Evolving intrinsic motivations for altruistic behavior}},
year = {2018}
}
@article{Schulman2017ProximalPO,
author = {Schulman, J and Wolski, F and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
journal = {ArXiv},
title = {{Proximal Policy Optimization Algorithms}},
volume = {abs/1707.0},
year = {2017}
}
@inproceedings{perolat2021poincare,
author = {Perolat, Julien and Munos, Remi and Lespiau, Jean-Baptiste and Omidshafiei, Shayegan and Rowland, Mark and Ortega, Pedro and Burch, Neil and Anthony, Thomas and Balduzzi, David and {De Vylder}, Bart and Others},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Perolat et al.{\_}2021{\_}From Poincar{\'{e}} recurrence to convergence in imperfect information games Finding equilibrium via regularization.pdf:pdf},
organization = {PMLR},
pages = {8525--8535},
title = {{From Poincar{\'{e}} recurrence to convergence in imperfect information games: Finding equilibrium via regularization}},
year = {2021}
}
@article{lucas2013considerations,
author = {Lucas, Jeffrey W and Morrell, Kevin and Posard, Marek},
file = {:Users/hikaruasano/Documents/mendeley/Lucas, Morrell, Posard{\_}2013{\_}Considerations on the ‘replication problem'in sociology.pdf:pdf},
journal = {The American Sociologist},
number = {2},
pages = {217--232},
publisher = {Springer},
title = {{Considerations on the ‘replication problem'in sociology}},
volume = {44},
year = {2013}
}
@inproceedings{Kaiser2020Model,
author = {Kaiser, {\L}ukasz and Babaeizadeh, Mohammad and Mi{\l}os, Piotr and Osi{\'{n}}ski, B{\l}a{\.{z}}ej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Mohiuddin, Afroz and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Kaiser et al.{\_}2020{\_}Model Based Reinforcement Learning for Atari.pdf:pdf},
title = {{Model Based Reinforcement Learning for Atari}},
url = {https://openreview.net/forum?id=S1xCPJHtDB},
year = {2020}
}
@book{BB29389394,
author = {Pinker, Steven and 明美, 橘 and 雪子, 坂田},
pages = {pp.311--314},
publisher = {草思社},
title = {21世紀の啓蒙 : 理性、科学、ヒューマニズム、進歩 下},
url = {https://ci.nii.ac.jp/ncid/BB29389394},
year = {2019}
}
@inproceedings{nalisnick2016improving,
abstract = {This paper investigates the popular neural word embedding method Word2vec as a source of evidence in document ranking. In contrast to NLP applications of word2vec, which tend to use only the input embeddings, we retain both the input and the output embeddings, allowing us to calculate a different word similarity that may be more suitable for document ranking. We map the query words into the input space and the document words into the output space, and compute a relevance score by aggregating the cosine similarities across all the query-document word pairs. We postulate that the proposed Dual Embedding Space Model (DESM) provides evidence that a document is about a query term, in addition to and complementing the traditional term frequency based approach.},
author = {Nalisnick, Eric and Mitra, Bhaskar and Craswell, Nick and Caruana, Rich},
booktitle = {WWW'16},
edition = {WWW'16},
month = {apr},
publisher = {WWW - World Wide Web Consortium (W3C)},
title = {{Improving Document Ranking with Dual Word Embeddings}},
url = {https://www.microsoft.com/en-us/research/publication/improving-document-ranking-with-dual-word-embeddings/},
year = {2016}
}
@inproceedings{NEURIPS2020_7967cc8e,
author = {Christianos, Filippos and Sch{\"{a}}fer, Lukas and Albrecht, Stefano},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Larochelle, H and Ranzato, M and Hadsell, R and Balcan, M F and Lin, H},
pages = {10707--10717},
publisher = {Curran Associates, Inc.},
title = {{Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning}},
url = {https://proceedings.neurips.cc/paper/2020/file/7967cc8e3ab559e68cc944c44b1cf3e8-Paper.pdf},
volume = {33},
year = {2020}
}
@inproceedings{pmlr-v119-jiang20b,
abstract = {Finite-horizon sequential experimental design (SED) arises naturally in many contexts, including hyperparameter tuning in machine learning among more traditional settings. Computing the optimal policy for such problems requires solving Bellman equations, which are generally intractable. Most existing work resorts to severely myopic approximations by limiting the decision horizon to only a single time-step, which can underweight exploration in favor of exploitation. We present BINOCULARS: Batch-Informed NOnmyopic Choices, Using Long-horizons for Adaptive, Rapid SED, a general framework for deriving efficient, nonmyopic approximations to the optimal experimental policy. Our key idea is simple and surprisingly effective: we first compute a one-step optimal batch of experiments, then select a single point from this batch to evaluate. We realize BINOCULARS for Bayesian optimization and Bayesian quadrature – two notable example problems with radically different objectives – and demonstrate that BINOCULARS significantly outperforms significantly outperforms myopic alternatives in real-world scenarios.},
author = {Jiang, Shali and Chai, Henry and Gonzalez, Javier and Garnett, Roman},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
editor = {III, Hal Daum{\'{e}} and Singh, Aarti},
pages = {4794--4803},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{{\{}BINOCULARS{\}} for efficient, nonmyopic sequential experimental design}},
url = {https://proceedings.mlr.press/v119/jiang20b.html},
volume = {119},
year = {2020}
}
@techreport{Claus1998,
abstract = {Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-learning in cooperative multiagent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (opti-mal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.},
author = {Claus, Caroline and Boutilier, Craig},
booktitle = {aaai.org},
file = {:Users/hikaruasano/Documents/mendeley/Claus, Boutilier{\_}1998{\_}The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems.pdf:pdf},
title = {{The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems}},
url = {www.aaai.org},
year = {1998}
}
@inproceedings{NIPS2016_9d268236,
author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Lee, D and Sugiyama, M and Luxburg, U and Guyon, I and Garnett, R},
file = {:Users/hikaruasano/Documents/mendeley/Hardt et al.{\_}2016{\_}Equality of Opportunity in Supervised Learning.pdf:pdf},
publisher = {Curran Associates, Inc.},
title = {{Equality of Opportunity in Supervised Learning}},
url = {https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf},
volume = {29},
year = {2016}
}
@article{article,
author = {Zhang, Kaiqing and Yang, Zhuoran and Liu, Han and Zhang, Tong and Başar, Tamer},
file = {:Users/hikaruasano/Documents/mendeley/Zhang et al.{\_}2018{\_}Fully decentralized multi-agent reinforcement learning with networked agents.pdf:pdf},
title = {{Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents}},
year = {2018}
}
@article{chow2019lyapunov,
author = {Chow, Yinlam and Nachum, Ofir and Faust, Aleksandra and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
file = {:Users/hikaruasano/Documents/mendeley/Chow et al.{\_}2019{\_}Lyapunov-based safe policy optimization for continuous control.pdf:pdf},
journal = {International Conference on Learning Representations},
title = {{Lyapunov-based safe policy optimization for continuous control}},
year = {2019}
}
@article{10.1214/aos/1032526965,
author = {Dette, Holger},
doi = {10.1214/aos/1032526965},
journal = {The Annals of Statistics},
keywords = {Bayesian {\$}D{\$}-optimal designs,Elfving's theorem,Nonlinear regression,geometric characterization},
number = {3},
pages = {1225--1234},
publisher = {Institute of Mathematical Statistics},
title = {{A note on Bayesian c- and D-optimal designs in nonlinear regression models}},
url = {https://doi.org/10.1214/aos/1032526965},
volume = {24},
year = {1996}
}
@inproceedings{fernandez2006probabilistic,
author = {Fern{\'{a}}ndez, Fernando and Veloso, Manuela},
booktitle = {Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems},
file = {:Users/hikaruasano/Documents/mendeley/Fern{\'{a}}ndez, Veloso{\_}2006{\_}Probabilistic policy reuse in a reinforcement learning agent.pdf:pdf},
pages = {720--727},
title = {{Probabilistic policy reuse in a reinforcement learning agent}},
year = {2006}
}
@article{Amzal2006BayesianOptimalDV,
author = {Amzal, B and Bois, F and Parent, E and Robert, C},
file = {:Users/hikaruasano/Documents/mendeley/Amzal et al.{\_}2006{\_}Bayesian-Optimal Design via Interacting Particle Systems.pdf:pdf},
journal = {Journal of the American Statistical Association},
pages = {773--785},
title = {{Bayesian-Optimal Design via Interacting Particle Systems}},
volume = {101},
year = {2006}
}
@inproceedings{8793721,
author = {Mohseni-Kabir, Anahita and Isele, David and Fujimura, Kikuo},
booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2019.8793721},
pages = {3370--3376},
title = {{Interaction-Aware Multi-Agent Reinforcement Learning for Mobile Agents with Individual Goals}},
year = {2019}
}
@book{BB27827628,
author = {有沙, 江間},
number = {80},
publisher = {化学同人},
series = {DOJIN選書},
title = {{AI社会の歩き方 : 人工知能とどう付き合うか}},
url = {https://ci.nii.ac.jp/ncid/BB27827628},
year = {2019}
}
@article{Sato2017,
abstract = {—Turn-based strategy games are interesting testbeds for developing artificial players because their rules present developers with several challenges. Currently, Monte-Carlo tree search variants are often utilized to address these challenges. However, we consider it worthwhile introducing minimax search variants with pruning techniques because a turn-based strategy is in some points similar to the games of chess and Shogi, in which minimax variants are known to be effective. Thus, we introduced three forward-pruning techniques to enable us to apply alpha beta search (as a minimax search variant) to turn-based strategy games. This type of search involves fixing unit action orders, generating unit actions selectively, and limiting the number of moving units in a search. We applied our proposed pruning methods by implementing an alpha beta-based artificial player in the Turn-based strategy Academic Package (TUBSTAP) open platform of our institute. This player competed against first-and second-rank players in the TUBSTAP AI competition in 2016. Our proposed player won against the other players in five different maps with an average winning ratio exceeding 70{\%}.},
author = {Sato, Naoyuki and Ikeda, Kokolo},
doi = {10.1109/CIG.2016.7860427},
file = {:Users/hikaruasano/Documents/mendeley/Sato, Ikeda{\_}2017{\_}Three types of forward pruning techniques to apply the alpha beta algorithm to turn-based strategy games.pdf:pdf},
isbn = {9781509018833},
issn = {23254289},
journal = {IEEE Conference on Computatonal Intelligence and Games, CIG},
title = {{Three types of forward pruning techniques to apply the alpha beta algorithm to turn-based strategy games}},
year = {2017}
}
@inproceedings{ng1999policy,
author = {Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
booktitle = {Icml},
file = {:Users/hikaruasano/Documents/mendeley/Ng, Harada, Russell{\_}1999{\_}Policy invariance under reward transformations Theory and application to reward shaping.pdf:pdf},
pages = {278--287},
title = {{Policy invariance under reward transformations: Theory and application to reward shaping}},
volume = {99},
year = {1999}
}
@inproceedings{munos2016safe,
author = {Munos, R{\'{e}}mi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/hikaruasano/Documents/mendeley/Munos et al.{\_}2016{\_}Safe and efficient off-policy reinforcement learning.pdf:pdf},
pages = {1054--1062},
title = {{Safe and efficient off-policy reinforcement learning}},
year = {2016}
}
@article{cai2021safe,
author = {Cai, Zhiyuan and Cao, Huanhui and Lu, Wenjie and Zhang, Lin and Xiong, Hao},
file = {:Users/hikaruasano/Documents/mendeley/Cai et al.{\_}2021{\_}Safe multi-agent reinforcement learning through decentralized multiple control barrier functions.pdf:pdf},
journal = {arXiv preprint arXiv:2103.12553},
title = {{Safe multi-agent reinforcement learning through decentralized multiple control barrier functions}},
year = {2021}
}
@article{Xu2018FairGANFG,
author = {Xu, Depeng and Yuan, Shuhan and Zhang, Lu and Wu, Xintao},
file = {:Users/hikaruasano/Documents/mendeley/Xu et al.{\_}2018{\_}FairGAN Fairness-aware Generative Adversarial Networks.pdf:pdf},
journal = {2018 IEEE International Conference on Big Data (Big Data)},
pages = {570--575},
title = {{FairGAN: Fairness-aware Generative Adversarial Networks}},
year = {2018}
}
@article{wang2019boosting,
author = {Wang, Che and Ross, Keith},
file = {:Users/hikaruasano/Documents/mendeley/Wang, Ross{\_}2019{\_}Boosting soft actor-critic Emphasizing recent experience without forgetting the past.pdf:pdf},
journal = {arXiv preprint arXiv:1906.04009},
title = {{Boosting soft actor-critic: Emphasizing recent experience without forgetting the past}},
year = {2019}
}
@misc{「平成30年度エ99:online,
annote = {(Accessed on 07/18/2020)},
author = {資源エネルギー庁},
howpublished = {https://www.enecho.meti.go.jp/about/whitepaper/2019html/},
title = {「平成30年度エネルギーに関する年次報告」（エネルギー白書2019）},
year = {2019}
}
@book{BB21819128,
author = {総務省},
number = {平成28年版},
pages = {p.235},
publisher = {日経印刷,全国官報販売協同組合 (発売)},
series = {情報通信白書 / 総務省編},
title = {{IoT・ビッグデータ・AI : ネットワークとデータが創造する新たな価値}},
url = {https://ci.nii.ac.jp/ncid/BB21819128},
year = {2016}
}
@article{zhu2022hierarchical,
author = {Zhu, Wei and Hayashibe, Mitsuhiro},
file = {:Users/hikaruasano/Documents/mendeley/Zhu, Hayashibe{\_}2022{\_}A hierarchical deep reinforcement learning framework with high efficiency and generalization for fast and safe navig.pdf:pdf},
journal = {IEEE Transactions on Industrial Electronics},
publisher = {IEEE},
title = {{A hierarchical deep reinforcement learning framework with high efficiency and generalization for fast and safe navigation}},
year = {2022}
}
@article{article,
author = {Valavanidis, Athanasios},
pages = {1--26},
title = {{The Shift to Diesel Fuel Engines and How the Emission Scandal of Diesel Vehicles Unfolded. World Energy Consumption of Transportation Sector}},
volume = {1},
year = {2018}
}
@inproceedings{Fu2018,
abstract = {Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.},
archivePrefix = {arXiv},
arxivId = {1710.11248},
author = {Fu, Justin and Luo, Katie and Levine, Sergey},
booktitle = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
eprint = {1710.11248},
file = {:Users/hikaruasano/Documents/mendeley/Fu, Luo, Levine{\_}2018{\_}Learning robust rewards with adversarial inverse reinforcement learning.pdf:pdf},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Learning robust rewards with adversarial inverse reinforcement learning}},
year = {2018}
}
@inproceedings{pmlr-v80-jin18c,
abstract = {Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial observations by using finite length observation histories or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to an advantage-like function and is robust to partially observed state. We demonstrate that this new algorithm can substantially outperform strong baseline methods on several partially observed reinforcement learning tasks: learning first-person 3D navigation in Doom and Minecraft, and acting in the presence of partially observed objects in Doom and Pong.},
author = {Jin, Peter and Keutzer, Kurt and Levine, Sergey},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
editor = {Dy, Jennifer and Krause, Andreas},
file = {:Users/hikaruasano/Documents/mendeley/Jin, Keutzer, Levine{\_}2018{\_}Regret Minimization for Partially Observable Deep Reinforcement Learning.pdf:pdf},
pages = {2342--2351},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Regret Minimization for Partially Observable Deep Reinforcement Learning}},
url = {https://proceedings.mlr.press/v80/jin18c.html},
volume = {80},
year = {2018}
}
@article{130007847096,
author = {武, 小坂},
doi = {10.19014/proceedingsissj.9.0_c1-5},
journal = {情報システム学会 全国大会論文集},
number = {0},
pages = {c1--5},
publisher = {一般社団法人 情報システム学会},
title = {社会科学の転回と情報資源の再利用研究},
url = {https://ci.nii.ac.jp/naid/130007847096/},
volume = {9},
year = {2013}
}
@article{hoekstra2019underestimated,
author = {Hoekstra, Auke},
journal = {Joule},
number = {6},
pages = {1412--1414},
publisher = {Elsevier},
title = {{The underestimated potential of battery electric vehicles to reduce emissions}},
volume = {3},
year = {2019}
}
@article{bem2011feeling,
author = {Bem, Daryl J},
journal = {Journal of personality and social psychology},
number = {3},
pages = {407},
publisher = {American Psychological Association},
title = {{Feeling the future: experimental evidence for anomalous retroactive influences on cognition and affect.}},
volume = {100},
year = {2011}
}
@article{murphy2007sex,
author = {Murphy, Patricia J and Campbell, Scott S},
file = {:Users/hikaruasano/Documents/mendeley/Murphy, Campbell{\_}2007{\_}Sex hormones, sleep, and core body temperature in older postmenopausal women.pdf:pdf},
journal = {Sleep},
number = {12},
pages = {1788--1794},
publisher = {Oxford University Press},
title = {{Sex hormones, sleep, and core body temperature in older postmenopausal women}},
volume = {30},
year = {2007}
}
@inproceedings{lin2020connectivity,
author = {Lin, Juntong and Yang, Xuyun and Zheng, Peiwei and Cheng, Hui},
booktitle = {Conference on Robot Learning},
file = {:Users/hikaruasano/Documents/mendeley/Lin et al.{\_}2020{\_}Connectivity guaranteed multi-robot navigation via deep reinforcement learning.pdf:pdf},
organization = {PMLR},
pages = {661--670},
title = {{Connectivity guaranteed multi-robot navigation via deep reinforcement learning}},
year = {2020}
}
@misc{差し迫る環境危機64:online,
annote = {(Accessed on 07/22/2020)},
author = {Japanグループ, PwC},
howpublished = {https://www.pwc.com/jp/ja/knowledge/thoughtleadership/2020/assets/pdf/automotive-insight-vol15.pdf},
title = {差し迫る環境危機に対応するには},
year = {2020}
}
@article{120006867727,
author = {大岩, 昌子},
issn = {1347-9911},
journal = {名古屋外国語大学外国語学部紀要},
month = {aug},
number = {47},
pages = {117--132},
publisher = {名古屋外国語大学},
title = {日仏におけるチーズの歴史・文化的変遷と現状 : パリ見本市視察報告を兼ねて (水谷修名誉教授 武藤ナンシー名誉教授 黒田十三名誉教授 森川正博名誉教授 記念号)},
url = {https://ci.nii.ac.jp/naid/120006867727/en/},
year = {2014}
}
@article{Zong_Zheng_Li_Jin_2022,
author = {Zong, Zefang and Zheng, Meng and Li, Yong and Jin, Depeng},
doi = {10.1609/aaai.v36i9.21236},
file = {:Users/hikaruasano/Documents/mendeley/Zong et al.{\_}2022{\_}MAPDP Cooperative Multi-Agent Reinforcement Learning to Solve Pickup and Delivery Problems(2).pdf:pdf},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
number = {9},
pages = {9980--9988},
title = {{MAPDP: Cooperative Multi-Agent Reinforcement Learning to Solve Pickup and Delivery Problems}},
url = {https://ojs.aaai.org/index.php/AAAI/article/view/21236},
volume = {36},
year = {2022}
}
@inproceedings{mezghani2022learning,
author = {Mezghani, Lina and Sukhbaatar, Sainbayar and Bojanowski, Piotr and Lazaric, Alessandro and Alahari, Karteek},
booktitle = {6th Annual Conference on Robot Learning},
file = {:Users/hikaruasano/Documents/mendeley/Mezghani et al.{\_}2022{\_}Learning Goal-Conditioned Policies Offline with Self-Supervised Reward Shaping.pdf:pdf},
keywords = {representation learning: offline-RL},
mendeley-tags = {representation learning: offline-RL},
title = {{Learning Goal-Conditioned Policies Offline with Self-Supervised Reward Shaping}},
url = {https://openreview.net/forum?id=8tmKW-NG2bH},
year = {2022}
}
@article{chiellino2010konnen,
author = {Chiellino, U and Winkle, Th and Graab, B and Ernstberger, A and Donner, E and Nerlich, M},
journal = {Zeitschrift f{\"{u}}r Verkehrssicherheit},
number = {2010},
pages = {131--137},
title = {{Was k{\"{o}}nnen Fahrerassistenzsysteme im Unfallgeschehen leisten}},
volume = {3},
year = {2010}
}
@inproceedings{das2017learning,
author = {Das, Abhishek and Kottur, Satwik and Moura, Jos{\'{e}} M F and Lee, Stefan and Batra, Dhruv},
booktitle = {Proceedings of the IEEE international conference on computer vision},
file = {:Users/hikaruasano/Documents/mendeley/Das et al.{\_}2017{\_}Learning cooperative visual dialog agents with deep reinforcement learning.pdf:pdf},
pages = {2951--2960},
title = {{Learning cooperative visual dialog agents with deep reinforcement learning}},
year = {2017}
}
@techreport{ZhangZhuoranYang2019,
abstract = {Recent years have witnessed significant advances in reinforcement learning (RL), which has registered great success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.},
archivePrefix = {arXiv},
arxivId = {1911.10635v1},
author = {{Zhang Zhuoran Yang}, Kaiqing and Bas, Tamer},
eprint = {1911.10635v1},
file = {:Users/hikaruasano/Documents/mendeley/Zhang Zhuoran Yang, Bas{\_}2019{\_}Multi-Agent Reinforcement Learning A Selective Overview of Theories and Algorithms.pdf:pdf},
title = {{Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms}},
year = {2019}
}
@book{Kato2018,
abstract = {文献あり 索引あり.},
author = {Kato, Kimikazu. and 加藤, 公一},
isbn = {9784797393965},
publisher = {Esubikurieitibu},
title = {{K機械学習のエッセンス : 実装しながら学ぶPython、数学、アルゴリズム}},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2003439730},
year = {2018}
}
@inproceedings{Duan2016BenchmarkingDR,
author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, J and Abbeel, P},
booktitle = {ICML},
title = {{Benchmarking Deep Reinforcement Learning for Continuous Control}},
year = {2016}
}
@inproceedings{zhang2013coordinating,
author = {Zhang, Chongjie and Lesser, Victor},
booktitle = {Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems},
file = {:Users/hikaruasano/Documents/mendeley/Zhang, Lesser{\_}2013{\_}Coordinating multi-agent reinforcement learning with limited communication.pdf:pdf;:Users/hikaruasano/Documents/mendeley/Zhang, Lesser{\_}2013{\_}Coordinating multi-agent reinforcement learning with limited communication(2).pdf:pdf},
pages = {1101--1108},
title = {{Coordinating multi-agent reinforcement learning with limited communication}},
year = {2013}
}
@article{jiang2020online,
author = {Jiang, Guangxin and Hong, L Jeff and Nelson, Barry L},
file = {:Users/hikaruasano/Documents/mendeley/Jiang, Hong, Nelson{\_}2020{\_}Online risk monitoring using offline simulation.pdf:pdf},
journal = {INFORMS Journal on Computing},
number = {2},
pages = {356--375},
publisher = {INFORMS},
title = {{Online risk monitoring using offline simulation}},
volume = {32},
year = {2020}
}
@inproceedings{kim2021landmarkguided,
author = {Kim, Junsu and Seo, Younggyo and Shin, Jinwoo},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Beygelzimer, A and Dauphin, Y and Liang, P and Vaughan, J Wortman},
file = {:Users/hikaruasano/Documents/mendeley/Kim, Seo, Shin{\_}2021{\_}Landmark-Guided Subgoal Generation in Hierarchical Reinforcement Learning(2).pdf:pdf},
title = {{Landmark-Guided Subgoal Generation in Hierarchical Reinforcement Learning}},
url = {https://openreview.net/forum?id=IWhFd34QSSj},
year = {2021}
}
@misc{コンテナ不足世界53:online,
annote = {(Accessed on 02/10/2022)},
author = {NHKニュース},
howpublished = {https://www3.nhk.or.jp/news/html/20211123/k10013358661000.html},
title = {コンテナ不足 世界の物流混乱 国連“世界経済の回復に影響も” | 新型コロナ 経済影響}
}
@inproceedings{10.5555/3306127.3331810,
abstract = {Modelling teammates' policies in cooperative multi-agent systems has long been an interest and also a big challenge for the reinforcement learning (RL) community. The interest lies in the fact that if the agent knows the teammates' policies, it can adjust its own policy accordingly to arrive at proper cooperations; while the challenge is that the agents' policies are changing continuously because they are learning concurrently to adapt to each other. In this paper, we present ATTention Multi-Agent Deep Deterministic Policy Gradient (ATT-MADDPG) to address this challenge. ATT-MADDPG extends DDPG, a single-agent actor-critic RL method, with two special designs. First, as a necessary step to model the teammates' policies, the agent should get access to the observations and actions of teammates. ATT-MADDPG adopts a centralized critic to collect such information. Second, ATT-MADDPG further enhances the centralized critic with an attention mechanism in a principled way. This attention mechanism introduces a special structure to explicitly model the dynamic joint policy of teammates in an adaptive manner, making sure that the collected information can be processed in an effective way. As a result, all agents will cooperate with each other efficiently. We evaluate our method on both benchmark tasks and the real-world packet routing tasks. Results show that ATT-MADDPG not only outperforms the state-of-the-art RL-based and rule-based methods by a large margin, but also achieves better scalability and robustness.},
address = {Richland, SC},
author = {Mao, Hangyu and Zhang, Zhengchao and Xiao, Zhen and Gong, Zhibo},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
file = {:Users/hikaruasano/Documents/mendeley/Mao et al.{\_}2019{\_}Modelling the Dynamic Joint Policy of Teammates with Attention Multi-Agent DDPG.pdf:pdf},
isbn = {9781450363099},
keywords = {agent modelling,deep reinforcement learning,multi-agent reinforcement learning,teammates modelling},
pages = {1108--1116},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
series = {AAMAS '19},
title = {{Modelling the Dynamic Joint Policy of Teammates with Attention Multi-Agent DDPG}},
year = {2019}
}
@book{BA79120757pp.55.65,
author = {学, 赤川},
pages = {pp.55--65},
publisher = {勁草書房},
title = {構築主義を再構築する},
url = {https://ci.nii.ac.jp/ncid/BA79120757},
year = {2006}
}
@article{dAlessandro2017ConscientiousCA,
author = {D'Alessandro, Brian and O'Neil, Cathy and LaGatta, Tom},
journal = {Big data},
pages = {120--134},
title = {{Conscientious Classification: A Data Scientist's Guide to Discrimination-Aware Classification}},
volume = {5 2},
year = {2017}
}
@article{dafoe2020open,
author = {Dafoe, Allan and Hughes, Edward and Bachrach, Yoram and Collins, Tantum and McKee, Kevin R and Leibo, Joel Z and Larson, Kate and Graepel, Thore},
journal = {arXiv preprint arXiv:2012.08630},
title = {{Open problems in cooperative AI}},
year = {2020}
}
@article{Barriga2017,
abstract = {Significant progress has been made in recent years towards stronger Real-Time Strategy (RTS) game playing agents. Some of the latest approaches have focused on enhancing standard game tree search techniques with a smart sampling of the search space, or on directly reducing this search space. However, experiments have thus far only been performed using small scenarios. We provide experimental results on the performance of these agents on increasingly larger scenarios. Our main contribution is {\textless}formula{\textgreater}{\textless}tex{\textgreater}{\$}Puppet{\$}{\textless}/tex{\textgreater}{\textless}/formula{\textgreater} {\textless}formula{\textgreater}{\textless}tex{\textgreater}{\$}Search{\$}{\textless}/tex{\textgreater}{\textless}/formula{\textgreater}, a new adversarial search framework that reduces the search space by using scripts that can expose choice points to a look-ahead search procedure. Selecting a combination of a script and decisions for its choice points represents an abstract move to be applied next. Such moves can be directly executed in the actual game, or in an abstract representation of the game state which can be used by an adversarial tree search algorithm. We tested {\textless}formula{\textgreater}{\textless}tex{\textgreater}{\$}Puppet{\$}{\textless}/tex{\textgreater}{\textless}/formula{\textgreater} {\textless}formula{\textgreater}{\textless}tex{\textgreater}{\$}Search{\$}{\textless}/tex{\textgreater}{\textless}/formula{\textgreater} in microRTS, an abstract RTS game popular within the research community, allowing us to directly compare our algorithm against state-of-the-art agents published in the last few years. We show a similar performance to other scripted and search based agents on smaller scenarios, while outperforming them on larger ones.},
author = {Barriga, Nicolas Arturo and Stanescu, Marius and Buro, Michael},
doi = {10.1109/TCIAIG.2017.2717902},
file = {:Users/hikaruasano/Documents/mendeley/Barriga, Stanescu, Buro{\_}2017{\_}Game Tree Search Based on Non-Deterministic Action Scripts in Real-Time Strategy Games.pdf:pdf},
issn = {1943068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {Adversarial Search,Artificial intelligence,Complexity theory,Decision trees,Games,Heuristic Search,Monte Carlo methods,Monte-Carlo Tree Search,Real-Time Strategy (RTS) Games,Real-time systems,Standards},
number = {June},
title = {{Game Tree Search Based on Non-Deterministic Action Scripts in Real-Time Strategy Games}},
year = {2017}
}
@inproceedings{zimmer2021learning,
author = {Zimmer, Matthieu and Glanois, Claire and Siddique, Umer and Weng, Paul},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Zimmer et al.{\_}2021{\_}Learning fair policies in decentralized cooperative multi-agent reinforcement learning.pdf:pdf},
organization = {PMLR},
pages = {12967--12978},
title = {{Learning fair policies in decentralized cooperative multi-agent reinforcement learning}},
year = {2021}
}
@misc{MeetQThe62:online,
annote = {(Accessed on 01/31/2021)},
author = {Q, Meet},
howpublished = {https://www.genderlessvoice.com/},
title = {{The First Genderless Voice}}
}
@misc{gaiyoupd97:online,
annote = {(Accessed on 06/28/2020)},
author = {江東5区広域避難推進協議会},
howpublished = {https://www.city.koto.lg.jp/057101/bosai/bosai-top/topics/documents/gaiyou.pdf},
title = {江東５区大規模水害広域避難計画 概要},
year = {2018}
}
@article{Chen2010,
author = {Chen, Jack},
file = {:Users/hikaruasano/Documents/mendeley/Chen{\_}2010{\_}Applications of Artificial Intelligence and Machine Learning in Othello.pdf:pdf},
journal = {TJHSST Computer System Lab},
pages = {1--17},
title = {{Applications of Artificial Intelligence and Machine Learning in Othello}},
year = {2010}
}
@article{2018a,
author = {裕貴, 瀧川},
file = {:Users/hikaruasano/Documents/mendeley/裕貴{\_}2018{\_}社会学との関係から見た計算社会科学の現状と課題.pdf:pdf},
journal = {理論と方法},
number = {1},
pages = {132--148},
title = {社会学との関係から見た計算社会科学の現状と課題},
volume = {33},
year = {2018}
}
@inproceedings{kuba2022trust,
author = {Kuba, Jakub Grudzien and Chen, Ruiqing and Wen, Muning and Wen, Ying and Sun, Fanglei and Wang, Jun and Yang, Yaodong},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Kuba et al.{\_}2022{\_}Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning.pdf:pdf},
title = {{Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning}},
url = {https://openreview.net/forum?id=EcGGFkNTxdJ},
year = {2022}
}
@article{doi:10.1177/1064804619884160,
abstract = {Machine learning models developed from real-world data can inherit potential, preexisting bias in the dataset. When these models are used to inform decisions involving human beings, fairness concerns inevitably arise. Imposing certain fairness constraints in the training of models can be effective only if appropriate criteria are applied. However, a fairness criterion can be defined/assessed only when the interaction between the decisions and the underlying population is well understood. We introduce two feedback models describing how people react when receiving machine-aided decisions and illustrate that some commonly used fairness criteria can end with undesirable consequences while reinforcing discrimination.},
author = {Zhang, Xueru and Khalili, Mohammad Mahdi and Liu, Mingyan},
doi = {10.1177/1064804619884160},
file = {:Users/hikaruasano/Documents/mendeley/Zhang, Khalili, Liu{\_}2020{\_}Long-Term Impacts of Fair Machine Learning.pdf:pdf},
journal = {Ergonomics in Design},
number = {3},
pages = {7--11},
title = {{Long-Term Impacts of Fair Machine Learning}},
url = {https://doi.org/10.1177/1064804619884160},
volume = {28},
year = {2020}
}
@article{Ding2020,
abstract = {Communication lays the foundation for human cooperation. It is also crucial for multi-agent cooperation. However, existing work focuses on broadcast communication, which is not only impractical but also leads to information redundancy that could even impair the learning process. To tackle these difficulties, we propose $\backslash$textit{\{}Individually Inferred Communication{\}} (I2C), a simple yet effective model to enable agents to learn a prior for agent-agent communication. The prior knowledge is learned via causal inference and realized by a feed-forward neural network that maps the agent's local observation to a belief about who to communicate with. The influence of one agent on another is inferred via the joint action-value function in multi-agent reinforcement learning and quantified to label the necessity of agent-agent communication. Furthermore, the agent policy is regularized to better exploit communicated messages. Empirically, we show that I2C can not only reduce communication overhead but also improve the performance in a variety of multi-agent cooperative scenarios, comparing to existing methods.},
archivePrefix = {arXiv},
arxivId = {2006.06455},
author = {Ding, Ziluo and Huang, Tiejun and Lu, Zongqing},
eprint = {2006.06455},
file = {:Users/hikaruasano/Documents/mendeley/Ding, Huang, Lu{\_}2020{\_}Learning Individually Inferred Communication for Multi-Agent Cooperation.pdf:pdf},
month = {jun},
title = {{Learning Individually Inferred Communication for Multi-Agent Cooperation}},
url = {http://arxiv.org/abs/2006.06455},
year = {2020}
}
@inproceedings{tang2021collaborative,
author = {Tang, Bohan and Zhong, Yiqi and Neumann, Ulrich and Wang, Gang and Chen, Siheng and Zhang, Ya},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Beygelzimer, A and Dauphin, Y and Liang, P and Vaughan, J Wortman},
file = {:Users/hikaruasano/Documents/mendeley/Tang et al.{\_}2021{\_}Collaborative Uncertainty in Multi-Agent Trajectory Forecasting.pdf:pdf},
title = {{Collaborative Uncertainty in Multi-Agent Trajectory Forecasting}},
url = {https://openreview.net/forum?id=sO4tOk2lg9I},
year = {2021}
}
@book{BC04771667,
author = {Eberhardt, Jennifer Lynn and 希美, 山岡 and 史明, 高},
publisher = {明石書店},
title = {無意識のバイアス : 人はなぜ人種差別をするのか},
url = {https://ci.nii.ac.jp/ncid/BC04771667},
year = {2020}
}
@inproceedings{kim2021policy,
author = {Kim, Dong Ki and Liu, Miao and Riemer, Matthew D and Sun, Chuangchuang and Abdulhai, Marwa and Habibi, Golnaz and Lopez-Cot, Sebastian and Tesauro, Gerald and How, Jonathan},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Kim et al.{\_}2021{\_}A policy gradient algorithm for learning to learn in multiagent reinforcement learning.pdf:pdf},
organization = {PMLR},
pages = {5541--5550},
title = {{A policy gradient algorithm for learning to learn in multiagent reinforcement learning}},
year = {2021}
}
@inproceedings{pmlr-v48-wangf16,
abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
address = {New York, New York, USA},
author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
editor = {Balcan, Maria Florina and Weinberger, Kilian Q},
pages = {1995--2003},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Dueling Network Architectures for Deep Reinforcement Learning}},
url = {https://proceedings.mlr.press/v48/wangf16.html},
volume = {48},
year = {2016}
}
@misc{通商白書201682:online,
annote = {(Accessed on 01/21/2021)},
author = {経済産業省},
howpublished = {https://www.meti.go.jp/report/tsuhaku2016/whitepaper{\_}2016.html},
pages = {131},
title = {通商白書2016},
year = {2016}
}
@techreport{Lowe,
abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Uc, Pieter Abbeel and Openai, Berkeley and Openai, Igor Mordatch},
file = {:Users/hikaruasano/Documents/mendeley/Lowe et al.{\_}Unknown{\_}Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.pdf:pdf},
title = {{Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}}
}
@article{石原孟2011急拡大する洋上風力発電の現状と将来展望,
author = {石原孟},
journal = {風力エネルギー},
number = {2},
pages = {4--8},
publisher = {一般社団法人 日本風力エネルギー学会},
title = {急拡大する洋上風力発電の現状と将来展望},
volume = {35},
year = {2011}
}
@book{fedorov2013theory,
author = {Fedorov, Valerii Vadimovich},
publisher = {Elsevier},
title = {{Theory of optimal experiments}},
year = {2013}
}
@article{黒川,
author = {久幸, 黒川 and 智貴, 高野 and 理沙, 鈴木 and 三郎, 鶴田},
journal = {日本航海学会論文集},
pages = {1--9},
title = {{国際海上コンテナ輸送におけるCO{\_}2排出量削減策の実行可能性に関する研究}},
volume = {124},
year = {2011}
}
@inproceedings{10.1145/3461702.3462521,
abstract = {Typically, fair machine learning research focuses on a single decision maker and assumes that the underlying population is stationary. However, many of the critical domains motivating this work are characterized by competitive marketplaces with many decision makers. Realistically, we might expect only a subset of them to adopt any non-compulsory fairness-conscious policy, a situation that political philosophers call partial compliance. This possibility raises important questions: how does partial compliance and the consequent strategic behavior of decision subjects affect the allocation outcomes? If k{\%} of employers were to voluntarily adopt a fairness-promoting intervention, should we expect k{\%} progress (in aggregate) towards the benefits of universal adoption, or will the dynamics of partial compliance wash out the hoped-for benefits? How might adopting a global (versus local) perspective impact the conclusions of an auditor? In this paper, we propose a simple model of an employment market, leveraging simulation as a tool to explore the impact of both interaction effects and incentive effects on outcomes and auditing metrics. Our key findings are that at equilibrium: (1) partial compliance by k{\%} of employers can result in far less than proportional (k{\%}) progress towards the full compliance outcomes; (2) the gap is more severe when fair employers match global (vs local) statistics; (3) choices of local vs global statistics can paint dramatically different pictures of the performance vis-a-vis fairness desiderata of compliant versus non-compliant employers; (4) partial compliance based on local parity measures can induce extreme segregation. Finally, we discuss implications for auditors and insights concerning the design of regulatory frameworks.},
address = {New York, NY, USA},
author = {Dai, Jessica and Fazelpour, Sina and Lipton, Zachary},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
doi = {10.1145/3461702.3462521},
file = {:Users/hikaruasano/Documents/mendeley/Dai, Fazelpour, Lipton{\_}2021{\_}Fair Machine Learning Under Partial Compliance.pdf:pdf},
isbn = {9781450384735},
keywords = {distributive justice,fair machine learning,fairness,hiring,regulation,segregation,simulations},
pages = {55--65},
publisher = {Association for Computing Machinery},
series = {AIES '21},
title = {{Fair Machine Learning Under Partial Compliance}},
url = {https://doi.org/10.1145/3461702.3462521},
year = {2021}
}
@inproceedings{horgan2018distributed,
author = {Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and van Hasselt, Hado and Silver, David},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Horgan et al.{\_}2018{\_}Distributed Prioritized Experience Replay.pdf:pdf},
title = {{Distributed Prioritized Experience Replay}},
url = {https://openreview.net/forum?id=H1Dy---0Z},
year = {2018}
}
@inproceedings{Armstrong2015MotivatedVS,
author = {Armstrong, Stuart},
booktitle = {AAAI Workshop: AI and Ethics},
title = {{Motivated Value Selection for Artificial Agents}},
year = {2015}
}
@article{7937882,
author = {Glotfelter, Paul and Cort{\'{e}}s, Jorge and Egerstedt, Magnus},
doi = {10.1109/LCSYS.2017.2710943},
file = {:Users/hikaruasano/Documents/mendeley/Glotfelter, Cort{\'{e}}s, Egerstedt{\_}2017{\_}Nonsmooth Barrier Functions With Applications to Multi-Robot Systems.pdf:pdf},
journal = {IEEE Control Systems Letters},
number = {2},
pages = {310--315},
title = {{Nonsmooth Barrier Functions With Applications to Multi-Robot Systems}},
volume = {1},
year = {2017}
}
@misc{【解説】なぜアメ89:online,
annote = {(Accessed on 01/31/2021)},
author = {BBCニュース},
howpublished = {https://www.bbc.com/japanese/features-and-analysis-52916317},
title = {【解説】 なぜアメリカで大勢が怒っているのか 人種に関する3つのデータ},
year = {2020}
}
@misc{千葉市：千葉市空39:online,
annote = {(Accessed on 07/15/2020)},
author = {千葉市},
howpublished = {https://www.city.chiba.jp/toshi/kenchiku/jutakuseisaku/akiya-taisakukeikaku.html},
title = {千葉市空家等対策計画},
year = {2019}
}
@inproceedings{10.5555/3495724.3496911,
abstract = {Model-based reinforcement learning algorithms with probabilistic dynamical models are amongst the most data-efficient learning methods. This is often attributed to their ability to distinguish between epistemic and aleatoric uncertainty. However, while most algorithms distinguish these two uncertainties for learning the model, they ignore it when optimizing the policy, which leads to greedy and insufficient exploration. At the same time, there are no practical solvers for optimistic exploration algorithms. In this paper, we propose a practical optimistic exploration algorithm (H-UCRL). H-UCRL reparameterizes the set of plausible models and hallucinates control directly on the epistemic uncertainty. By augmenting the input space with the hallucinated inputs, H-UCRL can be solved using standard greedy planners. Furthermore, we analyze H-UCRL and construct a general regret bound for well-calibrated models, which is provably sublinear in the case of Gaussian Process models. Based on this theoretical foundation, we show how optimistic exploration can be easily combined with state-of-the-art reinforcement learning algorithms and different probabilistic models. Our experiments demonstrate that optimistic exploration significantly speeds-up learning when there are penalties on actions, a setting that is notoriously difficult for existing model-based reinforcement learning algorithms.},
address = {Red Hook, NY, USA},
author = {Curi, Sebastian and Berkenkamp, Felix and Krause, Andreas},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
file = {:Users/hikaruasano/Documents/mendeley/Curi, Berkenkamp, Krause{\_}2020{\_}Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning.pdf:pdf},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
series = {NIPS'20},
title = {{Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning}},
year = {2020}
}
@inproceedings{Rainforth2018OnNM,
author = {Rainforth, Tom and Cornish, R and Yang, H and Warrington, Andrew},
booktitle = {ICML},
file = {:Users/hikaruasano/Documents/mendeley/Rainforth et al.{\_}2018{\_}On Nesting Monte Carlo Estimators.pdf:pdf},
title = {{On Nesting Monte Carlo Estimators}},
year = {2018}
}
@techreport{Ramachandran,
abstract = {Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elici-tation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.},
author = {Ramachandran, Deepak and Amir, Eyal},
booktitle = {aaai.org},
file = {:Users/hikaruasano/Documents/mendeley/Ramachandran, Amir{\_}Unknown{\_}Bayesian Inverse Reinforcement Learning(2).pdf:pdf},
keywords = {Markov-decision processes,reinforcement learning},
title = {{Bayesian Inverse Reinforcement Learning}},
url = {https://gateway.itc.u-tokyo.ac.jp/Papers/IJCAI/2007/IJCAI07-416.pdf,DanaInfo=www.aaai.org}
}
@article{albrecht2016exploiting,
author = {Albrecht, Stefano V and Ramamoorthy, Subramanian},
journal = {Journal of Artificial Intelligence Research},
pages = {1135--1178},
title = {{Exploiting causality for selective belief filtering in dynamic Bayesian networks}},
volume = {55},
year = {2016}
}
@inproceedings{qu2019value,
author = {Qu, Chao and Mannor, Shie and Xu, Huan and Qi, Yuan and Song, Le and Xiong, Junwu},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/hikaruasano/Documents/mendeley/Zhang, Yang, Bacsar{\_}2019{\_}Decentralized Multi-Agent Reinforcement Learning with Networked Agents Recent Advances.pdf:pdf},
pages = {1184--1193},
title = {{Value propagation for decentralized networked deep multi-agent reinforcement learning}},
year = {2019}
}
@inbook{10.5555/3454287.3454579,
abstract = {We propose new methods for learning control policies and neural network Lyapunov functions for nonlinear control problems, with provable guarantee of stability. The framework consists of a learner that attempts to find the control and Lyapunov functions, and a falsifier that finds counterexamples to quickly guide the learner towards solutions. The procedure terminates when no counterexample is found by the falsifier, in which case the controlled nonlinear system is provably stable. The approach significantly simplifies the process of Lyapunov control design, provides end-to-end correctness guarantee, and can obtain much larger regions of attraction than existing methods such as LQR and SOS/SDP. We show experiments on how the new methods obtain high-quality solutions for challenging robot control problems such as path tracking for wheeled vehicles and humanoid robot balancing.},
address = {Red Hook, NY, USA},
author = {Chang, Ya-Chien and Roohi, Nima and Gao, Sicun},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
file = {:Users/hikaruasano/Documents/mendeley/Chang, Roohi, Gao{\_}2019{\_}Neural Lyapunov Control.pdf:pdf},
publisher = {Curran Associates Inc.},
title = {{Neural Lyapunov Control}},
year = {2019}
}
@article{110002808230,
author = {理一郎, 溝口},
doi = {10.11517/jjsai.14.6_977},
issn = {0912-8085},
journal = {人工知能学会誌 = Journal of Japanese Society for Artificial Intelligence},
month = {nov},
number = {6},
pages = {977--988},
publisher = {一般社団法人 人工知能学会},
title = {オントロジー研究の基礎と応用 (「オントロジーの基礎と応用」)},
url = {https://ci.nii.ac.jp/naid/110002808230/},
volume = {14},
year = {1999}
}
@inproceedings{NIPS2016_f442d33f,
author = {Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Lee, D and Sugiyama, M and Luxburg, U and Guyon, I and Garnett, R},
file = {:Users/hikaruasano/Documents/mendeley/Kulkarni et al.{\_}2016{\_}Hierarchical Deep Reinforcement Learning Integrating Temporal Abstraction and Intrinsic Motivation.pdf:pdf},
publisher = {Curran Associates, Inc.},
title = {{Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation}},
url = {https://proceedings.neurips.cc/paper/2016/file/f442d33fa06832082290ad8544a8da27-Paper.pdf},
volume = {29},
year = {2016}
}
@misc{posttrut8:online,
annote = {(Accessed on 07/09/2021)},
author = {{Oxford Advanced Learner's Dictionary}},
howpublished = {https://www.oxfordlearnersdictionaries.com/definition/english/post-truth?q=post-truth（2021年7月9日）},
title = {post-truth adjective},
year = {2021}
}
@inproceedings{zafar2017fairness,
author = {Zafar, Muhammad Bilal and Valera, Isabel and {Gomez Rodriguez}, Manuel and Gummadi, Krishna P},
booktitle = {Proceedings of the 26th international conference on world wide web},
file = {:Users/hikaruasano/Documents/mendeley/Zafar et al.{\_}2017{\_}Fairness beyond disparate treatment {\&} disparate impact Learning classification without disparate mistreatment.pdf:pdf},
pages = {1171--1180},
title = {{Fairness beyond disparate treatment {\&} disparate impact: Learning classification without disparate mistreatment}},
year = {2017}
}
@inproceedings{ma2017forecasting,
author = {Ma, Wei-Chiu and Huang, De-An and Lee, Namhoon and Kitani, Kris M},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
file = {:Users/hikaruasano/Documents/mendeley/Ma et al.{\_}2017{\_}Forecasting interactive dynamics of pedestrians with fictitious play.pdf:pdf},
pages = {774--782},
title = {{Forecasting interactive dynamics of pedestrians with fictitious play}},
year = {2017}
}
@inproceedings{rana2020multiplicative,
author = {Rana, Krishan and Dasagi, Vibhavari and Talbot, Ben and Milford, Michael and S{\"{u}}nderhauf, Niko},
booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
file = {:Users/hikaruasano/Documents/mendeley/Rana et al.{\_}2020{\_}Multiplicative controller fusion Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim.pdf:pdf},
organization = {IEEE},
pages = {6069--6076},
title = {{Multiplicative controller fusion: Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim-to-real transfer}},
year = {2020}
}
@article{doi:10.1126/scitranslmed.aaf5027,
abstract = {The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences. The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for “truth.”},
author = {Goodman, Steven N and Fanelli, Daniele and Ioannidis, John P A},
doi = {10.1126/scitranslmed.aaf5027},
journal = {Science Translational Medicine},
number = {341},
pages = {341ps12--341ps12},
title = {{What does research reproducibility mean?}},
url = {https://www.science.org/doi/abs/10.1126/scitranslmed.aaf5027},
volume = {8},
year = {2016}
}
@article{zhu2021deep,
author = {Zhu, Kai and Zhang, Tao},
file = {:Users/hikaruasano/Documents/mendeley/Zhu, Zhang{\_}2021{\_}Deep reinforcement learning based mobile robot navigation A review.pdf:pdf},
journal = {Tsinghua Science and Technology},
number = {5},
pages = {674--691},
publisher = {TUP},
title = {{Deep reinforcement learning based mobile robot navigation: A review}},
volume = {26},
year = {2021}
}
@article{藤島喜嗣2016社会心理学における,
author = {藤島喜嗣 and 樋口匡貴},
journal = {心理学評論},
number = {1},
pages = {84--97},
publisher = {心理学評論刊行会},
title = {社会心理学における “p-hacking” の実践例},
volume = {59},
year = {2016}
}
@article{levine2018reinforcement,
author = {Levine, Sergey},
file = {:Users/hikaruasano/Documents/mendeley/Levine{\_}2018{\_}Reinforcement learning and control as probabilistic inference Tutorial and review(2).pdf:pdf},
journal = {arXiv preprint arXiv:1805.00909},
title = {{Reinforcement learning and control as probabilistic inference: Tutorial and review}},
year = {2018}
}
@article{rosman2016bayesian,
author = {Rosman, Benjamin and Hawasly, Majd and Ramamoorthy, Subramanian},
file = {:Users/hikaruasano/Documents/mendeley/Rosman, Hawasly, Ramamoorthy{\_}2016{\_}Bayesian policy reuse.pdf:pdf},
journal = {Machine Learning},
number = {1},
pages = {99--127},
publisher = {Springer},
title = {{Bayesian policy reuse}},
volume = {104},
year = {2016}
}
@article{doi:10.1073/pnas.1915841117,
abstract = {Behavioral scientists need a principled methodology for working with large datasets. We offer an exploratory approach that combines ideas from machine learning and psychology, and we conduct a case study in the domain of moral reasoning. We demonstrate that our approach allows us to both build a powerful and interpretable computational model, and identify subtle principles humans employ in moral dilemmas. The method we offer can be applied in any field with large datasets. Do large datasets provide value to psychologists? Without a systematic methodology for working with such datasets, there is a valid concern that analyses will produce noise artifacts rather than true effects. In this paper, we offer a way to enable researchers to systematically build models and identify novel phenomena in large datasets. One traditional approach is to analyze the residuals of models—the biggest errors they make in predicting the data—to discover what might be missing from those models. However, once a dataset is sufficiently large, machine learning algorithms approximate the true underlying function better than the data, suggesting, instead, that the predictions of these data-driven models should be used to guide model building. We call this approach “Scientific Regret Minimization” (SRM), as it focuses on minimizing errors for cases that we know should have been predictable. We apply this exploratory method on a subset of the Moral Machine dataset, a public collection of roughly 40 million moral decisions. Using SRM, we find that incorporating a set of deontological principles that capture dimensions along which groups of agents can vary (e.g., sex and age) improves a computational model of human moral judgment. Furthermore, we are able to identify and independently validate three interesting moral phenomena: criminal dehumanization, age of responsibility, and asymmetric notions of responsibility.},
author = {Agrawal, Mayank and Peterson, Joshua C and Griffiths, Thomas L},
doi = {10.1073/pnas.1915841117},
file = {:Users/hikaruasano/Documents/mendeley/Agrawal, Peterson, Griffiths{\_}2020{\_}Scaling up psychology via Scientific Regret Minimization.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
number = {16},
pages = {8825--8835},
title = {{Scaling up psychology via Scientific Regret Minimization}},
url = {https://www.pnas.org/doi/abs/10.1073/pnas.1915841117},
volume = {117},
year = {2020}
}
@book{BB2884265X,
author = {通, 西垣 and 茂生, 河島},
number = {667},
publisher = {中央公論新社},
series = {中公新書ラクレ},
title = {{AI倫理 : 人工知能は「責任」をとれるのか}},
url = {https://ci.nii.ac.jp/ncid/BB2884265X},
year = {2019}
}
@inproceedings{wang2021large,
abstract = {Multi-agent environments with large numbers of agents are difficult to solve due to the complexity associated with drawing sufficient samples for learning. While recent work has addressed the possibility of using transfer learning to improve sample complexities of reinforcement learning algorithms, methods for transfer- ring knowledge in multi-agent domains across differing numbers of agents have rarely been considered. To address the bottleneck with sampling from large scale environments, we propose a joint critic structure motivated from graph convo- lutional networks and coordination graphs that allows for the direct transfer of parameters into environments with varying amounts of agents. We further consider fine-tuning the transferred policy and critic networks on the target domain and pro- vide the motivation for doing so in cooperative environments where agent behavior is determined by a subset of the total population. Finally, we provide empirical results validating our claims on such environments, including popular multi-agent benchmark environments.},
author = {Wang, Ethan and Chen, Binghong and Song, Le},
booktitle = {Deep RL Workshop NeurIPS 2021},
file = {:Users/hikaruasano/Documents/mendeley/Wang, Chen, Song{\_}2021{\_}Large Scale Coordination Transfer for Cooperative Multi-Agent Reinforcement Learning.pdf:pdf},
title = {{Large Scale Coordination Transfer for Cooperative Multi-Agent Reinforcement Learning}},
url = {https://openreview.net/forum?id=1Edj1dfQb7b},
year = {2021}
}
@inproceedings{pmlr-v80-fujimoto18a,
abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
editor = {Dy, Jennifer and Krause, Andreas},
pages = {1587--1596},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Addressing Function Approximation Error in Actor-Critic Methods}},
url = {https://proceedings.mlr.press/v80/fujimoto18a.html},
volume = {80},
year = {2018}
}
@article{jaderberg2017population,
author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Others},
file = {:Users/hikaruasano/Documents/mendeley/Jaderberg et al.{\_}2017{\_}Population based training of neural networks.pdf:pdf},
journal = {arXiv preprint arXiv:1711.09846},
title = {{Population based training of neural networks}},
year = {2017}
}
@article{Lai2006,
author = {Lai, Yung-ling and Chang, Shiao-fan and Ko, Ting-chun},
file = {:Users/hikaruasano/Documents/mendeley/Lai, Chang, Ko{\_}2006{\_}Applying Genetic Algorithm and Self-Learning on Computer Othello.pdf:pdf},
number = {March},
pages = {1709--1714},
title = {{Applying Genetic Algorithm and Self-Learning on Computer Othello}},
year = {2006}
}
@book{Olver2014,
address = {Cham},
author = {Olver, Peter J.},
doi = {10.1007/978-3-319-02099-0},
file = {:Users/hikaruasano/Documents/mendeley/Olver{\_}2014{\_}Introduction to Partial Differential Equations.pdf:pdf},
isbn = {978-3-319-02098-3},
publisher = {Springer International Publishing},
series = {Undergraduate Texts in Mathematics},
title = {{Introduction to Partial Differential Equations}},
url = {http://link.springer.com/10.1007/978-3-319-02099-0},
year = {2014}
}
@article{davies2021advancing,
author = {Davies, Alex and Veli{\v{c}}kovi{\'{c}}, Petar and Buesing, Lars and Blackwell, Sam and Zheng, Daniel and Toma{\v{s}}ev, Nenad and Tanburn, Richard and Battaglia, Peter and Blundell, Charles and Juh{\'{a}}sz, Andr{\'{a}}s and Others},
file = {:Users/hikaruasano/Documents/mendeley/Davies et al.{\_}2021{\_}Advancing mathematics by guiding human intuition with AI.pdf:pdf},
journal = {Nature},
number = {7887},
pages = {70--74},
publisher = {Nature Publishing Group},
title = {{Advancing mathematics by guiding human intuition with AI}},
volume = {600},
year = {2021}
}
@article{navidi2020human,
author = {Navidi, Neda and Chabot, Francois and Kurandwad, Sagar and Lustigman, Irv and Robert, Vincent and Szriftgiser, Gregory and Schuch, Andrea},
file = {:Users/hikaruasano/Documents/mendeley/Navidi et al.{\_}2020{\_}Human and Multi-Agent collaboration in a human-MARL teaming framework.pdf:pdf},
journal = {arXiv preprint arXiv:2006.07301},
title = {{Human and Multi-Agent collaboration in a human-MARL teaming framework}},
year = {2020}
}
@techreport{Obane,
abstract = {The Japanese government has set a target of 80{\%} emission reduction by 2050. To realize this target, ground-mounted photovoltaics (PV) and onshore wind energy is expected. However, some of these plants, for instance in the forest, had a negative impact on local nature or wildlife. As well as, development of these additional energies could put pressure on available land resources. Hence, it is required to install these energies on lands with few or no competing uses. The main objectives of this study are to identify the land with few or no competing in Japan and assess their technical potential in these lands. This study finds that the surface area of these lands is 3,428 km 2 which is about only 1{\%} of its total lands. In 72{\%} of this area, PV system and onshore wind may have to compete for development on these lands. In the case that PV systems are installed in these lands, 230 GW of PV system could be installed. On the other hand, in the case that onshore will be installed in these lands, 25 GW of onshore wind and 64 GW of PV system in remained lands can be installed. However, these potentials are concentrated in the area with low electric demand such as Hokkaido, Tohoku, where these potentials are excess over maximum peak electric demand. Thus, it is most indispensable to consider its uneven distribution against total potentials to develop future energy mix scenario or energy policies.},
author = {Obane, Hideaki and Nagai, Yu and Asano, Kenji},
file = {:Users/hikaruasano/Documents/mendeley/Obane, Nagai, Asano{\_}2019{\_}土地利用を考慮した太陽光発電および 陸上風力の導入ポテンシャル評価.pdf:pdf},
institution = {電力中央研究所},
title = {土地利用を考慮した太陽光発電および 陸上風力の導入ポテンシャル評価},
year = {2019}
}
@inproceedings{hausman2017multi,
author = {Hausman, Karol and Chebotar, Yevgen and Schaal, Stefan and Sukhatme, Gaurav and Lim, Joseph J},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/hikaruasano/Desktop/mendeley/Florensa,Duan,Abbeel-2017-Stochastic neural networks for hierarchical reinforcement learning.pdf:pdf},
pages = {1235--1245},
title = {{Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets}},
year = {2017}
}
@misc{気象庁Japan75:online,
annote = {(Accessed on 07/22/2020)},
author = {気象庁},
howpublished = {https://www.data.jma.go.jp/cpdinfo/ipcc/ar5/index.html{\#}spm},
title = {{IPCC第5次評価報告書 第1作業部会報告書 政策決定者向け要約}},
year = {2015}
}
@article{published_papers/25041097,
author = {筒井淳也},
file = {:Users/hikaruasano/Documents/mendeley/筒井淳也{\_}2019{\_}計量社会学と因果推論.pdf:pdf},
journal = {理論と方法},
number = {1},
pages = {35--46},
title = {計量社会学と因果推論},
volume = {34},
year = {2019}
}
@inproceedings{10.5555/3157382.3157584,
abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
address = {Red Hook, NY, USA},
author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
isbn = {9781510838819},
pages = {4356--4364},
publisher = {Curran Associates Inc.},
series = {NIPS'16},
title = {{Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings}},
year = {2016}
}
@inproceedings{song2018multi,
author = {Song, Jiaming and Ren, Hongyu and Sadigh, Dorsa and Ermon, Stefano},
booktitle = {Advances in neural information processing systems},
file = {:Users/hikaruasano/Documents/mendeley/Song et al.{\_}2018{\_}Multi-agent generative adversarial imitation learning.pdf:pdf},
pages = {7461--7472},
title = {{Multi-agent generative adversarial imitation learning}},
year = {2018}
}
@misc{平成21年度：事86:online,
annote = {(Accessed on 01/20/2021)},
author = {公正取引委員会},
howpublished = {https://www.jftc.go.jp/dk/kiketsu/jirei/h21mokuji/h21jirei2.html},
title = {(平成21年度：事例2)新日本石油(株)と新日鉱ホールディングス(株)の経営統合}
}
@article{Drovandi2014ASM,
author = {Drovandi, C and McGree, J and Pettitt, A},
file = {:Users/hikaruasano/Documents/mendeley/Drovandi, McGree, Pettitt{\_}2014{\_}A Sequential Monte Carlo Algorithm to Incorporate Model Uncertainty in Bayesian Sequential Design.pdf:pdf},
journal = {Journal of Computational and Graphical Statistics},
pages = {24 -- 3},
title = {{A Sequential Monte Carlo Algorithm to Incorporate Model Uncertainty in Bayesian Sequential Design}},
volume = {23},
year = {2014}
}
@article{espeholt2019seed,
author = {Espeholt, Lasse and Marinier, Rapha{\"{e}}l and Stanczyk, Piotr and Wang, Ke and Michalski, Marcin},
file = {:Users/hikaruasano/Documents/mendeley/Espeholt et al.{\_}2019{\_}SEED RL Scalable and Efficient Deep-RL with Accelerated Central Inference.pdf:pdf},
journal = {arXiv preprint arXiv:1910.06591},
title = {{SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference}},
year = {2019}
}
@inproceedings{9683088,
author = {Li, Jingqi and Fridovich-Keil, David and Sojoudi, Somayeh and Tomlin, Claire J},
booktitle = {2021 60th IEEE Conference on Decision and Control (CDC)},
doi = {10.1109/CDC45484.2021.9683088},
file = {:Users/hikaruasano/Documents/mendeley/Li et al.{\_}2021{\_}Augmented Lagrangian Method for Instantaneously Constrained Reinforcement Learning Problems.pdf:pdf},
pages = {2982--2989},
title = {{Augmented Lagrangian Method for Instantaneously Constrained Reinforcement Learning Problems}},
year = {2021}
}
@article{giles2008multilevel,
author = {Giles, Michael B},
file = {:Users/hikaruasano/Documents/mendeley/Giles{\_}2008{\_}Multilevel monte carlo path simulation.pdf:pdf},
journal = {Operations research},
number = {3},
pages = {607--617},
publisher = {INFORMS},
title = {{Multilevel monte carlo path simulation}},
volume = {56},
year = {2008}
}
@article{Bunting2013TheWO,
author = {Bunting, W and Garcia, L and Edwards, Ezekiel},
journal = {PSN: Politics of Race},
pages = {p.47},
title = {{The War on Marijuana in Black and White}},
year = {2013}
}
@inbook{inbook,
author = {Perkins, H},
doi = {10.1007/978-3-319-05308-0_2},
file = {:Users/hikaruasano/Documents/mendeley/Perkins{\_}2014{\_}Misperception Is Reality The “Reign of Error” About Peer Risk Behaviour Norms Among Youth and Young Adults.pdf:pdf},
isbn = {978-3-319-05307-3},
pages = {11--36},
title = {{Misperception Is Reality: The “Reign of Error” About Peer Risk Behaviour Norms Among Youth and Young Adults}},
year = {2014}
}
@inproceedings{10.5555/3463952.3464065,
abstract = {High-performing teams learn effective communication strategies to judiciously share information and reduce the cost of communication overhead. Within multi-agent reinforcement learning, synthesizing effective policies requires reasoning about when to communicate, whom to communicate with, and how to process messages. We propose a novel multi-agent reinforcement learning algorithm, Multi-Agent Graph-attentIon Communication (MAGIC), with a graph-attention communication protocol in which we learn 1) a Scheduler to help with the problems of when to communicate and whom to address messages to, and 2) a Message Processor using Graph Attention Networks (GATs) with dynamic graphs to deal with communication signals. The Scheduler consists of a graph attention encoder and a differentiable attention mechanism, which outputs dynamic, differentiable graphs to the Message Processor, which enables the Scheduler and Message Processor to be trained end-to-end. We evaluate our approach on a variety of cooperative tasks, including Google Research Football. Our method outperforms baselines across all domains, achieving {\~{}}10.5{\%} increase in reward in the most challenging domain. We also show MAGIC communicates {\$}27.4{\%}{\$} more efficiently on average than baselines, is robust to stochasticity, and scales to larger state-action spaces. Finally, we demonstrate MAGIC on a physical, multi-robot testbed.},
address = {Richland, SC},
author = {Niu, Yaru and Paleja, Rohan and Gombolay, Matthew},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
file = {:Users/hikaruasano/Documents/mendeley/Kamiran, {\v{Z}}liobait.e{\_}2013{\_}Explainable and non-explainable discrimination in classification(2).pdf:pdf},
isbn = {9781450383073},
keywords = {graph-based communication,multi-agent communication,multi-agent reinforcement learning},
pages = {964--973},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
series = {AAMAS '21},
title = {{Multi-Agent Graph-Attention Communication and Teaming}},
year = {2021}
}
@inproceedings{scetbon2021equitable,
author = {Scetbon, Meyer and Meunier, Laurent and Atif, Jamal and Cuturi, Marco},
booktitle = {International Conference on Artificial Intelligence and Statistics},
file = {:Users/hikaruasano/Documents/mendeley/Scetbon et al.{\_}2021{\_}Equitable and optimal transport with multiple agents.pdf:pdf},
organization = {PMLR},
pages = {2035--2043},
title = {{Equitable and optimal transport with multiple agents}},
year = {2021}
}
@inproceedings{8790204,
author = {Sun, Yuan and Li, Xiaodong and Ernst, Andreas and Omidvar, Mohammad Nabi},
booktitle = {2019 IEEE Congress on Evolutionary Computation (CEC)},
doi = {10.1109/CEC.2019.8790204},
file = {:Users/hikaruasano/Documents/mendeley/Sun et al.{\_}2019{\_}Decomposition for Large-scale Optimization Problems with Overlapping Components.pdf:pdf},
pages = {326--333},
title = {{Decomposition for Large-scale Optimization Problems with Overlapping Components}},
year = {2019}
}
@article{HUAN2013288,
abstract = {The optimal selection of experimental conditions is essential to maximizing the value of data for inference and prediction, particularly in situations where experiments are time-consuming and expensive to conduct. We propose a general mathematical framework and an algorithmic approach for optimal experimental design with nonlinear simulation-based models; in particular, we focus on finding sets of experiments that provide the most information about targeted sets of parameters. Our framework employs a Bayesian statistical setting, which provides a foundation for inference from noisy, indirect, and incomplete data, and a natural mechanism for incorporating heterogeneous sources of information. An objective function is constructed from information theoretic measures, reflecting expected information gain from proposed combinations of experiments. Polynomial chaos approximations and a two-stage Monte Carlo sampling method are used to evaluate the expected information gain. Stochastic approximation algorithms are then used to make optimization feasible in computationally intensive and high-dimensional settings. These algorithms are demonstrated on model problems and on nonlinear parameter inference problems arising in detailed combustion kinetics.},
author = {Huan, Xun and Marzouk, Youssef M},
doi = {https://doi.org/10.1016/j.jcp.2012.08.013},
issn = {0021-9991},
journal = {Journal of Computational Physics},
keywords = {Bayesian inference,Chemical kinetics,Nonlinear experimental design,Optimal experimental design,Shannon information,Stochastic approximation,Uncertainty quantification},
number = {1},
pages = {288--317},
title = {{Simulation-based optimal Bayesian experimental design for nonlinear systems}},
url = {https://www.sciencedirect.com/science/article/pii/S0021999112004597},
volume = {232},
year = {2013}
}
@article{doi:10.1146/annurev-soc-073117-041106,
abstract = {Machine learning is a field at the intersection of statistics and computer science that uses algorithms to extract information and knowledge from data. Its applications increasingly find their way into economics, political science, and sociology. We offer a brief introduction to this vast toolbox and illustrate its current uses in the social sciences, including distilling measures from new data sources, such as text and images; characterizing population heterogeneity; improving causal inference; and offering predictions to aid policy decisions and theory development. We argue that, in addition to serving similar purposes in sociology, machine learning tools can speak to long-standing questions on the limitations of the linear modeling framework, the criteria for evaluating empirical findings, transparency around the context of discovery, and the epistemological core of the discipline.},
author = {Molina, Mario and Garip, Filiz},
doi = {10.1146/annurev-soc-073117-041106},
file = {:Users/hikaruasano/Documents/mendeley/Molina, Garip{\_}2019{\_}Machine Learning for Sociology.pdf:pdf},
journal = {Annual Review of Sociology},
number = {1},
pages = {27--45},
title = {{Machine Learning for Sociology}},
url = {https://doi.org/10.1146/annurev-soc-073117-041106},
volume = {45},
year = {2019}
}
@book{BB28189968,
author = {Salganik, Matthew J and 裕貴, 瀧川 and 淳, 常松 and 拓人, 阪本 and 真也, 大林},
publisher = {有斐閣},
title = {ビット・バイ・ビット : デジタル社会調査入門},
url = {https://ci.nii.ac.jp/ncid/BB28189968},
year = {2019}
}
@inbook{Kamiran2013,
abstract = {Nowadays more and more decisions in lending, recruitment, grant or study applications are partially being automated based on computational models (classifiers) premised on historical data. If the historical data was discriminating towards socially and legally protected groups, a model learnt over this data will make discriminatory decisions in the future. As a solution, most of the discrimination free modeling techniques force the treatment of the sensitive groups to be equal and do not take into account that some differences may be explained by other factors and thus justified. For example, disproportional recruitment rates for males and females may be explainable by the fact that more males have higher education; treating males and females equally will introduce reverse discrimination, which may be undesirable as well. Given that the law or domain experts specify which factors are discriminatory (e.g. gender, marital status) and which can be used for explanation (e.g. education), this chapter presents a methodology how to quantify the tolerable difference in treatment of the sensitive groups. We instruct how to measure, which part of the difference is explainable and present the local learning techniques that remove exactly the illegal discrimination, allowing the differences in decisions to be present as long as they are explainable.},
address = {Berlin, Heidelberg},
author = {Kamiran, Faisal and {\v{Z}}liobaitė, Indrė},
booktitle = {Discrimination and Privacy in the Information Society: Data Mining and Profiling in Large Databases},
doi = {10.1007/978-3-642-30487-3_8},
editor = {Custers, Bart and Calders, Toon and Schermer, Bart and Zarsky, Tal},
file = {:Users/hikaruasano/Documents/mendeley/Kamiran, {\v{Z}}liobaitė{\_}2013{\_}Explainable and Non-explainable Discrimination in Classification.pdf:pdf},
isbn = {978-3-642-30487-3},
pages = {155--170},
publisher = {Springer Berlin Heidelberg},
title = {{Explainable and Non-explainable Discrimination in Classification}},
url = {https://doi.org/10.1007/978-3-642-30487-3{\_}8},
year = {2013}
}
@article{Long2013FastEO,
author = {Long, Quan and Scavino, M and Tempone, R and Wang, Suojin},
journal = {Computer Methods in Applied Mechanics and Engineering},
pages = {24--39},
title = {{Fast estimation of expected information gains for Bayesian experimental designs based on Laplace approximations}},
volume = {259},
year = {2013}
}
@misc{原子力発電のグリ31:online,
annote = {(Accessed on 01/31/2022)},
author = {日経ビジネス電子版},
howpublished = {https://business.nikkei.com/atcl/seminar/19/00023/011100304/，1月31日閲覧},
title = {{原子力発電のグリーン認定をめぐりドイツとEUが対立}},
year = {2021}
}
@inproceedings{qin2021learning,
author = {Qin, Zengyi and Zhang, Kaiqing and Chen, Yuxiao and Chen, Jingkai and Fan, Chuchu},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Qin et al.{\_}2021{\_}Learning Safe Multi-agent Control with Decentralized Neural Barrier Certificates.pdf:pdf},
title = {{Learning Safe Multi-agent Control with Decentralized Neural Barrier Certificates}},
url = {https://openreview.net/forum?id=P6{\_}q1BRxY8Q},
year = {2021}
}
@inproceedings{pirotta2015multi,
author = {Pirotta, Matteo and Parisi, Simone and Restelli, Marcello},
booktitle = {Twenty-Ninth AAAI Conference on Artificial Intelligence},
file = {:Users/hikaruasano/Documents/mendeley/Pirotta, Parisi, Restelli{\_}2015{\_}Multi-objective reinforcement learning with continuous pareto frontier approximation.pdf:pdf},
title = {{Multi-objective reinforcement learning with continuous pareto frontier approximation}},
year = {2015}
}
@article{lecun2015deep,
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
journal = {nature},
number = {7553},
pages = {436--444},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@article{1390572174784642432,
author = {愛彦, 国里},
doi = {10.34360/00011014},
journal = {専修人間科学論集. 心理学篇},
pages = {21--33},
publisher = {専修大学人間科学学会},
title = {再現可能な心理学研究入門},
url = {https://cir.nii.ac.jp/crid/1390572174784642432},
volume = {10},
year = {2020}
}
@inproceedings{NEURIPS2019_b4189d9d,
author = {Zhao, Han and Gordon, Geoff},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d$\backslash$textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
file = {:Users/hikaruasano/Documents/mendeley/Zhao, Gordon{\_}2019{\_}Inherent Tradeoffs in Learning Fair Representations.pdf:pdf},
publisher = {Curran Associates, Inc.},
title = {{Inherent Tradeoffs in Learning Fair Representations}},
url = {https://proceedings.neurips.cc/paper/2019/file/b4189d9de0fb2b9cce090bd1a15e3420-Paper.pdf},
volume = {32},
year = {2019}
}
@inproceedings{10.5555/3327757.3327904,
abstract = {In many real-world reinforcement learning (RL) problems, besides optimizing the main objective function, an agent must concurrently avoid violating a number of constraints. In particular, besides optimizing performance, it is crucial to guarantee the safety of an agent during training as well as deployment (e.g., a robot should avoid taking actions - exploratory or not - which irrevocably harm its hardware). To incorporate safety in RL, we derive algorithms under the framework of constrained Markov decision processes (CMDPs), an extension of the standard Markov decision processes (MDPs) augmented with constraints on expected cumulative costs. Our approach hinges on a novel Lyapunov method. We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local linear constraints. Leveraging these theoretical underpinnings, we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. To illustrate their effectiveness, we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.},
address = {Red Hook, NY, USA},
author = {Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
file = {:Users/hikaruasano/Documents/mendeley/Chow et al.{\_}2018{\_}A Lyapunov-Based Approach to Safe Reinforcement Learning.pdf:pdf},
pages = {8103--8112},
publisher = {Curran Associates Inc.},
series = {NIPS'18},
title = {{A Lyapunov-Based Approach to Safe Reinforcement Learning}},
year = {2018}
}
@article{Sutskever2009,
abstract = {The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difficulty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM, which is a very slight modification of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls.},
author = {Sutskever, Ilya and Hinton, Geoffrey and Taylor, Graham},
file = {:Users/hikaruasano/Documents/mendeley/Sutskever, Hinton, Taylor{\_}2009{\_}The recurrent temporal restricted boltzmann machine.pdf:pdf},
isbn = {9781605609492},
journal = {Advances in Neural Information Processing Systems 21 - Proceedings of the 2008 Conference},
pages = {1601--1608},
title = {{The recurrent temporal restricted boltzmann machine}},
year = {2009}
}
@article{sun2020novel,
author = {Sun, Hao and Peng, Zhenghao and Dai, Bo and Guo, Jian and Lin, Dahua and Zhou, Bolei},
journal = {arXiv preprint arXiv:2005.10696},
title = {{Novel policy seeking with constrained optimization}},
year = {2020}
}
@article{liu2004sleep,
author = {Liu, Xianchen},
file = {:Users/hikaruasano/Documents/mendeley/Liu{\_}2004{\_}Sleep and adolescent suicidal behavior.pdf:pdf},
journal = {Sleep},
number = {7},
pages = {1351--1358},
publisher = {Oxford University Press},
title = {{Sleep and adolescent suicidal behavior}},
volume = {27},
year = {2004}
}
@article{10.1613/jair.1.12889,
abstract = {Opponent modeling is the ability to use prior knowledge and observations in order to predict the behavior of an opponent. This survey presents a comprehensive overview of existing opponent modeling techniques for adversarial domains, many of which must address stochastic, continuous, or concurrent actions, and sparse, partially observable payoff structures. We discuss all the components of opponent modeling systems, including feature extraction, learning algorithms, and strategy abstractions. These discussions lead us to propose a new form of analysis for describing and predicting the evolution of game states over time. We then introduce a new framework that facilitates method comparison, analyze a representative selection of techniques using the proposed framework, and highlight common trends among recently proposed methods. Finally, we list several open problems and discuss future research directions inspired by AI research on opponent modeling and related research in other disciplines.},
address = {El Segundo, CA, USA},
author = {Nashed, Samer and Zilberstein, Shlomo},
doi = {10.1613/jair.1.12889},
file = {:Users/hikaruasano/Documents/mendeley/Nashed, Zilberstein{\_}2022{\_}A Survey of Opponent Modeling in Adversarial Domains.pdf:pdf},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
keywords = {game playing,machine learning,multiagent systems,software agents},
month = {may},
publisher = {AI Access Foundation},
title = {{A Survey of Opponent Modeling in Adversarial Domains}},
url = {https://doi.org/10.1613/jair.1.12889},
volume = {73},
year = {2022}
}
@inproceedings{chen2017decentralized,
author = {Chen, Yu Fan and Liu, Miao and Everett, Michael and How, Jonathan P},
booktitle = {2017 IEEE international conference on robotics and automation (ICRA)},
file = {:Users/hikaruasano/Documents/mendeley/Chen et al.{\_}2017{\_}Decentralized non-communicating multiagent collision avoidance with deep reinforcement learning.pdf:pdf},
organization = {IEEE},
pages = {285--292},
title = {{Decentralized non-communicating multiagent collision avoidance with deep reinforcement learning}},
year = {2017}
}
@article{soc11010009,
abstract = {Reliability, transparency, and ethical crises pushed many social science disciplines toward dramatic changes, in particular psychology and more recently political science. This paper discusses why sociology should also change. It reviews sociology as a discipline through the lens of current practices, definitions of sociology, positions of sociological associations, and a brief consideration of the arguments of three highly influential yet epistemologically diverse sociologists: Weber, Merton, and Habermas. It is a general overview for students and sociologists to quickly familiarize themselves with the state of sociology or explore the idea of open science and its relevance to their discipline.},
author = {Breznau, Nate},
doi = {10.3390/soc11010009},
file = {:Users/hikaruasano/Documents/mendeley/Breznau{\_}2021{\_}Does Sociology Need Open Science.pdf:pdf},
issn = {2075-4698},
journal = {Societies},
number = {1},
title = {{Does Sociology Need Open Science?}},
url = {https://www.mdpi.com/2075-4698/11/1/9},
volume = {11},
year = {2021}
}
@article{Wiedmann2015,
abstract = {This original research paper addresses a key issue in sustainability science: How many and which natural resources are needed to sustain modern economies? Simple as it may seem, this question is far from trivial to answer and has indeed not been addressed satisfactorily in the scholarly literature. We use the most comprehensive and most highly resolved economic input–output framework of the world economy together with a detailed database of global material flows to calculate the full material requirements of all countries covering a period of two decades. Called the “material footprint,” this indicator provides a consumption perspective of resource use and new insights into the actual resource productivity of nations.},
author = {Wiedmann, Thomas O. and Schandl, Heinz and Lenzen, Manfred and Moran, Daniel and Suh, Sangwon and West, James and Kanemoto, Keiichiro},
doi = {10.1073/PNAS.1220362110},
file = {:Users/hikaruasano/Documents/mendeley/Wiedmann et al.{\_}2015{\_}The material footprint of nations.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
month = {may},
number = {20},
pages = {6271--6276},
pmid = {24003158},
publisher = {National Academy of Sciences},
title = {{The material footprint of nations}},
url = {https://www.pnas.org/content/112/20/6271},
volume = {112},
year = {2015}
}
@inproceedings{kapturowski2018recurrent,
author = {Kapturowski, Steven and Ostrovski, Georg and Dabney, Will and Quan, John and Munos, Remi},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Kapturowski et al.{\_}2019{\_}Recurrent Experience Replay in Distributed Reinforcement Learning.pdf:pdf},
title = {{Recurrent Experience Replay in Distributed Reinforcement Learning}},
url = {https://openreview.net/forum?id=r1lyTjAqYX},
year = {2019}
}
@article{Baranes2013,
abstract = {We introduce the Self-Adaptive Goal Generation Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsically motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters. We present experiments with high-dimensional continuous sensorimotor spaces in three different robotic setups: (1) learning the inverse kinematics in a highly-redundant robotic arm, (2) learning omnidirectional locomotion with motor primitives in a quadruped robot, and (3) an arm learning to control a fishing rod with a flexible wire. We show that (1) exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots; (2) selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly, as well as more efficient than different standard active motor babbling methods; (3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1301.4862},
author = {Baranes, Adrien and Oudeyer, Pierre Yves},
doi = {10.1016/j.robot.2012.05.008},
eprint = {1301.4862},
file = {:Users/hikaruasano/Desktop/mendeley/BaranesOudeyerIROS10 (1).pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Active learning,Autonomous motor learning,Competence based intrinsic motivation,Curiosity-driven task space exploration,Developmental robotics,Goal babbling,Inverse models,Motor development},
month = {jan},
number = {1},
pages = {49--73},
title = {{Active learning of inverse models with intrinsically motivated goal exploration in robots}},
volume = {61},
year = {2013}
}
@techreport{Gelly2012,
abstract = {The ancient oriental game of Go has long been considered a grand challenge for artificial intelligence. For decades, computer Go has defied the classical methods in game tree search that worked so successfully for chess and checkers. However , recent play in computer Go has been transformed by a new paradigm for tree search based on Monte-Carlo methods. Programs based on Monte-Carlo tree search now play at human-master levels and are beginning to challenge top professional players. In this paper we describe the leading algorithms for Monte-Carlo tree search and explain how they have advanced the state of the art in computer Go.},
author = {Gelly, Sylvain and Kocsis, Levente and Schoenauer, Marc and Sebag, Mich{\`{e}}le and Silver, David and Szepesvari, Csaba and Teytaud, Olivier and Szepesv{\'{a}}ri, Csaba},
booktitle = {Communications-ACM},
file = {:Users/hikaruasano/Documents/mendeley/Gelly et al.{\_}2012{\_}The Grand Challenge of Computer Go Monte Carlo Tree Search and Extensions.pdf:pdf},
keywords = {()},
number = {3},
pages = {106--113},
publisher = {Association for Computing Machinery},
title = {{The Grand Challenge of Computer Go: Monte Carlo Tree Search and Extensions}},
url = {https://hal.inria.fr/hal-00695370v1},
volume = {55},
year = {2012}
}
@article{kapoor2022leakage,
author = {Kapoor, Sayash and Narayanan, Arvind},
file = {:Users/hikaruasano/Documents/mendeley/Kapoor, Narayanan{\_}2022{\_}Leakage and the reproducibility crisis in ML-based science.pdf:pdf},
journal = {arXiv preprint arXiv:2207.07048},
title = {{Leakage and the reproducibility crisis in ML-based science}},
year = {2022}
}
@misc{世界の自動車市場66:online,
annote = {(Accessed on 07/15/2020)},
author = {東洋経済オンライン},
howpublished = {https://toyokeizai.net/articles/-/211399},
title = {世界の自動車市場､成長の限界はどこなのか},
year = {2018}
}
@book{1986ポスト,
author = {Lyotard, Jean Fran{\c{c}}ois and 康夫, 小林},
isbn = {9784891761592},
publisher = {水声社},
series = {叢書言語の政治},
title = {ポスト・モダンの条件: 知・社会・言語ゲーム},
url = {https://books.google.co.jp/books?id=Y9-IAAAACAAJ},
year = {1986}
}
@article{article,
author = {Sodomka, E and Hilliard, E M and Littman, M L and Greenwald, Amy},
file = {:Users/hikaruasano/Documents/mendeley/Sodomka et al.{\_}2013{\_}Coco-Q Learning in stochastic games with side payments.pdf:pdf},
journal = {30th International Conference on Machine Learning, ICML 2013},
pages = {2521--2529},
title = {{Coco-Q: Learning in stochastic games with side payments}},
year = {2013}
}
@article{2008,
author = {坂地泰紀},
journal = {言語処理学会第14回年次大会論文集, 2008},
pages = {1144--1147},
title = {構文パターンを用いた因果関係の抽出},
url = {http://ci.nii.ac.jp/naid/10026776862/ja/},
year = {2008}
}
@inproceedings{xiong2022hisarl,
author = {Xiong, Zikang and Agarwal, Ishika and Jagannathan, Suresh},
booktitle = {SafeAI@ AAAI},
file = {:Users/hikaruasano/Documents/mendeley/Xiong, Agarwal, Jagannathan{\_}2022{\_}HiSaRL A Hierarchical Framework for Safe Reinforcement Learning.pdf:pdf},
title = {{HiSaRL: A Hierarchical Framework for Safe Reinforcement Learning.}},
year = {2022}
}
@book{BB27843602,
author = {持橋, 大地 and 大羽, 成征 and 講談社サイエンティフィク},
publisher = {講談社},
series = {MLP機械学習プロフェッショナルシリーズ},
title = {{ガウス過程と機械学習 = Gaussian process and machine learning}},
url = {http://ci.nii.ac.jp/ncid/BB27843602},
year = {2019}
}
@inproceedings{mordatch2018emergence,
author = {Mordatch, Igor and Abbeel, Pieter},
booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
file = {:Users/hikaruasano/Documents/mendeley/Mordatch, Abbeel{\_}2018{\_}Emergence of grounded compositional language in multi-agent populations.pdf:pdf},
title = {{Emergence of grounded compositional language in multi-agent populations}},
year = {2018}
}
@article{Kitazato2018,
abstract = {Inverse Reinforcement Learning (IRL) is a promising framework for estimating a reward function given the behavior of an expert.However, the IRL problem is ill-posed because infinitely many reward functions can be consistent with the expert's observed behavior. To resolve this issue, IRL algorithms have been proposed to determine alternative choices of the reward function that reproduce the behavior of the expert, but these algorithms do not consider the learning efficiency. In this paper, we propose a new formulation and algorithm for IRL to estimate the reward function that maximizes the learning efficiency. This new formulation is an extension of an existing IRL algorithm, and we introduce a genetic algorithm approach to solve the new reward function. We show the effectiveness of our approach by comparing the performance of our proposed method against existing algorithms.},
author = {Kitazato, Yuki and Arai, Sachiyo},
doi = {10.5220/0006729502760283},
file = {:Users/hikaruasano/Documents/mendeley/Kitazato, Arai{\_}2018{\_}Estimation of Reward Function Maximizing Learning Efficiency in Inverse Reinforcement Learning(2).pdf:pdf},
isbn = {9789897582752},
journal = {scitepress.org},
keywords = {Genetic Algorithm,Inverse Reinforcement Learning},
title = {{Estimation of Reward Function Maximizing Learning Efficiency in Inverse Reinforcement Learning}},
url = {https://gateway.itc.u-tokyo.ac.jp/papers/2018/67295/67295.pdf,DanaInfo=www.scitepress.org},
year = {2018}
}
@article{canese2021multi,
author = {Canese, Lorenzo and Cardarilli, Gian Carlo and {Di Nunzio}, Luca and Fazzolari, Rocco and Giardino, Daniele and Re, Marco and Span{\`{o}}, Sergio},
file = {:Users/hikaruasano/Documents/mendeley/Canese et al.{\_}2021{\_}Multi-agent reinforcement learning A review of challenges and applications.pdf:pdf},
journal = {Applied Sciences},
number = {11},
pages = {4948},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Multi-agent reinforcement learning: A review of challenges and applications}},
volume = {11},
year = {2021}
}
@article{Cook2008OptimalOT,
author = {Cook, A and Gibson, G and Gilligan, C},
journal = {Biometrics},
pages = {860--868},
title = {{Optimal observation times in experimental epidemic processes.}},
volume = {64 3},
year = {2008}
}
@article{Holland2018TheDN,
author = {Holland, Sarah and Hosny, A and Newman, Sarah and Joseph, Joshua and Chmielinski, Kasia},
journal = {ArXiv},
title = {{The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards}},
volume = {abs/1805.0},
year = {2018}
}
@article{published_papers/27831880,
author = {筒井淳也},
file = {:Users/hikaruasano/Documents/mendeley/筒井淳也{\_}2020{\_}文系縮小圧力のなかでの社会学の立ち位置：科学との類似性と異質性のあいだで.pdf:pdf},
journal = {フォーラム現代社会学},
pages = {48--58},
title = {文系縮小圧力のなかでの社会学の立ち位置：科学との類似性と異質性のあいだで},
volume = {19},
year = {2020}
}
@article{Friedler2016OnT,
author = {Friedler, Sorelle A and Scheidegger, C and Venkatasubramanian, Suresh},
file = {:Users/hikaruasano/Documents/mendeley/Friedler, Scheidegger, Venkatasubramanian{\_}2016{\_}On the (im)possibility of fairness.pdf:pdf},
journal = {ArXiv},
title = {{On the (im)possibility of fairness}},
volume = {abs/1609.0},
year = {2016}
}
@misc{WhyPower83:online,
annote = {(Accessed on 07/18/2020)},
author = {POWER},
howpublished = {https://www.powermag.com/why-power-to-gas-may-flourish-in-a-renewables-heavy-world/},
title = {{Why Power-to-Gas May Flourish in a Renewables-Heavy World}},
year = {2019}
}
@book{BC01389260,
author = {裕子編, 藤垣},
number = {2},
pages = {pp.86--93},
publisher = {東京大学出版会},
series = {科学技術社会論の挑戦 / 藤垣裕子責任編集 ; 小林傳司 [ほか] 協力編集},
title = {科学技術と社会 : 具体的課題群},
url = {https://ci.nii.ac.jp/ncid/BC01389260},
year = {2020}
}
@article{moody2022reproducibility,
author = {Moody, James W and Keister, Lisa A and Ramos, Maria C},
file = {:Users/hikaruasano/Documents/mendeley/Moody, Keister, Ramos{\_}2022{\_}Reproducibility in the social sciences.pdf:pdf},
journal = {Annual Review of Sociology},
pages = {65--85},
publisher = {Annual Reviews},
title = {{Reproducibility in the social sciences}},
volume = {48},
year = {2022}
}
@misc{筑波大学との特別35:online,
annote = {(Accessed on 01/12/2021)},
author = {Technologies, Pixie Dust},
howpublished = {https://pixiedusttech.com/news{\_}20180530/},
title = {筑波大学との特別共同研究事業契約の見直し及び新株予約権の発行に関するお知らせ},
year = {2018}
}
@inproceedings{zhang2018mitigating,
author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
file = {:Users/hikaruasano/Documents/mendeley/Zhang, Lemoine, Mitchell{\_}2018{\_}Mitigating unwanted biases with adversarial learning.pdf:pdf},
pages = {335--340},
title = {{Mitigating unwanted biases with adversarial learning}},
year = {2018}
}
@article{DBLP:journals/corr/abs-2005-08414,
author = {Goda, Takashi and Hironaka, Tomohiko and Kitade, Wataru},
journal = {CoRR},
title = {{Unbiased {\{}MLMC{\}} stochastic gradient-based optimization of Bayesian experimental designs}},
url = {https://arxiv.org/abs/2005.08414},
volume = {abs/2005.0},
year = {2020}
}
@article{Xie2021CongestionawareMT,
author = {Xie, Xu and Zhang, Chi and Zhu, Yixin and Wu, Ying Nian and Zhu, Song-Chun},
file = {:Users/hikaruasano/Documents/mendeley/Xie et al.{\_}2021{\_}Congestion-aware Multi-agent Trajectory Prediction for Collision Avoidance.pdf:pdf},
journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
pages = {13693--13700},
title = {{Congestion-aware Multi-agent Trajectory Prediction for Collision Avoidance}},
year = {2021}
}
@book{Eiben2015,
abstract = {An overview of evolutionary computing, the collective name for problem-solving techniques based on principles of biological evolution, such as natural selection and genetic inheritance. This book contains quick-reference information on related topics. It is of interest to evolutionary computing specialists and researchers working in other fields. TS - BibTeX // WorldCat},
author = {Eiben, A. E. and Smith, J. E.},
booktitle = {Natural Computing Series},
doi = {10.1007/978-3-662-44874-8},
file = {:Users/hikaruasano/Documents/mendeley/Eiben, Smith{\_}2015{\_}Natural Computing Series Introduction to Evolutionary Computing.pdf:pdf},
isbn = {978-3-662-44873-1},
issn = {1063-6560},
pages = {287},
pmid = {24981987},
title = {{Natural Computing Series Introduction to Evolutionary Computing}},
year = {2015}
}
@inproceedings{9636568,
author = {Meng, Yue and Qin, Zengyi and Fan, Chuchu},
booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS51168.2021.9636568},
file = {:Users/hikaruasano/Documents/mendeley/Meng, Qin, Fan{\_}2021{\_}Reactive and Safe Road User Simulations using Neural Barrier Certificates.pdf:pdf},
pages = {6299--6306},
title = {{Reactive and Safe Road User Simulations using Neural Barrier Certificates}},
year = {2021}
}
@article{Wakefield1994AnEL,
author = {Wakefield, J},
journal = {The Statistician},
pages = {13--29},
title = {{An Expected Loss Approach to the Design of Dosage Regimens Via Sampling‐Based Methods}},
volume = {43},
year = {1994}
}
@inproceedings{pmlr-v115-jiang20a,
abstract = {We propose an approach to fair classification that enforces independence between the classifier outputs and sensitive information by minimizing Wasserstein-1 distances. The approach has desirable theoretical properties and is robust to specific choices of the threshold used to obtain class predictions from model outputs.We introduce different methods that enable hid-ing sensitive information at test time or have a simple and fast implementation. We show empirical performance against different fair-ness baselines on several benchmark fairness datasets.},
author = {Jiang, Ray and Pacchiano, Aldo and Stepleton, Tom and Jiang, Heinrich and Chiappa, Silvia},
booktitle = {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
editor = {Adams, Ryan P and Gogate, Vibhav},
file = {:Users/hikaruasano/Documents/mendeley/Jiang et al.{\_}2020{\_}Wasserstein Fair Classification.pdf:pdf},
pages = {862--872},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Wasserstein Fair Classification}},
url = {https://proceedings.mlr.press/v115/jiang20a.html},
volume = {115},
year = {2020}
}
@article{Warren2017,
abstract = {We describe a protocol that facilitates low friction peer-to-peer exchange of ERC20 tokens on the Ethereum blockchain. The protocol is intended to serve as an open standard and common building block, driving interoperability among decentralized applications (dApps) that incorporate exchange functionality. Trades are executed by a system of Ethereum smart contracts that are publicly acces-sible, free to use and that any dApp can hook into. DApps built on top of the protocol can access public liquidity pools or create their own liquidity pool and charge transaction fees on the resulting volume. The protocol is unopinionated: it does not impose costs on its users or arbitrarily extract value from one group of users to benefit another. Decentralized governance is used to continuously and securely integrate updates into the base protocol without disrupting dApps or end users.},
author = {Warren, Will and Bandeali, Amir},
doi = {10.1007/s004649901080},
file = {:Users/hikaruasano/Documents/mendeley/Warren, Bandeali{\_}2017{\_}0x An open protocol for decentralized exchange on the Ethereum blockchain.pdf:pdf},
issn = {09302794},
journal = {URl: https://github. com/0xProject {\ldots}},
pages = {1--16},
title = {{0x : An open protocol for decentralized exchange on the Ethereum blockchain}},
url = {https://upload.icodreamer.com/whitepaper/1531460555.pdf},
year = {2017}
}
@article{Degris2012OffPolicyA,
author = {Degris, T and White, Martha and Sutton, R},
file = {:Users/hikaruasano/Documents/mendeley/Degris, White, Sutton{\_}2012{\_}Off-Policy Actor-Critic.pdf:pdf},
journal = {ArXiv},
title = {{Off-Policy Actor-Critic}},
volume = {abs/1205.4},
year = {2012}
}
@inproceedings{10.5555/3398761.3398941,
abstract = {Human players in professional team sports achieve high level coordination by dynamically choosing complementary skills and executing primitive actions to perform these skills. As a step toward creating intelligent agents with this capability for fully cooperative multi-agent settings, we propose a two-level hierarchical multi-agent reinforcement learning (MARL) algorithm with unsupervised skill discovery. Agents learn useful and distinct skills at the low level via independent Q-learning, while they learn to select complementary latent skill variables at the high level via centralized multi-agent training with an extrinsic team reward. The set of low-level skills emerges from an intrinsic reward that solely promotes the decodability of latent skill variables from the trajectory of a low-level skill, without the need for hand-crafted rewards for each skill. For scalable decentralized execution, each agent independently chooses latent skill variables and primitive actions based on local observations. Our overall method enables the use of general cooperative MARL algorithms for training high level policies and single-agent RL for training low level skills. Experiments on a stochastic high dimensional team game show the emergence of useful skills and cooperative team play. The interpretability of the learned skills show the promise of the proposed method for achieving human-AI cooperation in team sports games.},
address = {Richland, SC},
author = {Yang, Jiachen and Borovikov, Igor and Zha, Hongyuan},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
file = {:Users/hikaruasano/Documents/mendeley/Yang, Borovikov, Zha{\_}2020{\_}Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill Discovery.pdf:pdf},
isbn = {9781450375184},
keywords = {hierarchical learning,multi-agent learning,option discovery},
pages = {1566--1574},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
series = {AAMAS '20},
title = {{Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill Discovery}},
year = {2020}
}
@article{Ohsawa2020,
author = {Ohsawa, Shohei},
file = {:Users/hikaruasano/Documents/mendeley/Ohsawa{\_}2020{\_}Generative Interaction Networks.pdf:pdf},
title = {{Generative Interaction Networks}},
year = {2020}
}
@techreport{Clyde96exploringexpected,
author = {Clyde, Merlise A and M{\"{u}}ller, Peter and Parmigiani, Giovanni},
file = {:Users/hikaruasano/Documents/mendeley/Clyde, M{\"{u}}ller, Parmigiani{\_}1996{\_}Exploring Expected Utility Surfaces by Markov Chains.pdf:pdf},
title = {{Exploring Expected Utility Surfaces by Markov Chains}},
year = {1996}
}
@article{doi:10.1086/702899,
abstract = {Social influence may lead individuals to choose what is popular over what is best. Whenever this happens, it further increases the popularity advantage of the inferior choice, compelling subsequent decision makers to follow suit. The author argues that despite this positive feedback effect, discordances between popularity and quality will usually self-correct. Reanalyzing past experimental studies in which social information initially heavily favored inferior options, the author shows that in each experiment superior alternatives gained in popularity. This article also reports on a new experiment in which a larger number of subject choices allowed trials to be run to convergence and shows that in each trial the superior alternative eventually achieved popular dominance. To explain the persistent dominance of bestsellers, celebrities, and memes of seemingly questionable quality in everyday life in terms of social influence processes, one must identify conditions that render positive feedback so strong that self-correcting dynamics are prevented.},
author = {van de Rijt, Arnout},
doi = {10.1086/702899},
file = {:Users/hikaruasano/Documents/mendeley/van de Rijt{\_}2019{\_}Self-Correcting Dynamics in Social Influence Processes.pdf:pdf},
journal = {American Journal of Sociology},
number = {5},
pages = {1468--1495},
title = {{Self-Correcting Dynamics in Social Influence Processes}},
url = {https://doi.org/10.1086/702899},
volume = {124},
year = {2019}
}
@book{2000,
abstract = {本体価格: 2400円},
author = {照夫, 小山 and 国立情報学研究所},
isbn = {4621048007},
number = {2},
publisher = {丸善},
title = {知識モデリング},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2001215149},
year = {2000}
}
@inproceedings{10.1609/aaai.v33i01.33013387,
abstract = {Reinforcement Learning (RL) algorithms have found limited success beyond simulated applications, and one main reason is the absence of safety guarantees during the learning process. Real world systems would realistically fail or break before an optimal controller can be learned. To address this issue, we propose a controller architecture that combines (1) a model-free RL-based controller with (2) model-based controllers utilizing control barrier functions (CBFs) and (3) online learning of the unknown system dynamics, in order to ensure safety during learning. Our general framework leverages the success of RL algorithms to learn high-performance controllers, while the CBF-based controllers both guarantee safety and guide the learning process by constraining the set of explorable polices. We utilize Gaussian Processes (GPs) to model the system dynamics and its uncertainties.Our novel controller synthesis algorithm, RL-CBF., guarantees safety with high probability during the learning process, regardless of the RL algorithm used, and demonstrates greater policy exploration efficiency. We test our algorithm on (1) control of an inverted pendulum and (2) autonomous car-following with wireless vehicle-to-vehicle communication, and show that our algorithm attains much greater sample efficiency in learning than other state-of-the-art algorithms and maintains safety during the entire learning process.},
author = {Cheng, Richard and Orosz, G{\'{a}}bor and Murray, Richard M and Burdick, Joel W},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
doi = {10.1609/aaai.v33i01.33013387},
file = {:Users/hikaruasano/Documents/mendeley/Cheng et al.{\_}2019{\_}End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks.pdf:pdf},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
series = {AAAI'19/IAAI'19/EAAI'19},
title = {{End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks}},
url = {https://doi.org/10.1609/aaai.v33i01.33013387},
year = {2019}
}
@misc{2021102253:online,
annote = {(Accessed on 01/14/2022)},
author = {経済産業省},
howpublished = {https://www.meti.go.jp/press/2021/10/20211022005/20211022005-1.pdf},
title = {エネルギー基本計画},
year = {2021}
}
@article{永井明彦2014挑戦的,
author = {永井明彦},
journal = {開発工学},
number = {2},
pages = {123--126},
publisher = {日本開発工学会},
title = {{挑戦的 SME の新事業創出を実現するメディエータ}},
volume = {33},
year = {2014}
}
@article{Krishnan2016HIRLHI,
author = {Krishnan, Sanjay and Garg, Animesh and Liaw, Richard and Miller, Lauren and Pokorny, Florian T and Goldberg, Kenneth Y},
file = {:Users/hikaruasano/Documents/mendeley/Krishnan et al.{\_}2016{\_}HIRL Hierarchical Inverse Reinforcement Learning for Long-Horizon Tasks with Delayed Rewards.pdf:pdf},
journal = {ArXiv},
title = {{HIRL: Hierarchical Inverse Reinforcement Learning for Long-Horizon Tasks with Delayed Rewards}},
volume = {abs/1604.0},
year = {2016}
}
@inproceedings{pmlr-v155-xie21a,
abstract = {Seamlessly interacting with humans or robots is hard because these agents are non-stationary. They update their policy in response to the ego agent's behavior, and the ego agent must anticipate these changes to co-adapt. Inspired by humans, we recognize that robots do not need to explicitly model every low-level action another agent will make; instead, we can capture the latent strategy of other agents through high-level representations. We propose a reinforcement learning-based framework for learning latent representations of an agent's policy, where the ego agent identifies the relationship between its behavior and the other agent's future strategy. The ego agent then leverages these latent dynamics to influence the other agent, purposely guiding them towards policies suitable for co-adaptation. Across several simulated domains and a real-world air hockey game, our approach outperforms the alternatives and learns to influence the other agent.},
author = {Xie, Annie and Losey, Dylan and Tolsma, Ryan and Finn, Chelsea and Sadigh, Dorsa},
booktitle = {Proceedings of the 2020 Conference on Robot Learning},
editor = {Kober, Jens and Ramos, Fabio and Tomlin, Claire},
file = {:Users/hikaruasano/Documents/mendeley/Xie et al.{\_}2021{\_}Learning Latent Representations to Influence Multi-Agent Interaction.pdf:pdf},
pages = {575--588},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Learning Latent Representations to Influence Multi-Agent Interaction}},
url = {https://proceedings.mlr.press/v155/xie21a.html},
volume = {155},
year = {2021}
}
@article{Nabi2019LearningOF,
author = {Nabi, Razieh and Malinsky, Daniel and Shpitser, Ilya},
file = {:Users/hikaruasano/Documents/mendeley/Nabi, Malinsky, Shpitser{\_}2019{\_}Learning Optimal Fair Policies.pdf:pdf},
journal = {International Conference on Machine Learning},
pages = {4674--4682},
title = {{Learning Optimal Fair Policies}},
volume = {97},
year = {2019}
}
@inproceedings{li2022hyar,
author = {Li, Boyan and Tang, Hongyao and ZHENG, Y A N and HAO, Jianye and Li, Pengyi and Wang, Zhen and Meng, Zhaopeng and Wang, L I},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Li et al.{\_}2022{\_}Hy{\{}AR{\}} Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation.pdf:pdf},
title = {{Hy{\{}AR{\}}: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation}},
url = {https://openreview.net/forum?id=64trBbOhdGU},
year = {2022}
}
@article{zhang2018fully,
author = {Zhang, Kaiqing and Yang, Zhuoran and Liu, Han and Zhang, Tong and Ba$\backslash$csar, Tamer},
file = {:Users/hikaruasano/Documents/mendeley/Zhang et al.{\_}2018{\_}Fully decentralized multi-agent reinforcement learning with networked agents.pdf:pdf},
journal = {arXiv preprint arXiv:1802.08757},
title = {{Fully decentralized multi-agent reinforcement learning with networked agents}},
year = {2018}
}
@inproceedings{grupen2022cooperative,
author = {Grupen, Niko A and Selman, Bart and Lee, Daniel D},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
file = {:Users/hikaruasano/Documents/mendeley/Grupen, Selman, Lee{\_}2022{\_}Cooperative Multi-Agent Fairness and Equivariant Policies.pdf:pdf},
number = {9},
pages = {9350--9359},
title = {{Cooperative Multi-Agent Fairness and Equivariant Policies}},
volume = {36},
year = {2022}
}
@misc{sheng2020learning,
author = {Sheng, Junjie and Wang, Xiangfeng and Jin, Bo and Yan, Junchi and Li, Wenhao and Chang, Tsung-Hui and Wang, Jun and Zha, Hongyuan},
file = {:Users/hikaruasano/Documents/mendeley/Sheng et al.{\_}2020{\_}Learning Structured Communication for Multi-agent Reinforcement Learning.pdf:pdf},
title = {{Learning Structured Communication for Multi-agent Reinforcement Learning}},
url = {https://openreview.net/forum?id=BklWt24tvH},
year = {2020}
}
@article{liu2021multimodal,
abstract = {Predicting multiple plausible future trajectories of the nearby vehicles is crucial for the safety of autonomous driving. Recent motion prediction approaches attempt to achieve such multimodal motion prediction by implicitly regularizing the feature or explicitly generating multiple candidate proposals. However, it remains challenging since the latent features may concentrate on the most frequent mode of the data while the proposal-based methods depend largely on the prior knowledge to generate and select the proposals. In this work, we propose a novel transformer framework for multimodal motion prediction, termed as mmTransformer. A novel network architecture based on stacked transformers is designed to model the multimodal- ity at feature level with a set of fixed independent propos- als. A region-based training strategy is then developed to induce the multimodality of the generated proposals. Ex- periments on Argoverse dataset show that the proposed model achieves the state-of-the-art performance on mo- tion prediction, substantially improving the diversity and the accuracy of the predicted trajectories. Demo video and code are available at https://decisionforce. github.io/mmTransformer.},
author = {Liu, Yicheng and Zhang, Jinghuai and Fang, Liangji and Jiang, Qinhong and Zhou, Bolei},
file = {:Users/hikaruasano/Documents/mendeley/Liu et al.{\_}2021{\_}Multimodal Motion Prediction with Stacked Transformers.pdf:pdf},
journal = {Computer Vision and Pattern Recognition},
title = {{Multimodal Motion Prediction with Stacked Transformers}},
year = {2021}
}
@inproceedings{pmlr-v70-koh17a,
abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
author = {Koh, Pang Wei and Liang, Percy},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
editor = {Precup, Doina and Teh, Yee Whye},
pages = {1885--1894},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Understanding Black-box Predictions via Influence Functions}},
url = {https://proceedings.mlr.press/v70/koh17a.html},
volume = {70},
year = {2017}
}
@article{Lin2020,
abstract = {We study distributed reinforcement learning (RL) for a network of agents. The objective is to find localized policies that maximize the (discounted) global reward. In general, scalability is a challenge in this setting because the size of the global state/action space can be exponential in the number of agents. Scalable algorithms are only known in cases where dependencies are local, e.g., between neighbors. In this work, we propose a Scalable Actor Critic framework that applies in settings where the dependencies are non-local and provide a finite-time error bound that shows how the convergence rate depends on the depth of the dependencies in the network. Additionally, as a byproduct of our analysis, we obtain novel finite-time convergence results for a general stochastic approximation scheme and for temporal difference learning with state aggregation that apply beyond the setting of RL in networked systems.},
archivePrefix = {arXiv},
arxivId = {2006.06555},
author = {Lin, Yiheng and Qu, Guannan and Huang, Longbo and Wierman, Adam},
eprint = {2006.06555},
file = {:Users/hikaruasano/Documents/mendeley/Lin et al.{\_}2020{\_}Distributed Reinforcement Learning in Multi-Agent Networked Systems.pdf:pdf},
month = {jun},
title = {{Distributed Reinforcement Learning in Multi-Agent Networked Systems}},
url = {http://arxiv.org/abs/2006.06555},
year = {2020}
}
@misc{次世代自動車戦略20:online,
annote = {(Accessed on 07/22/2020)},
author = {次世代自動車戦略2010},
howpublished = {https://www.hkd.meti.go.jp/hokis/mono{\_}kondan2/data02{\_}2.pdf},
title = {2010年4月次世代自動車研究会},
year = {2010}
}
@inproceedings{DBLP:journals/corr/KingmaB14,
author = {Kingma, Diederik P and Ba, Jimmy},
booktitle = {3rd International Conference on Learning Representations, {\{}ICLR{\}} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
editor = {Bengio, Yoshua and LeCun, Yann},
title = {{Adam: {\{}A{\}} Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2015}
}
@article{根笹賢一2007カーナビ経路探索における運転者希望経路の反映,
author = {根笹賢一 and 宮岡伸一郎},
journal = {情報処理学会研究報告高度交通システム (ITS)},
number = {116 (2007-ITS-031)},
pages = {1--7},
title = {カーナビ経路探索における運転者希望経路の反映},
volume = {2007},
year = {2007}
}
@book{DoshishaDaigaku.Shogakkai.2015,
author = {Murai, Akihiko},
booktitle = {同志社商学},
issn = {0387-2858},
number = {6},
pages = {1161--1214},
publisher = {同志社大學商學会},
title = {{Dōshisha shōgaku.}},
url = {http://ci.nii.ac.jp/naid/120005641870/ja/},
volume = {66},
year = {2015}
}
@book{2001,
author = {浩紀, 東},
publisher = {講談社現代新書},
title = {動物化するポストモダン : オタクから見た日本社会},
year = {2001}
}
@misc{プラットフォーム15:online,
annote = {(Accessed on 01/21/2021)},
author = {寿也, 実積},
booktitle = {JILISレポート},
howpublished = {https://jilis.org/report/2020/jilisreport-vol3no1.pdf},
title = {プラットフォーム事業をめぐる競争政策についての論点整理},
volume = {3},
year = {2020}
}
@article{yang2020learning,
author = {Yang, Jiachen and Li, Ang and Farajtabar, Mehrdad and Sunehag, Peter and Hughes, Edward and Zha, Hongyuan},
file = {:Users/hikaruasano/Documents/mendeley/Yang et al.{\_}2020{\_}Learning to Incentivize Other Learning Agents.pdf:pdf},
journal = {arXiv preprint arXiv:2006.06051},
title = {{Learning to Incentivize Other Learning Agents}},
year = {2020}
}
@misc{Automate32:online,
annote = {(Accessed on 07/14/2020)},
author = {of Transportation, U.S. Department},
howpublished = {https://www.nhtsa.gov/sites/nhtsa.dot.gov/files/documents/13069a-ads2.0{\_}090617{\_}v9a{\_}tag.pdf},
title = {{Automated Driving Systems: A Vision for Safety}},
year = {2017}
}
@article{castro2019inverse,
author = {Castro, Pablo Samuel and Li, Shijian and Zhang, Daqing},
file = {:Users/hikaruasano/Documents/mendeley/Castro, Li, Zhang{\_}2019{\_}Inverse Reinforcement Learning with Multiple Ranked Experts.pdf:pdf},
journal = {arXiv preprint arXiv:1907.13411},
title = {{Inverse Reinforcement Learning with Multiple Ranked Experts}},
year = {2019}
}
@article{mullainathan2017machine,
author = {Mullainathan, Sendhil and Spiess, Jann},
file = {:Users/hikaruasano/Documents/mendeley/Mullainathan, Spiess{\_}2017{\_}Machine learning an applied econometric approach.pdf:pdf},
journal = {Journal of Economic Perspectives},
number = {2},
pages = {87--106},
publisher = {American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203-2418},
title = {{Machine learning: an applied econometric approach}},
volume = {31},
year = {2017}
}
@book{BB25924984,
author = {実, 保苅},
publisher = {岩波書店},
series = {岩波現代文庫},
title = {ラディカル・オーラル・ヒストリー : オーストラリア先住民アボリジニの歴史実践},
url = {https://ci.nii.ac.jp/ncid/BB25924984},
year = {2018}
}
@article{Haarnoja2018SoftAA,
author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, G and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, P and Levine, Sergey},
journal = {ArXiv},
title = {{Soft Actor-Critic Algorithms and Applications}},
volume = {abs/1812.0},
year = {2018}
}
@article{Giles2019MultilevelNS,
author = {Giles, M and Haji-Ali, A},
file = {:Users/hikaruasano/Documents/mendeley/Giles, Haji-Ali{\_}2019{\_}Multilevel Nested Simulation for Efficient Risk Estimation.pdf:pdf},
journal = {SIAM/ASA J. Uncertain. Quantification},
pages = {497--525},
title = {{Multilevel Nested Simulation for Efficient Risk Estimation}},
volume = {7},
year = {2019}
}
@article{okumura2022ctrm,
author = {Okumura, Keisuke and Yonetani, Ryo and Nishimura, Mai and Kanezaki, Asako},
file = {:Users/hikaruasano/Documents/mendeley/Okumura et al.{\_}2022{\_}CTRMs Learning to Construct Cooperative Timed Roadmaps for Multi-agent Path Planning in Continuous Spaces.pdf:pdf},
journal = {arXiv preprint arXiv:2201.09467},
title = {{CTRMs: Learning to Construct Cooperative Timed Roadmaps for Multi-agent Path Planning in Continuous Spaces}},
year = {2022}
}
@article{article,
author = {Liu, Fei and Zeng, Guangzhou},
doi = {10.1016/j.eswa.2008.08.026},
file = {:Users/hikaruasano/Documents/mendeley/Liu, Zeng{\_}2009{\_}Study of genetic algorithm with reinforcement learning to solve the TSP.pdf:pdf},
journal = {Expert Systems with Applications},
pages = {6995--7001},
title = {{Study of genetic algorithm with reinforcement learning to solve the TSP}},
volume = {36},
year = {2009}
}
@misc{h29whole95:online,
annote = {(Accessed on 07/13/2020)},
author = {環境庁},
howpublished = {https://www.env.go.jp/earth/report/h31-01/h29{\_}whole.pdf},
title = {平成２９年度再生可能エネルギーに関する ゾーニング基礎情報等の整備・公開等に関する 委託業務報告書},
year = {2018}
}
@inproceedings{ndousse2021emergent,
author = {Ndousse, Kamal K and Eck, Douglas and Levine, Sergey and Jaques, Natasha},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Ndousse et al.{\_}2021{\_}Emergent social learning via multi-agent reinforcement learning.pdf:pdf},
organization = {PMLR},
pages = {7991--8004},
title = {{Emergent social learning via multi-agent reinforcement learning}},
year = {2021}
}
@inproceedings{leonardos2022global,
author = {Leonardos, Stefanos and Overman, Will and Panageas, Ioannis and Piliouras, Georgios},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Leonardos et al.{\_}2022{\_}Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games.pdf:pdf},
title = {{Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games}},
url = {https://openreview.net/forum?id=gfwON7rAm4},
year = {2022}
}
@inproceedings{lanctot2017unified,
author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and P{\'{e}}rolat, Julien and Silver, David and Graepel, Thore},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/hikaruasano/Documents/mendeley/Lanctot et al.{\_}2017{\_}A unified game-theoretic approach to multiagent reinforcement learning.pdf:pdf},
pages = {4190--4203},
title = {{A unified game-theoretic approach to multiagent reinforcement learning}},
year = {2017}
}
@article{wang2020cooperation,
author = {Wang, Rose E and Kew, J Chase and Lee, Dennis and Lee, Tsang-Wei Edward and Zhang, Tingnan and Ichter, Brian and Tan, Jie and Faust, Aleksandra},
file = {:Users/hikaruasano/Documents/mendeley/Wang et al.{\_}2020{\_}Cooperation without Coordination Hierarchical Predictive Planning for Decentralized Multiagent Navigation.pdf:pdf},
title = {{Cooperation without Coordination: Hierarchical Predictive Planning for Decentralized Multiagent Navigation.}},
year = {2020}
}
@article{1390001204647434112,
author = {功毅, 池田 and 界, 平石},
doi = {10.24602/sjpr.59.1_3},
file = {:Users/hikaruasano/Documents/mendeley/功毅, 界{\_}2016{\_}心理学における再現可能性危機：問題の構造と解決策.pdf:pdf},
issn = {0386-1058},
journal = {心理学評論},
number = {1},
pages = {3--14},
publisher = {心理学評論刊行会},
title = {心理学における再現可能性危機：問題の構造と解決策},
url = {https://cir.nii.ac.jp/crid/1390001204647434112},
volume = {59},
year = {2016}
}
@article{zhang2020multi,
author = {Zhang, Tianjun and Xu, Huazhe and Wang, Xiaolong and Wu, Yi and Keutzer, Kurt and Gonzalez, Joseph E and Tian, Yuandong},
journal = {arXiv preprint arXiv:2010.08531},
title = {{Multi-agent collaboration via reward attribution decomposition}},
year = {2020}
}
@article{Mller2005SimulationBO,
author = {M{\"{u}}ller, P},
journal = {Handbook of Statistics},
pages = {509--518},
title = {{Simulation Based Optimal Design}},
volume = {25},
year = {2005}
}
@article{McGree2012AdaptiveBC,
author = {McGree, J and Drovandi, C and Thompson, M H and Eccleston, J and Duffull, S and Mengersen, K and Pettitt, A and Goggin, Timothy K},
file = {:Users/hikaruasano/Documents/mendeley/McGree et al.{\_}2012{\_}Adaptive Bayesian compound designs for dose finding studies.pdf:pdf},
journal = {Journal of Statistical Planning and Inference},
pages = {1480--1492},
title = {{Adaptive Bayesian compound designs for dose finding studies}},
volume = {142},
year = {2012}
}
@article{milkowski2018replicability,
author = {Mi{\l}kowski, Marcin and Hensel, Witold M and Hohol, Mateusz},
file = {:Users/hikaruasano/Documents/mendeley/Mi{\l}kowski, Hensel, Hohol{\_}2018{\_}Replicability or reproducibility On the replication crisis in computational neuroscience and sharing only.pdf:pdf},
journal = {Journal of computational neuroscience},
number = {3},
pages = {163--172},
publisher = {Springer},
title = {{Replicability or reproducibility? On the replication crisis in computational neuroscience and sharing only relevant detail}},
volume = {45},
year = {2018}
}
@inproceedings{NEURIPS2019_7690dd4d,
author = {Zhang, Xueru and Khaliligarekani, Mohammadmahdi and Tekin, Cem and Liu, Mingyan},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d$\backslash$textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
file = {:Users/hikaruasano/Documents/mendeley/Zhang et al.{\_}2019{\_}Group Retention when Using Machine Learning in Sequential Decision Making the Interplay between User Dynamics and Fair.pdf:pdf},
publisher = {Curran Associates, Inc.},
title = {{Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness}},
url = {https://proceedings.neurips.cc/paper/2019/file/7690dd4db7a92524c684e3191919eb6b-Paper.pdf},
volume = {32},
year = {2019}
}
@inproceedings{Li2021DeepIC,
abstract = {Multi-agent reinforcement learning (MARL) requires coordination to efficiently solve certain tasks. Fully centralized control is often infeasible in such domains due to the size of joint action spaces. Coordination graph formalizations allow reasoning about the joint action based on the structure of interactions. However, they often require domain expertise in their design and can be difficult for dynamic environments with changing coordination requirements. This paper introduces the deep implicit coordination graph (DICG) architecture for such scenarios. DICG consists of a module for in- ferring the dynamic coordination graph structure which is then used by a graph neural network module to learn to implicitly rea- son about the joint actions or values. DICG allows learning the tradeoff between full centralization and decentralization via stan- dard actor-critic methods to significantly improve coordination for domains with large number of agents. We apply DICG to both centralized-training-centralized-execution and centralized-training- decentralized-execution regimes. We demonstrate that DICG solves the relative overgeneralization pathology in predatory-prey tasks as well as outperforms various MARL baselines on the challeng- ing StarCraft II Multi-agent Challenge (SMAC) and traffic junction environments.},
author = {Li, Sheng and Gupta, Jayesh K and Morales, Peter and Allen, R and Kochenderfer, Mykel J},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
file = {:Users/hikaruasano/Documents/mendeley/Li et al.{\_}2020{\_}Deep Implicit Coordination Graphs for Multi-agent Reinforcement Learning.pdf:pdf},
keywords = {Coordination,Deep Reinforcement Learning,Graph Neural Network,Multi-agent System},
pages = {764--772},
title = {{Deep Implicit Coordination Graphs for Multi-agent Reinforcement Learning}},
year = {2020}
}
@inproceedings{wang2020shapley,
author = {Wang, Jianhong and Zhang, Yuan and Kim, Tae-Kyun and Gu, Yunjie},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
number = {05},
pages = {7285--7292},
title = {{Shapley Q-value: A local reward approach to solve global reward games}},
volume = {34},
year = {2020}
}
@article{Silver2017,
abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
archivePrefix = {arXiv},
arxivId = {1712.01815},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
eprint = {1712.01815},
file = {:Users/hikaruasano/Documents/mendeley/Silver et al.{\_}2017{\_}Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.pdf:pdf},
pages = {1--19},
title = {{Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}},
url = {http://arxiv.org/abs/1712.01815},
year = {2017}
}
@inproceedings{ota2020efficient,
author = {Ota, Kei and Sasaki, Yoko and Jha, Devesh K and Yoshiyasu, Yusuke and Kanezaki, Asako},
booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
file = {:Users/hikaruasano/Documents/mendeley/Ota et al.{\_}2020{\_}Efficient exploration in constrained environments with goal-oriented reference path(2).pdf:pdf},
organization = {IEEE},
pages = {6061--6068},
title = {{Efficient exploration in constrained environments with goal-oriented reference path}},
year = {2020}
}
@inproceedings{chane2021goal,
author = {Chane-Sane, Elliot and Schmid, Cordelia and Laptev, Ivan},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Chane-Sane, Schmid, Laptev{\_}2021{\_}Goal-conditioned reinforcement learning with imagined subgoals.pdf:pdf},
organization = {PMLR},
pages = {1430--1440},
title = {{Goal-conditioned reinforcement learning with imagined subgoals}},
year = {2021}
}
@misc{4D69637226:online,
annote = {(Accessed on 07/15/2020)},
author = {国土交通省},
howpublished = {https://www.mlit.go.jp/common/001186187.pdf},
title = {都市の機能としての駐車場：土地利用と交通の要},
year = {2017}
}
@inproceedings{10.1145/3442188.3445912,
abstract = {Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.},
address = {New York, NY, USA},
author = {Finocchiaro, Jessie and Maio, Roland and Monachou, Faidra and Patro, Gourab K and Raghavan, Manish and Stoica, Ana-Andreea and Tsirtsis, Stratis},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
doi = {10.1145/3442188.3445912},
isbn = {9781450383097},
pages = {489--503},
publisher = {Association for Computing Machinery},
series = {FAccT '21},
title = {{Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness}},
url = {https://doi.org/10.1145/3442188.3445912},
year = {2021}
}
@article{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
eprint = {1312.6114},
file = {:Users/hikaruasano/Documents/mendeley/Kingma, Welling{\_}2013{\_}Auto-Encoding Variational Bayes.pdf:pdf},
month = {dec},
title = {{Auto-Encoding Variational Bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2013}
}
@inproceedings{sukhbaatar2016learning,
author = {Sukhbaatar, Sainbayar and Fergus, Rob and Others},
booktitle = {Advances in neural information processing systems},
file = {:Users/hikaruasano/Documents/mendeley/Sukhbaatar, Fergus, others{\_}2016{\_}Learning multiagent communication with backpropagation.pdf:pdf},
pages = {2244--2252},
title = {{Learning multiagent communication with backpropagation}},
year = {2016}
}
@article{zdeborova2020understanding,
author = {Zdeborov{\'{a}}, Lenka},
file = {:Users/hikaruasano/Documents/mendeley/Zdeborov{\'{a}}{\_}2020{\_}Understanding deep learning is also a job for physicists.pdf:pdf},
journal = {Nature Physics},
number = {6},
pages = {602--604},
publisher = {Nature Publishing Group},
title = {{Understanding deep learning is also a job for physicists}},
volume = {16},
year = {2020}
}
@inproceedings{pan2018human,
author = {Pan, Xinlei and Shen, Yilin},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
file = {:Users/hikaruasano/Documents/mendeley/Pan, Shen{\_}2018{\_}Human-interactive subgoal supervision for efficient inverse reinforcement learning.pdf:pdf},
organization = {International Foundation for Autonomous Agents and Multiagent Systems},
pages = {1380--1387},
title = {{Human-interactive subgoal supervision for efficient inverse reinforcement learning}},
year = {2018}
}
@misc{ipccar5w2:online,
annote = {(Accessed on 07/22/2020)},
howpublished = {$\backslash$url{\{}https://www.data.jma.go.jp/cpdinfo/ipcc/ar5/ipcc{\_}ar5{\_}wg1{\_}spm{\_}jpn.pdf{\}}},
title = {ipcc{\_}ar5{\_}wg1{\_}spm{\_}jpn.pdf}
}
@article{Tannenbaum2019SexAG,
author = {Tannenbaum, C and Ellis, R P and Eyssel, F and Zou, J and Schiebinger, L},
journal = {Nature},
pages = {137--146},
title = {{Sex and gender analysis improves science and engineering}},
volume = {575},
year = {2019}
}
@inproceedings{9164394,
author = {Nie, Zhenbang and Zeng, Peng and Yu, Haibin},
booktitle = {2020 Chinese Control And Decision Conference (CCDC)},
doi = {10.1109/CCDC49329.2020.9164394},
pages = {2667--2672},
title = {{Effective Decoupled Planning for Continuous Multi-Agent Pickup and Delivery}},
year = {2020}
}
@inproceedings{Jiang2020Graph,
author = {Jiang, Jiechuan and Dun, Chen and Huang, Tiejun and Lu, Zongqing},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Jiang et al.{\_}2020{\_}Graph Convolutional Reinforcement Learning.pdf:pdf},
title = {{Graph Convolutional Reinforcement Learning}},
url = {https://openreview.net/forum?id=HkxdQkSYDB},
year = {2020}
}
@misc{（平成10年度：49:online,
annote = {(Accessed on 01/20/2021)},
author = {公正取引委員会},
howpublished = {https://www.jftc.go.jp/dk/kiketsu/toukeishiryo/mondai/h10jirei4-01.html},
title = {（平成10年度：事例4） 秩父小野田(株)と日本セメント(株)の合併（平成10年7月合併届出受理，10月合併）（新会社名 太平洋セメント(株)）及び宇部興産(株)と三菱マテリアル(株)によるセメント事業の統合（平成10年8月営業譲受け届出受理，10月営業譲受け）（新会社名 宇部三菱セメント(株)）}
}
@article{XIE2007141,
abstract = {Otis Dudley Duncan, who died in November 2004, had enormous impact on the practice of quantitative reasoning in sociology and demography today. This paper traces the influence of Duncan as a quantitative sociologist within the context of the history of science. I locate Duncan's philosophy of social science within the tradition of “population thinking” that was begun by Charles Darwin and introduced to social science by Francis Galton. As part of this exploration, I distinguish two approaches to statistical analysis (emanating from the two main philosophical views of science): Gaussian or typological thinking, and Galtonian or population thinking. I examine in detail Duncan's views of quantitative reasoning in the social sciences, particularly his opinions on social measurement, path analysis, structural modeling, econometrics, and the Rasch model. An important theme of the paper is that Duncan quickly realized the difficulties and limitations of quantitative methodology in social science. In particular, he was bothered by inherent population heterogeneity that makes it futile to draw “law-like” inferences from statistical analyses in social science. Thus, Duncan was disdainful of the search for supposedly universal laws of society that would mimic those of the physical sciences, because he believed that such laws did not exist and would be meaningless. Instead, to Duncan, the main use of statistical tools was to summarize systematic patterns of population variability. The paper draws heavily on Duncan's previously unpublished personal communications.},
author = {Xie, Yu},
doi = {https://doi.org/10.1016/j.rssm.2007.05.006},
file = {:Users/hikaruasano/Documents/mendeley/Xie{\_}2007{\_}Otis Dudley Duncan's legacy The demographic approach to quantitative reasoning in social science.pdf:pdf},
issn = {0276-5624},
journal = {Research in Social Stratification and Mobility},
keywords = {Demographic approach,Duncan,Methodology,Population thinking,Rasch model,Statistical methods},
number = {2},
pages = {141--156},
title = {{Otis Dudley Duncan's legacy: The demographic approach to quantitative reasoning in social science}},
url = {https://www.sciencedirect.com/science/article/pii/S0276562407000170},
volume = {25},
year = {2007}
}
@misc{AppleEnv44:online,
annote = {(Accessed on 07/20/2020)},
author = {Apple},
howpublished = {https://www.apple.com/environment/pdf/Apple{\_}Environmental{\_}Responsibility{\_}Report{\_}2019.pdf},
title = {{Environmental Responsibility Report 2019}},
year = {2019}
}
@inproceedings{Serrino2019FindingFA,
author = {Serrino, Jack and Kleiman-Weiner, Max and Parkes, D and Tenenbaum, J},
booktitle = {NeurIPS},
file = {:Users/hikaruasano/Documents/mendeley/Serrino et al.{\_}2019{\_}Finding Friend and Foe in Multi-Agent Games.pdf:pdf},
title = {{Finding Friend and Foe in Multi-Agent Games}},
year = {2019}
}
@book{BC02891850,
author = {McIntyre, Lee C and 居村, 匠 and 大﨑, 智史 and 西橋, 卓也 and 大橋, 完太郎},
publisher = {人文書院},
title = {ポストトゥルース},
url = {https://ci.nii.ac.jp/ncid/BC02891850},
year = {2020}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:Users/hikaruasano/Documents/mendeley/Mnih et al.{\_}2015{\_}Human-level control through deep reinforcement learning.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}
@article{bovsansky2016algorithms,
author = {Bo{\v{s}}ansk$\backslash$`y, Branislav and Lis$\backslash$`y, Viliam and Lanctot, Marc and {\v{C}}erm{\'{a}}k, Ji$\backslash$vr$\backslash$'$\backslash$i and Winands, Mark H M},
file = {:Users/hikaruasano/Documents/mendeley/Bo{\v{s}}ansk`y et al.{\_}2016{\_}Algorithms for computing strategies in two-player simultaneous move games.pdf:pdf},
journal = {Artificial Intelligence},
pages = {1--40},
publisher = {Elsevier},
title = {{Algorithms for computing strategies in two-player simultaneous move games}},
volume = {237},
year = {2016}
}
@article{gawron2018life,
author = {Gawron, James H and Keoleian, Gregory A and {De Kleine}, Robert D and Wallington, Timothy J and Kim, Hyung Chul},
journal = {Environmental science {\&} technology},
number = {5},
pages = {3249--3256},
publisher = {ACS Publications},
title = {{Life cycle assessment of connected and automated vehicles: sensing and computing subsystem and vehicle level effects}},
volume = {52},
year = {2018}
}
@inproceedings{Maei2009ConvergentTL,
author = {Maei, H and Szepesvari, Csaba and Bhatnagar, S and Precup, Doina and Silver, D and Sutton, R},
booktitle = {NIPS},
file = {:Users/hikaruasano/Documents/mendeley/Maei et al.{\_}2009{\_}Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation.pdf:pdf},
title = {{Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation}},
year = {2009}
}
@article{130005033172,
author = {祐樹, 豊坂 and 英雄, 廣瀬},
doi = {10.11527/jceeek.2008.0.442.0},
journal = {電気関係学会九州支部連合大会講演論文集},
number = {0},
pages = {442},
publisher = {電気・情報関係学会九州支部連合大会委員会},
title = {{パンデミックモデルにおけるSEIR計算結果とMAS計算結果との整合性}},
url = {https://ci.nii.ac.jp/naid/130005033172/},
volume = {2008},
year = {2008}
}
@book{sutton2018reinforcement,
author = {Sutton, Richard S and Barto, Andrew G},
publisher = {MIT press},
title = {{Reinforcement learning: An introduction}},
year = {2018}
}
@article{yarkoni_2022,
author = {Yarkoni, Tal},
doi = {10.1017/S0140525X20001685},
journal = {Behavioral and Brain Sciences},
pages = {e1},
publisher = {Cambridge University Press},
title = {{The generalizability crisis}},
volume = {45},
year = {2022}
}
@inproceedings{yuan2021agent,
abstract = {Predicting accurate future trajectories of multiple agents is essential for autonomous systems but is challenging due to the complex interaction between agents and the uncer- tainty in each agent's future behavior. Forecasting multi- agent trajectories requires modeling two key dimensions: (1) time dimension, where we model the influence of past agent states over future states; (2) social dimension, where we model how the state of each agent affects others. Most prior methods model these two dimensions separately, e.g., first using a temporal model to summarize features over time for each agent independently and then modeling the interaction of the summarized features with a social model. This approach is suboptimal since independent feature en- coding over either the time or social dimension can result in a loss of information. Instead, we would prefer a method that allows an agent's state at one time to directly affect another agent's state at a future time. To this end, we propose a new Transformer, termed AgentFormer, that si- multaneously models the time and social dimensions. The model leverages a sequence representation of multi-agent trajectories by flattening trajectory features across time and agents. Since standard attention operations disregard the agent identity of each element in the sequence, AgentFormer uses a novel agent-aware attention mechanism that pre- serves agent identities by attending to elements of the same agent differently than elements of other agents. Based on AgentFormer, we propose a stochastic multi-agent trajec- tory prediction model that can attend to features of any agent at any previous timestep when inferring an agent's future position. The latent intent of all agents is also jointly modeled, allowing the stochasticity in one agent's behavior to affect other agents. Extensive experiments show that our method substantially improves the state of the art on well- established pedestrian and autonomous driving datasets.},
author = {Yuan, Ye and Weng, Xinshuo and Ou, Yanglan and Kitani, Kris},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
file = {:Users/hikaruasano/Documents/mendeley/Yuan et al.{\_}2021{\_}AgentFormer Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting.pdf:pdf},
title = {{AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting}},
year = {2021}
}
@inproceedings{pmlr-v80-haarnoja18b,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
editor = {Dy, Jennifer and Krause, Andreas},
pages = {1861--1870},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
url = {https://proceedings.mlr.press/v80/haarnoja18b.html},
volume = {80},
year = {2018}
}
@article{9743365,
author = {Li, Jian-Yu and Zhan, Zhi-Hui and Tan, Kay Chen and Zhang, Jun},
doi = {10.1109/TCYB.2022.3158391},
journal = {IEEE Transactions on Cybernetics},
pages = {1--15},
title = {{Dual Differential Grouping: A More General Decomposition Method for Large-Scale Optimization}},
year = {2022}
}
@inproceedings{scibior2021imagining,
author = {{\'{S}}cibior, Adam and Lioutas, Vasileios and Reda, Daniele and Bateni, Peyman and Wood, Frank},
booktitle = {2021 IEEE International Intelligent Transportation Systems Conference (ITSC)},
file = {:Users/hikaruasano/Documents/mendeley/{\'{S}}cibior et al.{\_}2021{\_}Imagining the road ahead Multi-agent trajectory prediction via differentiable simulation.pdf:pdf},
organization = {IEEE},
pages = {720--725},
title = {{Imagining the road ahead: Multi-agent trajectory prediction via differentiable simulation}},
year = {2021}
}
@inproceedings{10.5555/3524938.3525846,
abstract = {Safe reinforcement learning has been a promising approach for optimizing the policy of an agent that operates in safety-critical applications. In this paper, we propose an algorithm, SNO-MDP, that explores and optimizes Markov decision processes under unknown safety constraints. Specifically, we take a stepwise approach for optimizing safety and cumulative reward. In our method, the agent first learns safety constraints by expanding the safe region, and then optimizes the cumulative reward in the certified safe region. We provide theoretical guarantees on both the satisfaction of the safety constraint and the near-optimality of the cumulative reward under proper regularity assumptions. In our experiments, we demonstrate the effectiveness of SNO-MDP through two experiments: one uses a synthetic data in a new, openlyavailable environment named GP-SAFETY-GYM, and the other simulates Mars surface exploration by using real observation data.},
author = {Wachi, Akifumi and Sui, Yanan},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Wachi, Sui{\_}2020{\_}Safe Reinforcement Learning in Constrained Markov Decision Processes.pdf:pdf},
publisher = {JMLR.org},
series = {ICML'20},
title = {{Safe Reinforcement Learning in Constrained Markov Decision Processes}},
year = {2020}
}
@book{BB28639124,
author = {Berman, Morris and 元幸, 柴田},
pages = {p.36},
publisher = {文藝春秋},
title = {デカルトからベイトソンへ : 世界の再魔術化},
url = {https://ci.nii.ac.jp/ncid/BB28639124},
year = {2019}
}
@inproceedings{NIPS2014_792c7b5a,
author = {Zhang, Chongjie and Shah, Julie A},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N and Weinberger, K Q},
file = {:Users/hikaruasano/Documents/mendeley/Zhang, Shah{\_}2014{\_}Fairness in Multi-Agent Sequential Decision-Making.pdf:pdf},
publisher = {Curran Associates, Inc.},
title = {{Fairness in Multi-Agent Sequential Decision-Making}},
url = {https://proceedings.neurips.cc/paper/2014/file/792c7b5aae4a79e78aaeda80516ae2ac-Paper.pdf},
volume = {27},
year = {2014}
}
@article{Ryan2014TowardsBE,
author = {Ryan, Elizabeth G and Drovandi, C and Thompson, M H and Pettitt, A},
journal = {Comput. Stat. Data Anal.},
pages = {45--60},
title = {{Towards Bayesian experimental design for nonlinear models that require a large number of sampling times}},
volume = {70},
year = {2014}
}
@article{1390001205166783232,
author = {筒井, 淳也},
doi = {10.11218/ojjams.31.160},
issn = {09131442},
journal = {理論と方法},
number = {1},
pages = {160--166},
publisher = {数理社会学会},
title = {シンポジウム「日本の数理・計量社会学のこれまでとこれから」を振り返って},
url = {https://cir.nii.ac.jp/crid/1390001205166783232},
volume = {31},
year = {2016}
}
@misc{Energyco65:online,
annote = {(Accessed on 01/31/2022)},
author = {IEA},
howpublished = {https://www.iea.org/data-and-statistics/charts/energy-consumption-in-transport-in-iea-countries-2018},
title = {{Energy consumption in transport in IEA countries, 2018}},
year = {2020}
}
@inproceedings{Ho2016,
abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.},
archivePrefix = {arXiv},
arxivId = {1606.03476},
author = {Ho, Jonathan and Ermon, Stefano},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1606.03476},
issn = {10495258},
pages = {4572--4580},
publisher = {Neural information processing systems foundation},
title = {{Generative adversarial imitation learning}},
year = {2016}
}
@inproceedings{das2019tarmac,
author = {Das, Abhishek and Gervet, Th{\'{e}}ophile and Romoff, Joshua and Batra, Dhruv and Parikh, Devi and Rabbat, Mike and Pineau, Joelle},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Das et al.{\_}2019{\_}Tarmac Targeted multi-agent communication.pdf:pdf},
pages = {1538--1546},
title = {{Tarmac: Targeted multi-agent communication}},
year = {2019}
}
@article{li2021magat,
author = {Li, Qingbiao and Lin, Weizhe and Liu, Zhe and Prorok, Amanda},
doi = {10.1109/LRA.2021.3077863},
file = {:Users/hikaruasano/Documents/mendeley/Li et al.{\_}2021{\_}Message-Aware Graph Attention Networks for Large-Scale Multi-Robot Path Planning.pdf:pdf},
journal = {IEEE Robotics and Automation Letters},
number = {3},
pages = {5533--5540},
title = {{Message-Aware Graph Attention Networks for Large-Scale Multi-Robot Path Planning}},
volume = {6},
year = {2021}
}
@article{parisi2016multi,
author = {Parisi, Simone and Pirotta, Matteo and Restelli, Marcello},
file = {:Users/hikaruasano/Documents/mendeley/Parisi, Pirotta, Restelli{\_}2016{\_}Multi-objective reinforcement learning through continuous pareto manifold approximation.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {187--227},
title = {{Multi-objective reinforcement learning through continuous pareto manifold approximation}},
volume = {57},
year = {2016}
}
@inproceedings{NIPS2017_a486cd07,
author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
pages = {4066--4076},
publisher = {Curran Associates, Inc.},
title = {{Counterfactual Fairness}},
url = {https://proceedings.neurips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
volume = {30},
year = {2017}
}
@inproceedings{NIPS2017_3323fe11,
author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Guyon, I and Luxburg, U Von and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
file = {:Users/hikaruasano/Documents/mendeley/Lanctot et al.{\_}2017{\_}A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning(2).pdf:pdf},
publisher = {Curran Associates, Inc.},
title = {{A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning}},
url = {https://proceedings.neurips.cc/paper/2017/file/3323fe11e9595c09af38fe67567a9394-Paper.pdf},
volume = {30},
year = {2017}
}
@article{山口一臣2010日本ビール業界への警鐘,
author = {山口一臣 and Others},
journal = {成城大學經濟研究},
number = {189},
pages = {15--81},
title = {日本ビール業界への警鐘: 麒麟麦酒 100 年に見る日本ビール業界の課題},
year = {2010}
}
@incollection{Brachman1979,
author = {Brachman, Ronald J.},
booktitle = {Associative Networks},
doi = {10.1016/b978-0-12-256380-5.50007-4},
pages = {3--50},
publisher = {Elsevier},
title = {{ON THE EPISTEMOLOGICAL STATUS OF SEMANTIC NETWORKS**Prepared in part at Bolt Beranek and Newman Inc. under contracts sponsored by the Defense Advance Research Projects Agency and the Office of Naval Research. The views and conclusions stated are those of }},
year = {1979}
}
@article{McGree2012ASM,
author = {McGree, J and Drovandi, C and Pettitt, A},
file = {:Users/hikaruasano/Documents/mendeley/McGree, Drovandi, Pettitt{\_}2012{\_}A sequential Monte Carlo approach to the sequential design for discriminating between rival continuous da.pdf:pdf},
journal = {Science {\&} Engineering Faculty},
title = {{A sequential Monte Carlo approach to the sequential design for discriminating between rival continuous data models}},
year = {2012}
}
@article{kessel2010relationship,
author = {Kessel, Line and Johnson, Leif and Arvidsson, Henrik and Larsen, Michael},
file = {:Users/hikaruasano/Documents/mendeley/Kessel et al.{\_}2010{\_}The relationship between body and ambient temperature and corneal temperature.pdf:pdf},
journal = {Investigative ophthalmology {\&} visual science},
number = {12},
pages = {6593--6597},
publisher = {The Association for Research in Vision and Ophthalmology},
title = {{The relationship between body and ambient temperature and corneal temperature}},
volume = {51},
year = {2010}
}
@article{kozuno2021learning,
author = {Kozuno, Tadashi and M{\'{e}}nard, Pierre and Munos, Remi and Valko, Michal},
file = {:Users/hikaruasano/Documents/mendeley/Kozuno et al.{\_}2021{\_}Learning in two-player zero-sum partially observable Markov games with perfect recall.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {11987--11998},
title = {{Learning in two-player zero-sum partially observable Markov games with perfect recall}},
volume = {34},
year = {2021}
}
@article{1390291767808272000,
author = {平石, 界 and 中村, 大輝},
doi = {10.4216/jpssj.54.2_27},
file = {:Users/hikaruasano/Documents/mendeley/平石, 中村{\_}2022{\_}心理学における再現性危機の10年.pdf:pdf},
issn = {0289-3428},
journal = {科学哲学},
number = {2},
pages = {27--50},
publisher = {日本科学哲学会},
title = {心理学における再現性危機の10年},
url = {https://cir.nii.ac.jp/crid/1390291767808272000},
volume = {54},
year = {2022}
}
@techreport{Sakaji2011,
abstract = {This paper proposes a method that extracts causal knowledge by using clue expressions. Our method decides whether a sentence includes causal knowledge or not when the method extracts it. Therefore, our method can extract causal knowledge accurately. Furthermore, the advantage of our decision method is to extract causal knowledge manually without dictionaries and patterns.},
author = {Sakaji, * H and Sigeru, M},
file = {:Users/hikaruasano/Documents/mendeley/Sakaji, Sigeru{\_}2011{\_}Extraction of Causal Knowledge by Using Text Mining.pdf:pdf},
keywords = {Information Extraction,Knowledge Discovery,Text Mining},
title = {{Extraction of Causal Knowledge by Using Text Mining}},
url = {http://mecab.sourceforge.net/},
volume = {54},
year = {2011}
}
@techreport{Department2019,
author = {Department, United Nations of Economic Affairs Social Population Division},
file = {:Users/hikaruasano/Documents/mendeley/Department{\_}2019{\_}World Population Prospects 2019 Highlights.pdf:pdf},
keywords = {United Nations,age distribution,age structure,ageing,aging,demographic change,demographic dividend,demographic estimates,demographic estimation,demographic projections,demographic transition,demography,dependency,deterministic population projections,estimates,estimation,fertility,fertility decline,fertility projections,fertility rate,fertility transition,future,gender,global population data,growth,human,international migration,levels and trends,life expectancy,long-term forecasting,longevity,migration,mortality,mortality decline,mortality projections,mortality transition,net migration,older persons,population,population -- statistics,population ageing,population change,population characteristics,population data,population decline,population distribution,population dynamics,population estimates,population forecasting,population growth,population increase,population projections,population size,prediction,prediction interval,probabilistic population projections,probabilistic projections,projection,projections,scenarios,support ratio,sustainability,sustainable development,transition,trends,uncertainty,variants,world demography,world population},
title = {{World Population Prospects 2019 Highlights}},
year = {2019}
}
@incollection{NIPS2019_8691,
author = {Du, Yali and Han, Lei and Fang, Meng and Liu, Ji and Dai, Tianhong and Tao, Dacheng},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d$\backslash$textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
file = {:Users/hikaruasano/Desktop/mendeley/Du et al.{\_}2019{\_}LIIR Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning.pdf:pdf},
pages = {4403--4414},
publisher = {Curran Associates, Inc.},
title = {{LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning}},
url = {http://papers.nips.cc/paper/8691-liir-learning-individual-intrinsic-reward-in-multi-agent-reinforcement-learning.pdf},
year = {2019}
}
@inproceedings{foerster2017stabilising,
author = {Foerster, Jakob and Nardelli, Nantas and Farquhar, Gregory and Afouras, Triantafyllos and Torr, Philip H S and Kohli, Pushmeet and Whiteson, Shimon},
booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
file = {:Users/hikaruasano/Documents/mendeley/Foerster et al.{\_}2017{\_}Stabilising experience replay for deep multi-agent reinforcement learning.pdf:pdf},
organization = {JMLR. org},
pages = {1146--1155},
title = {{Stabilising experience replay for deep multi-agent reinforcement learning}},
year = {2017}
}
@misc{Unknownd,
author = {晃司, 川口 and 考行, 上田 and 智樹, 石倉},
title = {パンデミック時における社会経済活動の最適停止タイミング}
}
@article{Sheng2005MaximumLM,
author = {Sheng, Xiaohong and Hu, Y},
journal = {IEEE Trans. Signal Process.},
pages = {44--53},
title = {{Maximum likelihood multiple-source localization using acoustic energy measurements with wireless sensor networks}},
volume = {53},
year = {2005}
}
@article{ShahGidcAnkleshwar2GujaratINSorathiyaSureshDevjibhaiCadilaHealthcareLimited291GidcAnkleshwar2Gujarat39300INAgarwalVirendraKumarCadilaHealthcareLimited291GidcAnkleshwar2Gujarat39300INNirogiVenkaChirag2003,
author = {{Shah  Gidc Ankleshwar 2 Gujarat  IN), Sorathiya, Suresh Devjibhai (Cadila Healthcare Limited 291, Gidc Ankleshwar 2 Gujarat, 393 00, IN), Agarwal, Virendra Kumar (Cadila Healthcare Limited 291, Gidc Ankleshwar 2 Gujarat, 393 00, IN), Nirogi, Venka, Chirag}, 393 00 and {Shah  Gidc Ankleshwar 2 Gujarat  IN), Sorathiya, Suresh Devjibhai (Cadila Healthcare Limited 291, Gidc Ankleshwar 2 Gujarat, 393 00, IN), Agarwal, Virendra Kumar (Cadila Healthcare Limited 291, Gidc Ankleshwar 2 Gujarat, 393 00, IN), Nirogi, Venka, Chirag}, 393 00},
doi = {10.1038/scientificamerican1155-31},
file = {:Users/hikaruasano/Documents/mendeley/Shah Gidc Ankleshwar 2 Gujarat IN), Sorathiya, Suresh Devjibhai (Cadila Healthcare Limited 291, Gidc Ankleshwar 2 Gujarat, 393 00, IN),.pdf:pdf},
issn = {0036-8733},
number = {WO/2003/027106},
pages = {31--35},
title = {{Process for the Preparation of Crystalline Polymorph Ii of Lamivudine}},
url = {http://www.freepatentsonline.com/WO2003027106.html},
year = {2003}
}
@article{2016,
author = {秋元 and 明},
issn = {0387-3285},
journal = {政経論叢},
keywords = {ウズラ,主観的価値,主観貨幣価値変換,主観貨幣価値最大の需要関数,価値の表現,効用,消費動因},
number = {1},
pages = {77--120},
publisher = {明治大学政治経済研究所},
title = {主観価値理論に関する一研究 (中村文隆教授古稀記念論文集)},
url = {http://ci.nii.ac.jp/naid/120005890627/ja/},
volume = {84},
year = {2016}
}
@inproceedings{pmlr-v80-rabinowitz18a,
abstract = {Theory of mind (ToM) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We design a Theory of Mind neural network {\{}–{\}} a ToMnet {\{}–{\}} which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents' future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents' characteristics and mental states. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep RL agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test of recognising that others can hold false beliefs about the world.},
author = {Rabinowitz, Neil and Perbet, Frank and Song, Francis and Zhang, Chiyuan and Eslami, S M Ali and Botvinick, Matthew},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
editor = {Dy, Jennifer and Krause, Andreas},
file = {:Users/hikaruasano/Documents/mendeley/Rabinowitz et al.{\_}2018{\_}Machine Theory of Mind.pdf:pdf},
pages = {4218--4227},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Machine Theory of Mind}},
volume = {80},
year = {2018}
}
@article{Eiriksson2011,
author = {Eir{\'{i}}ksson, Hrafn},
file = {:Users/hikaruasano/Documents/mendeley/Eir{\'{i}}ksson{\_}2011{\_}Investigation of Multi Cut Pruning in Game-Tree Search.pdf:pdf},
title = {{Investigation of Multi Cut Pruning in Game-Tree Search}},
url = {skemman.is/stream/get/1946/9180/22971/1/research-report.pdf},
year = {2011}
}
@article{rockstrom2009planetary,
author = {Rockstr{\"{o}}m, Johan and Steffen, Will and Noone, Kevin and Persson, {\AA}sa and {Chapin III}, F Stuart and Lambin, Eric and Lenton, Timothy M and Scheffer, Marten and Folke, Carl and Schellnhuber, Hans Joachim and Others},
journal = {Ecology and society},
number = {2},
publisher = {JSTOR},
title = {{Planetary boundaries: exploring the safe operating space for humanity}},
volume = {14},
year = {2009}
}
@inproceedings{DBLP:conf/icml/ZhengPF18,
author = {Zheng, Sue and Pacheco, Jason and III, John W Fisher},
booktitle = {Proceedings of the 35th International Conference on Machine Learning, {\{}ICML{\}} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018},
editor = {Dy, Jennifer G and Krause, Andreas},
pages = {5936--5944},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{A Robust Approach to Sequential Information Theoretic Planning}},
url = {http://proceedings.mlr.press/v80/zheng18b.html},
volume = {80},
year = {2018}
}
@inproceedings{10.1145/3351095.3372878,
abstract = {As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses fixed data sets. To address this structural difficulty in the field, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the long-term consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.},
address = {New York, NY, USA},
author = {D'Amour, Alexander and Srinivasan, Hansa and Atwood, James and Baljekar, Pallavi and Sculley, D and Halpern, Yoni},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
doi = {10.1145/3351095.3372878},
file = {:Users/hikaruasano/Documents/mendeley/D'Amour et al.{\_}2020{\_}Fairness is Not Static Deeper Understanding of Long Term Fairness via Simulation Studies.pdf:pdf},
isbn = {9781450369367},
pages = {525--534},
publisher = {Association for Computing Machinery},
series = {FAT* '20},
title = {{Fairness is Not Static: Deeper Understanding of Long Term Fairness via Simulation Studies}},
url = {https://doi.org/10.1145/3351095.3372878},
year = {2020}
}
@article{Dewald,
author = {Dewald, JF and Meijer, AM and Oort, FJ and medicine {\ldots}, GA Kerkhof - Sleep and undefined 2010},
journal = {Elsevier},
title = {{The influence of sleep quality, sleep duration and sleepiness on school performance in children and adolescents: A meta-analytic review}},
url = {https://gateway.itc.u-tokyo.ac.jp/science/article/pii/S1087079209001002,DanaInfo=www.sciencedirect.com}
}
@article{玉井拓之2018都市避難シミュレーョンにおける追従性心理の導入と遅滞リスク軽減モデル提案,
author = {玉井拓之 and 山﨑達也 and 大和田泰伯 and 佐藤剛至 and 柄沢直之},
journal = {日本シミュレーション学会論文誌},
number = {1},
pages = {17--24},
publisher = {日本シミュレーション学会},
title = {都市避難シミュレーョンにおける追従性心理の導入と遅滞リスク軽減モデル提案},
volume = {10},
year = {2018}
}
@inproceedings{zhang2022reinforcement,
author = {Zhang, Zhi and Yang, Zhuoran and Liu, Han and Tokekar, Pratap and Huang, Furong},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Zhang et al.{\_}2022{\_}Reinforcement Learning under a Multi-agent Predictive State Representation Model Method and Theory.pdf:pdf},
title = {{Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory}},
url = {https://openreview.net/forum?id=PLDOnFoVm4},
year = {2022}
}
@book{DeBerg2008,
abstract = {This well-accepted introduction to computational geometry is a textbook for high-level undergraduate and low-level graduate courses. The focus is on algorithms and hence the book is well suited for students in computer science and engineering. Motivation is provided from the application areas: all solutions and techniques from computational geometry are related to particular applications in robotics, graphics, CAD/CAM, and geographic information systems. For students this motivation will be especially welcome. Modern insights in computational geometry are used to provide solutions that are both efficient and easy to understand and implement. All the basic techniques and topics from computational geometry, as well as several more advanced topics, are covered. The book is largely self-contained and can be used for self-study by anyone with a basic background in algorithms. In this third edition, besides revisions to the second edition, new sections discussing Voronoi diagrams of line segments, farthest-point Voronoi diagrams, and realistic input models have been added. {\textcopyright} 2008, 2000, 1997 Springer-Verlag Berlin Heidelberg.},
author = {{De Berg}, Mark and Cheong, Otfried and {Van Kreveld}, Marc and Overmars, Mark},
booktitle = {Computational Geometry: Algorithms and Applications},
doi = {10.1007/978-3-540-77974-2},
file = {:Users/hikaruasano/Documents/mendeley/De Berg et al.{\_}2008{\_}Computational geometry Algorithms and applications.pdf:pdf},
isbn = {9783540779735},
pages = {1--386},
publisher = {Springer Berlin Heidelberg},
title = {{Computational geometry: Algorithms and applications}},
year = {2008}
}
@inproceedings{7040372,
author = {Ames, Aaron D and Grizzle, Jessy W and Tabuada, Paulo},
booktitle = {53rd IEEE Conference on Decision and Control},
doi = {10.1109/CDC.2014.7040372},
pages = {6271--6278},
title = {{Control barrier function based quadratic programs with application to adaptive cruise control}},
year = {2014}
}
@inproceedings{pmlr-v80-hashimoto18a,
abstract = {Machine learning models (e.g., speech recognizers) trained on average loss suffer from representation disparity—minority groups (e.g., non-native speakers) carry less weight in the training objective, and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority group can shrink over time. In this paper, we first show that the status quo of empirical risk minimization (ERM) amplifies representation disparity over time, which can even turn initially fair models unfair. To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution. We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups. We demonstrate that DRO prevents disparity amplification on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task.},
author = {Hashimoto, Tatsunori and Srivastava, Megha and Namkoong, Hongseok and Liang, Percy},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
editor = {Dy, Jennifer and Krause, Andreas},
file = {:Users/hikaruasano/Documents/mendeley/Hashimoto et al.{\_}2018{\_}Fairness Without Demographics in Repeated Loss Minimization.pdf:pdf},
pages = {1929--1938},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Fairness Without Demographics in Repeated Loss Minimization}},
url = {https://proceedings.mlr.press/v80/hashimoto18a.html},
volume = {80},
year = {2018}
}
@inproceedings{xu2015show,
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
booktitle = {International conference on machine learning},
pages = {2048--2057},
title = {{Show, attend and tell: Neural image caption generation with visual attention}},
year = {2015}
}
@inproceedings{liu2020when2com,
author = {Liu, Yen-Cheng and Tian, Junjiao and Glaser, Nathaniel and Kira, Zsolt},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:Users/hikaruasano/Documents/mendeley/Liu et al.{\_}2020{\_}When2com Multi-Agent Perception via Communication Graph Grouping.pdf:pdf},
title = {{When2com: Multi-Agent Perception via Communication Graph Grouping}},
year = {2020}
}
@inproceedings{pmlr-v139-sim21b,
abstract = {Bayesian optimization (BO) is a popular tool for optimizing complex and costly-to-evaluate black-box objective functions. To further reduce the number of function evaluations, any party performing BO may be interested to collaborate with others to optimize the same objective function concurrently. To do this, existing BO algorithms have considered optimizing a batch of input queries in parallel and provided theoretical bounds on their cumulative regret reflecting inefficiency. However, when the objective function values are correlated with real-world rewards (e.g., money), parties may be hesitant to collaborate if they risk incurring larger cumulative regret (i.e., smaller real-world reward) than others. This paper shows that fairness and efficiency are both necessary for the collaborative BO setting. Inspired by social welfare concepts from economics, we propose a new notion of regret capturing these properties and a collaborative BO algorithm whose convergence rate can be theoretically guaranteed by bounding the new regret, both of which share an adjustable parameter for trading off between fairness vs. efficiency. We empirically demonstrate the benefits (e.g., increased fairness) of our algorithm using synthetic and real-world datasets.},
author = {Sim, Rachael Hwee Ling and Zhang, Yehong and Low, Bryan Kian Hsiang and Jaillet, Patrick},
booktitle = {Proceedings of the 38th International Conference on Machine Learning},
editor = {Meila, Marina and Zhang, Tong},
file = {:Users/hikaruasano/Documents/mendeley/Sim et al.{\_}2021{\_}Collaborative Bayesian Optimization with Fair Regret(2).pdf:pdf},
pages = {9691--9701},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Collaborative Bayesian Optimization with Fair Regret}},
url = {https://proceedings.mlr.press/v139/sim21b.html},
volume = {139},
year = {2021}
}
@inproceedings{Bellemare:2017:DPR:3305381.3305428,
author = {Bellemare, Marc G and Dabney, Will and Munos, R{\'{e}}mi},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {449--458},
publisher = {JMLR.org},
series = {ICML'17},
title = {{A Distributional Perspective on Reinforcement Learning}},
url = {http://dl.acm.org/citation.cfm?id=3305381.3305428},
year = {2017}
}
@article{Rifkin2015,
author = {Rifkin, Jeremy and 柴田, 裕之},
isbn = {9784140816875},
publisher = {NHK出版},
title = {限界費用ゼロ社会 : 「モノのインターネット」と共有型経済の台頭},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2003301443},
year = {2015}
}
@inproceedings{10.5555/3495724.3497535,
abstract = {Goal-conditioned hierarchical reinforcement learning (HRL) is a promising approach for scaling up reinforcement learning (RL) techniques. However, it often suffers from training inefficiency as the action space of the high-level, i.e., the goal space, is often large. Searching in a large goal space poses difficulties for both high-level subgoal generation and low-level policy learning. In this paper, we show that this problem can be effectively alleviated by restricting the high-level action space from the whole goal space to a k-step adjacent region of the current state using an adjacency constraint. We theoretically prove that the proposed adjacency constraint preserves the optimal hierarchical policy in deterministic MDPs, and show that this constraint can be practically implemented by training an adjacency network that can discriminate between adjacent and non-adjacent subgoals. Experimental results on discrete and continuous control tasks show that incorporating the adjacency constraint improves the performance of state-of-the-art HRL approaches in both deterministic and stochastic environments.},
address = {Red Hook, NY, USA},
author = {Zhang, Tianren and Guo, Shangqi and Tan, Tian and Hu, Xiaolin and Chen, Feng},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
file = {:Users/hikaruasano/Documents/mendeley/Zhang et al.{\_}2020{\_}Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning.pdf:pdf},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
series = {NIPS'20},
title = {{Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning}},
year = {2020}
}
@article{Mller2004OptimalBD,
author = {M{\"{u}}ller, P and Sans{\'{o}}, B and Iorio, M De},
file = {:Users/hikaruasano/Documents/mendeley/M{\"{u}}ller, Sans{\'{o}}, Iorio{\_}2004{\_}Optimal Bayesian Design by Inhomogeneous Markov Chain Simulation.pdf:pdf},
journal = {Journal of the American Statistical Association},
pages = {788--798},
title = {{Optimal Bayesian Design by Inhomogeneous Markov Chain Simulation}},
volume = {99},
year = {2004}
}
@article{1390001205167002624,
author = {盛山, 和夫},
doi = {10.11218/ojjams.3.57},
file = {:Users/hikaruasano/Documents/mendeley/盛山{\_}1988{\_}反照性と社会理論.pdf:pdf},
issn = {0913-1442},
journal = {理論と方法},
number = {1},
pages = {57--76},
publisher = {数理社会学会},
title = {反照性と社会理論},
url = {https://cir.nii.ac.jp/crid/1390001205167002624},
volume = {3},
year = {1988}
}
@article{boyd2007notes,
author = {Boyd, Stephen and Xiao, Lin and Mutapcic, Almir and Mattingley, Jacob},
journal = {Notes for EE364B, Stanford University},
pages = {1--36},
title = {{Notes on decomposition methods}},
volume = {635},
year = {2007}
}
@inproceedings{diuk2008object,
author = {Diuk, Carlos and Cohen, Andre and Littman, Michael L},
booktitle = {Proceedings of the 25th international conference on Machine learning},
file = {:Users/hikaruasano/Documents/mendeley/Diuk, Cohen, Littman{\_}2008{\_}An object-oriented representation for efficient reinforcement learning.pdf:pdf},
pages = {240--247},
title = {{An object-oriented representation for efficient reinforcement learning}},
year = {2008}
}
@inproceedings{harutyunyan2016q,
author = {Harutyunyan, Anna and Bellemare, Marc G and Stepleton, Tom and Munos, R{\'{e}}mi},
booktitle = {International Conference on Algorithmic Learning Theory},
file = {:Users/hikaruasano/Documents/mendeley/Harutyunyan et al.{\_}2016{\_}Q ({\$}{\$}{\$}{\$}lambda {\$}{\$}) with Off-Policy Corrections.pdf:pdf},
organization = {Springer},
pages = {305--320},
title = {{Q ({\$}{\$}{\$}\backslash{\$}lambda {\$}{\$}) with Off-Policy Corrections}},
year = {2016}
}
@article{2017,
author = {敦志, 須山 and 将, 杉山},
isbn = {9784061538320},
pages = {2},
publisher = {講談社},
title = {ベイズ推論による機械学習入門},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2003396380},
year = {2017}
}
@techreport{Busoniu2010,
abstract = {Multi-agent systems can be used to address problems in a variety of domains , including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must instead discover a solution on their own, using learning. A significant part of the research on multi-agent learning concerns reinforcement learning techniques. This chapter reviews a representative selection of Multi-Agent Reinforcement Learning (MARL) algorithms for fully cooperative, fully competitive, and more general (neither cooperative nor competitive) tasks. The benefits and challenges of MARL are described. A central challenge in the field is the formal statement of a multi-agent learning goal; this chapter reviews the learning goals proposed in the literature. The problem domains where MARL techniques have been applied are briefly discussed. Several MARL algorithms are applied to an illustrative example involving the coordinated transportation of an object by two cooperative robots. In an outlook for the MARL field, a set of important open issues are identified, and promising research directions to address these issues are outlined.},
author = {Bus¸oniu, L Bus¸oniu and Babu{\v{s}}ka, R and {De Schutter}, B and Bus¸oniu, Lucian Bus¸oniu and Babu{\v{s}}ka, Robert and {De Schutter}, Bart and Transac, Ieee},
file = {:Users/hikaruasano/Documents/mendeley/Bus¸oniu et al.{\_}2010{\_}Multi-agent reinforcement learning An overview.pdf:pdf},
number = {2},
pages = {156--172},
publisher = {Springer},
title = {{Multi-agent reinforcement learning: An overview}},
volume = {38},
year = {2010}
}
@incollection{mccarthy1981some,
author = {McCarthy, John and Hayes, Patrick J},
booktitle = {Readings in artificial intelligence},
pages = {431--450},
publisher = {Elsevier},
title = {{Some philosophical problems from the standpoint of artificial intelligence}},
year = {1981}
}
@inproceedings{dai2018sbeed,
author = {Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Dai et al.{\_}2018{\_}Sbeed Convergent reinforcement learning with nonlinear function approximation.pdf:pdf},
pages = {1125--1134},
title = {{Sbeed: Convergent reinforcement learning with nonlinear function approximation}},
year = {2018}
}
@article{1390564238108620800,
author = {伊勢田, 哲治},
doi = {10.11425/sst.8.5},
file = {:Users/hikaruasano/Documents/mendeley/伊勢田{\_}2019{\_}境界設定問題はどのように概念化されるべきか.pdf:pdf},
issn = {2186-4942},
journal = {Studies in Science and Technology},
number = {1},
pages = {5--12},
publisher = {科学・技術研究会},
title = {境界設定問題はどのように概念化されるべきか},
url = {https://cir.nii.ac.jp/crid/1390564238108620800},
volume = {8},
year = {2019}
}
@article{Goda2018MultilevelMC,
author = {Goda, T and Hironaka, Tomohiko and Iwamoto, Takeru},
file = {:Users/hikaruasano/Documents/mendeley/Goda, Hironaka, Iwamoto{\_}2018{\_}Multilevel Monte Carlo estimation of expected information gains.pdf:pdf},
journal = {Stochastic Analysis and Applications},
pages = {581--600},
title = {{Multilevel Monte Carlo estimation of expected information gains}},
volume = {38},
year = {2018}
}
@inproceedings{Finn2019,
abstract = {Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a reward function, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the real world, especially in domains such as robotics and dialog systems, where the reward could depend on the unknown positions of objects or the emotional state of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present, or in a controlled laboratory setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect in the unstructured real world? In this paper, we formalize this problem setting as semi-supervised reinforcement learning (SSRL), where the reward function can only be evaluated in a set of “labeled” MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of “unlabeled” MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent's own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging, continuous control tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.},
archivePrefix = {arXiv},
arxivId = {1612.00429},
author = {Finn, Chelsea and Yu, Tianhe and Fu, Justin and Abbeel, Pieter and Levine, Sergey},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1612.00429},
file = {:Users/hikaruasano/Documents/mendeley/Finn et al.{\_}2019{\_}Generalizing skills with semi-supervised reinforcement learning.pdf:pdf},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Generalizing skills with semi-supervised reinforcement learning}},
year = {2019}
}
@inproceedings{10.1145/3287560.3287589,
abstract = {Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption.We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought.},
address = {New York, NY, USA},
author = {Friedler, Sorelle A and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P and Roth, Derek},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
doi = {10.1145/3287560.3287589},
file = {:Users/hikaruasano/Documents/mendeley/Friedler et al.{\_}2019{\_}A Comparative Study of Fairness-Enhancing Interventions in Machine Learning.pdf:pdf},
isbn = {9781450361255},
keywords = {Fairness-aware machine learning,benchmarks},
pages = {329--338},
publisher = {Association for Computing Machinery},
series = {FAT* '19},
title = {{A Comparative Study of Fairness-Enhancing Interventions in Machine Learning}},
url = {https://doi.org/10.1145/3287560.3287589},
year = {2019}
}
@article{Liu2018DelayedIO,
author = {Liu, L and Dean, Sarah and Rolf, Esther and Simchowitz, Max and Hardt, Moritz},
file = {:Users/hikaruasano/Documents/mendeley/Liu et al.{\_}2018{\_}Delayed Impact of Fair Machine Learning.pdf:pdf},
journal = {ArXiv},
title = {{Delayed Impact of Fair Machine Learning}},
volume = {abs/1803.0},
year = {2018}
}
@article{yarkoni2017choosing,
author = {Yarkoni, Tal and Westfall, Jacob},
file = {:Users/hikaruasano/Documents/mendeley/Yarkoni, Westfall{\_}2017{\_}Choosing prediction over explanation in psychology Lessons from machine learning.pdf:pdf},
journal = {Perspectives on Psychological Science},
number = {6},
pages = {1100--1122},
publisher = {Sage Publications Sage CA: Los Angeles, CA},
title = {{Choosing prediction over explanation in psychology: Lessons from machine learning}},
volume = {12},
year = {2017}
}
@misc{スライド151:online,
annote = {(Accessed on 07/20/2020)},
author = {国土交通省},
howpublished = {https://www.mlit.go.jp/common/001105108.pdf},
title = {既存住宅ストックの現状について},
year = {20}
}
@book{BC02891850p162,
author = {McIntyre, Lee C and 匠, 居村 and 智史, 大﨑 and 卓也, 西橋 and 完太郎, 大橋},
pages = {p.162},
publisher = {人文書院},
title = {ポストトゥルース},
url = {https://ci.nii.ac.jp/ncid/BC02891850},
year = {2020}
}
@article{2015,
author = {祐輝, 竹下 and 諭, 池田},
issn = {0540-4924},
journal = {宮崎大學工學部紀要},
keywords = {Combinatorial optimization,Combinatorial theory,Perfect play,Rectangle Othello},
number = {44},
pages = {221--227},
publisher = {宮崎大学工学部},
title = {縮小盤オセロにおける完全解析},
url = {http://ci.nii.ac.jp/naid/120005652984/ja/},
year = {2015}
}
@article{Kleinegesse2020SequentialBE,
author = {Kleinegesse, Steven and Drovandi, Christopher C and Gutmann, Michael U},
journal = {ArXiv},
title = {{Sequential Bayesian Experimental Design for Implicit Models via Mutual Information}},
volume = {abs/2003.0},
year = {2020}
}
@inproceedings{han2020cooperative,
author = {Han, Ruihua and Chen, Shengduo and Hao, Qi},
booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
file = {:Users/hikaruasano/Documents/mendeley/Han, Chen, Hao{\_}2020{\_}Cooperative multi-robot navigation in dynamic environment with deep reinforcement learning.pdf:pdf},
organization = {IEEE},
pages = {448--454},
title = {{Cooperative multi-robot navigation in dynamic environment with deep reinforcement learning}},
year = {2020}
}
@inproceedings{corbett2017algorithmic,
author = {Corbett-Davies, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
booktitle = {Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining},
file = {:Users/hikaruasano/Documents/mendeley/Corbett-Davies et al.{\_}2017{\_}Algorithmic decision making and the cost of fairness.pdf:pdf},
pages = {797--806},
title = {{Algorithmic decision making and the cost of fairness}},
year = {2017}
}
@article{Nakata2020,
abstract = {Though a reinforcement learning framework has numerous achievements, it requires a careful shaping of a re-ward function that represents the objective of a task. There is a class of task in which an expert could demonstrate the optimal way of doing, but it is difficult to design a proper reward function. For these tasks, an inverse reinforcement learning approach seems useful because it makes it possible to estimates a reward function from expert's demonstrations. Most existing inverse reinforcement learning algorithms assume that an expert gives demonstrations in a unique environment. However, an expert also could provide demonstrations of tasks within other environments of which have a specific objective function. For example, though it is hard to represent objective explicitly for a driving task, the driver could give demonstrations under multiple situations. In such cases, it is natural to utilize these demonstrations in multiple environments to estimate expert's reward functions. We formulate this problem as Bayesian Inverse Rein-forcement Learning problem and propose a Markov Chain Monte Carlo method for the problem. Experimental results show that the proposed method quantitatively overperforms existing methods.},
author = {Nakata, Yusuke and Arai, Sachiyo},
doi = {10.1527/tjsai.G-J73},
file = {:Users/hikaruasano/Documents/mendeley/Nakata, Arai{\_}2020{\_}Bayesian Inverse Reinforcement Learning for Demonstrations of an Expert in Multiple Dynamics.pdf:pdf},
issn = {1346-0714},
journal = {Transactions of the Japanese Society for Artificial Intelligence},
keywords = {Bayesian inference,Inverse reinforcement learning,Markov decision processes,Reinforcement learning},
month = {jan},
number = {1},
pages = {G--J73{\_}1--10},
publisher = {Japanese Society for Artificial Intelligence},
title = {{Bayesian Inverse Reinforcement Learning for Demonstrations of an Expert in Multiple Dynamics}},
url = {https://www.jstage.jst.go.jp/article/tjsai/35/1/35{\_}G-J73/{\_}article/-char/ja/},
volume = {35},
year = {2020}
}
@article{Wood2017,
abstract = {Present-day blockchain architectures all suffer from a number of issues not least practical means of extensi-bility and scalability. We believe this stems from tying two very important parts of the consensus architecture, namely canonicality and validity, too closely together. This paper introduces an architecture, the heterogeneous multi-chain, which fundamentally sets the two apart. In compartmentalising these two parts, and by keeping the overall functionality provided to an absolute minimum of security and transport, we introduce practical means of core extensibility in situ. Scalability is addressed through a divide-and-conquer approach to these two functions, scaling out of its bonded core through the incentivisation of untrusted public nodes. The heterogeneous nature of this architecture enables many highly divergent types of consensus systems interop-erating in a trustless, fully decentralised " federation " , allowing open and closed networks to have trust-free access to each other. We put forward a means of providing backwards compatibility with one or more pre-existing networks such as Ethereum. We believe that such a system provides a useful base-level component in the overall search for a practically implementable system capable of achieving global-commerce levels of scalability and privacy.},
author = {Wood, Gavin},
file = {:Users/hikaruasano/Documents/mendeley/Wood{\_}2017{\_}Polkadot Vision for a Heterogeneous Multi-Chain Framework.pdf:pdf},
journal = {Whitepaper},
pages = {1--21},
title = {{Polkadot: Vision for a Heterogeneous Multi-Chain Framework}},
url = {https://github.com/w3f/polkadot-white-paper/raw/master/PolkaDotPaper.pdf},
year = {2017}
}
@article{130008114810,
author = {絢子, 鈴木 and 博, 西浦},
doi = {10.2169/naika.109.2276},
issn = {0021-5384},
journal = {日本内科学会雑誌},
number = {11},
pages = {2276--2280},
publisher = {一般社団法人 日本内科学会},
title = {{IV．感染症の数理モデルと対策}},
url = {https://ci.nii.ac.jp/naid/130008114810/},
volume = {109},
year = {2020}
}
@article{1050569094193167488,
author = {隆道, 朝倉 and 仁, 河野},
file = {:Users/hikaruasano/Documents/mendeley/隆道, 仁{\_}2020{\_}人工知能研究における工学と社会学との融合の可能.pdf:pdf},
issn = {0387-6055},
journal = {東京工芸大学工学部紀要. 人文・社会編 = The Academic Reports, the Faculty of Engineering, Tokyo Polytechnic University},
number = {2},
pages = {21--26},
publisher = {東京工芸大学工学部},
title = {人工知能研究における工学と社会学との融合の可能性と課題- 研究開発と社会的期待との関係に対する一考察 -},
url = {https://cir.nii.ac.jp/crid/1050569094193167488},
volume = {43},
year = {2020}
}
@book{Bishop2006,
abstract = {Textbook for graduates. The field of pattern recognition has undergone substantial development over the years. This book reflects these developments while providing a grounding in the basic concepts of pattern recognition and machine learning. It is aimed at advanced undergraduates or first year PhD students, as well as researchers and practitioners. Introduction. Example : polynomial curve fitting ; Probability theory ; Model selection ; The curse of dimensionality ; Decision theory ; Information theory -- Probability distributions. Binary variables ; Multinomial variables ; The Gaussian distribution ; The exponential family ; Nonparametric methods -- Linear models for regression. Linear basis function models ; The bias-variance decomposition ; Bayesian linear regression ; Bayesian model comparison ; The evidence approximation ; Limitations of fixed basis functions -- Linear models for classification. Discriminant functions ; Probabilistic generative models ; Probabilistic discriminative models ; The Laplace approximation ; Bayesian logistic regression -- Neural networks. Feed-forward network functions ; Network training ; Error backpropagation ; The Hessian matrix ; Regularization in neural networks ; Mixture density networks ; Bayesian neural networks -- Kernel methods. Dual representations ; Constructing kernels ; Radial basis function networks ; Gaussian processes -- Sparse Kernel machines. Maximum margin classifiers ; Relevance vector machines -- Graphical models. Bayesian networks ; Conditional independence ; Markov random fields ; Inference in graphical models -- Mixture models and EM. K-means clustering ; Mixtures of Gaussians ; An alternative view of EM ; The EM algorithm in general -- Approximate inference. Variational inference ; Illustration : variational mixture of Gaussians ; Variational linear regression ; Exponential family distributions ; Local variational methods ; Variational logistic regression ; Expectation propagation -- Sampling methods. Basic sampling algorithms ; Markov chain Monte Carlo ; Gibbs sampling ; Slice sampling ; The hybrid Monte Carlo algorithm ; Estimating the partition function-- Continuous latent variables. Principal component analysis ; Probabilistic PCA ; Kernel PCA ; Nonlinear latent variable models -- Sequential data. Markov models ; Hidden Markov models ; Linear dynamical systems -- Combining models. Bayesian model averaging ; Committees ; Boosting ; Tree-based models ; Conditional mixture models -- Data sets -- Probability distributions -- Properties of matrices -- Calculus of variations -- Lagrange multipliers.},
author = {Bishop, Christopher M.},
isbn = {9780387310732},
pages = {738},
publisher = {Springer},
title = {{Pattern recognition and machine learning}},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2002309071},
year = {2006}
}
@article{journals/corr/LazaridouPB16b,
author = {Lazaridou, Angeliki and Peysakhovich, Alexander and Baroni, Marco},
file = {:Users/hikaruasano/Documents/mendeley/Lazaridou, Peysakhovich, Baroni{\_}2016{\_}Multi-Agent Cooperation and the Emergence of (Natural) Language.pdf:pdf},
journal = {CoRR},
keywords = {dblp},
title = {{Multi-Agent Cooperation and the Emergence of (Natural) Language}},
url = {http://dblp.uni-trier.de/db/journals/corr/corr1612.html{\#}LazaridouPB16b},
volume = {abs/1612.0},
year = {2016}
}
@article{yang2021adaptive,
abstract = {Critical sectors of human society are progressing toward the adop- tion of powerful artificial intelligence (AI) agents, which are trained individually on behalf of self-interested principals but deployed in a shared environment. Short of direct centralized regulation of AI, which is as difficult an issue as regulation of human actions, one must design institutional mechanisms that indirectly guide agents' behaviors to safeguard and improve social welfare in the shared environment. Our paper focuses on one important class of such mechanisms: the problem of adaptive incentive design, whereby a central planner intervenes on the payoffs of an agent population via incentives in order to optimize a system objective. To tackle this problem in high-dimensional environments whose dynamics may be unknown or too complex to model, we propose a model-free meta-gradient method to learn an adaptive incentive function in the context of multi-agent reinforcement learning. Via the prin- ciple of online cross-validation, the incentive designer explicitly accounts for its impact on agents' learning and, through them, the impact on future social welfare. Experiments on didactic bench- mark problems show that the proposed method can induce selfish agents to learn near-optimal cooperative behavior and significantly outperform learning-oblivious baselines. When applied to a com- plex simulated economy, the proposed method finds tax policies that achieve better trade-off between economic productivity and equality than baselines, a result that we interpret via a detailed behavioral analysis.},
author = {Yang, Jiachen and Wang, Ethan and Trivedi, Rakshit and Zhao, Tuo and Zha, Hongyuan},
file = {:Users/hikaruasano/Documents/mendeley/Yang et al.{\_}2022{\_}Adaptive Incentive Design with Multi-Agent Meta-Gradient Reinforcement Learning.pdf:pdf},
journal = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
title = {{Adaptive Incentive Design with Multi-Agent Meta-Gradient Reinforcement Learning}},
year = {2022}
}
@article{Ohsawa2014,
abstract = {In social networking service (SNS), popularity of an entity (e.g., person, company and place) roles an important criterion for people and organizations, and several studies pose to predict the popularity. Although recent papers which addressing the problem of predicting popularity use the attributes of entity itself, typically, the popularity of entities depends on the attributes of other semantically related entities. Hence, we take an approach exploiting the background semantic structure of the entities. Usually, many factors affect a person's popularity: the occupation, the parents, the birthplace, etc. All affect popularity. Predicting the popularity with the semantic structure is almost equivalent to solving the question: What type of relation most affects user preferences for an entity on a social medium? Our proposed method for popularity prediction is presented herein for predicting popularity, on a social medium of a given entity as a function of information of semantically related entities using DBpedia as a data source. DBpedia is a large semantic network produced by the semantic web community. The method has two techniques: (1) integrating accounts on SNS and DBpedia and (2) feature generation based on relations among entities. This is the first paper to propose an analysis method for SNS using semantic network.},
author = {Ohsawa, Shohei and Matsuo, Yutaka},
doi = {10.1527/tjsai.29.469},
file = {:Users/hikaruasano/Library/Application Support/Mendeley Desktop/Downloaded/Ohsawa, Matsuo - 2014 - Popularity Prediction for Entities on SNS Using Semantic Relations.pdf:pdf},
issn = {1346-0714},
journal = {Transactions of the Japanese Society for Artificial Intelligence},
keywords = {entity linking,feature generation,popularity prediction,social networking service},
number = {5},
pages = {469--482},
title = {{Popularity Prediction for Entities on SNS Using Semantic Relations}},
url = {http://www.facebook.com/2http://www.twitter.com/3http://www.google.com/+},
volume = {29},
year = {2014}
}
@inproceedings{pmlr-v70-haarnoja17a,
abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
editor = {Precup, Doina and Teh, Yee Whye},
pages = {1352--1361},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Reinforcement Learning with Deep Energy-Based Policies}},
url = {https://proceedings.mlr.press/v70/haarnoja17a.html},
volume = {70},
year = {2017}
}
@book{Angeles2014,
address = {Cham},
author = {Angeles, Jorge},
doi = {10.1007/978-3-319-01851-5},
file = {:Users/hikaruasano/Documents/mendeley/Angeles{\_}2014{\_}Fundamentals of Robotic Mechanical Systems.pdf:pdf},
isbn = {978-3-319-01850-8},
publisher = {Springer International Publishing},
series = {Mechanical Engineering Series},
title = {{Fundamentals of Robotic Mechanical Systems}},
url = {http://link.springer.com/10.1007/978-3-319-01851-5},
volume = {124},
year = {2014}
}
@misc{2050年に向け52:online,
annote = {(Accessed on 01/14/2022)},
author = {経済産業省},
howpublished = {https://www.meti.go.jp/shingikai/energy{\_}environment/2050{\_}gas{\_}jigyo/index.html},
title = {2050年に向けたガス事業の在り方研究会中間とりまとめ（案）},
year = {2021}
}
@inproceedings{lowe2017multi,
author = {Lowe, Ryan and Wu, Yi I and Tamar, Aviv and Harb, Jean and Abbeel, OpenAI Pieter and Mordatch, Igor},
booktitle = {Advances in neural information processing systems},
file = {:Users/hikaruasano/Documents/mendeley/Lowe et al.{\_}2017{\_}Multi-agent actor-critic for mixed cooperative-competitive environments.pdf:pdf},
pages = {6379--6390},
title = {{Multi-agent actor-critic for mixed cooperative-competitive environments}},
year = {2017}
}
@article{shazeer2017outrageously,
author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
file = {:Users/hikaruasano/Documents/mendeley/Shazeer et al.{\_}2017{\_}Outrageously large neural networks The sparsely-gated mixture-of-experts layer.pdf:pdf},
journal = {arXiv preprint arXiv:1701.06538},
title = {{Outrageously large neural networks: The sparsely-gated mixture-of-experts layer}},
year = {2017}
}
@article{jin2020neural,
author = {Jin, Wanxin and Wang, Zhaoran and Yang, Zhuoran and Mou, Shaoshuai},
file = {:Users/hikaruasano/Documents/mendeley/Jin et al.{\_}2020{\_}Neural certificates for safe control policies.pdf:pdf},
journal = {arXiv preprint arXiv:2006.08465},
title = {{Neural certificates for safe control policies}},
year = {2020}
}
@inproceedings{pmlr-v162-willi22a,
abstract = {Learning in general-sum games is unstable and frequently leads to socially undesirable (Pareto-dominated) outcomes. To mitigate this, Learning with Opponent-Learning Awareness (LOLA) introduced opponent shaping to this setting, by accounting for each agent's influence on their opponents' anticipated learning steps. However, the original LOLA formulation (and follow-up work) is inconsistent because LOLA models other agents as naive learners rather than LOLA agents. In previous work, this inconsistency was suggested as a cause of LOLA's failure to preserve stable fixed points (SFPs). First, we formalize consistency and show that higher-order LOLA (HOLA) solves LOLA's inconsistency problem if it converges. Second, we correct a claim made in the literature by Sch{\{}{\"{a}}{\}}fer and Anandkumar (2019), proving that Competitive Gradient Descent (CGD) does not recover HOLA as a series expansion (and fails to solve the consistency problem). Third, we propose a new method called Consistent LOLA (COLA), which learns update functions that are consistent under mutual opponent shaping. It requires no more than second-order derivatives and learns consistent update functions even when HOLA fails to converge. However, we also prove that even consistent update functions do not preserve SFPs, contradicting the hypothesis that this shortcoming is caused by LOLA's inconsistency. Finally, in an empirical evaluation on a set of general-sum games, we find that COLA finds prosocial solutions and that it converges under a wider range of learning rates than HOLA and LOLA. We support the latter finding with a theoretical result for a simple game.},
author = {Willi, Timon and Letcher, Alistair Hp and Treutlein, Johannes and Foerster, Jakob},
booktitle = {Proceedings of the 39th International Conference on Machine Learning},
editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
file = {:Users/hikaruasano/Documents/mendeley/Willi et al.{\_}2022{\_}{\{}COLA{\}} Consistent Learning with Opponent-Learning Awareness.pdf:pdf},
pages = {23804--23831},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{{\{}COLA{\}}: Consistent Learning with Opponent-Learning Awareness}},
url = {https://proceedings.mlr.press/v162/willi22a.html},
volume = {162},
year = {2022}
}
@article{Beck2018FastBE,
author = {Beck, Joakim and Dia, Ben Mansour and Espath, L and Long, Quan and Tempone, R},
journal = {Computer Methods in Applied Mechanics and Engineering},
pages = {523--553},
title = {{Fast Bayesian experimental design: Laplace-based importance sampling for the expected information gain}},
volume = {334},
year = {2018}
}
@article{DBLP:journals/corr/abs-2006-04222,
author = {Iqbal, Shariq and de Witt, Christian A Schr{\"{o}}der and Peng, Bei and B{\"{o}}hmer, Wendelin and Whiteson, Shimon and Sha, Fei},
file = {:Users/hikaruasano/Documents/mendeley/Iqbal et al.{\_}2020{\_}{\{}AI-QMIX{\}} Attention and Imagination for Dynamic Multi-Agent Reinforcement Learning.pdf:pdf},
journal = {CoRR},
title = {{{\{}AI-QMIX:{\}} Attention and Imagination for Dynamic Multi-Agent Reinforcement Learning}},
url = {https://arxiv.org/abs/2006.04222},
volume = {abs/2006.0},
year = {2020}
}
@misc{027s010081:online,
annote = {(Accessed on 07/13/2020)},
author = {資源エネルギー庁},
howpublished = {https://www.meti.go.jp/shingikai/santeii/pdf/027{\_}s01{\_}00.pdf},
title = {電源種別（太陽光・風力）のコスト動向等について},
year = {2016}
}
@inproceedings{pmlr-v119-perdomo20a,
abstract = {When predictions support decisions they may influence the outcome they aim to predict. We call such predictions performative; the prediction influences the target. Performativity is a well-studied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining. We develop a risk minimization framework for performative prediction bringing together concepts from statistics, game theory, and causality. A conceptual novelty is an equilibrium notion we call performative stability. Performative stability implies that the predictions are calibrated not against past outcomes, but against the future outcomes that manifest from acting on the prediction. Our main results are necessary and sufficient conditions for the convergence of retraining to a performatively stable point of nearly minimal loss. In full generality, performative prediction strictly subsumes the setting known as strategic classification. We thus also give the first sufficient conditions for retraining to overcome strategic feedback effects.},
author = {Perdomo, Juan and Zrnic, Tijana and Mendler-D{\"{u}}nner, Celestine and Hardt, Moritz},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
editor = {III, Hal Daum{\'{e}} and Singh, Aarti},
file = {:Users/hikaruasano/Documents/mendeley/Perdomo et al.{\_}2020{\_}Performative Prediction(2).pdf:pdf},
pages = {7599--7609},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Performative Prediction}},
url = {https://proceedings.mlr.press/v119/perdomo20a.html},
volume = {119},
year = {2020}
}
@misc{気候変動を踏まえ35:online,
annote = {(Accessed on 07/22/2020)},
author = {国土保全局, 国土交通省水管理},
howpublished = {https://www.mlit.go.jp/river/shinngikai{\_}blog/chisui{\_}kentoukai/index.html},
title = {気候変動を踏まえた治水計画に係る技術検討会},
year = {2019}
}
@inproceedings{chen2021randomized,
author = {Chen, Xinyue and Wang, Che and Zhou, Zijian and Ross, Keith W},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Chen et al.{\_}2021{\_}Randomized Ensembled Double Q-Learning Learning Fast Without a Model.pdf:pdf},
title = {{Randomized Ensembled Double Q-Learning: Learning Fast Without a Model}},
url = {https://openreview.net/forum?id=AY8zfZm0tDd},
year = {2021}
}
@article{Shaikh2017AnEM,
author = {Shaikh, Samiulla and Vishwakarma, Harit and Mehta, Sameep and Varshney, Kush R and Ramamurthy, K and Wei, Dennis},
journal = {ArXiv},
title = {{An End-To-End Machine Learning Pipeline That Ensures Fairness Policies}},
volume = {abs/1710.0},
year = {2017}
}
@article{xiong2018parametrized,
author = {Xiong, Jiechao and Wang, Qing and Yang, Zhuoran and Sun, Peng and Han, Lei and Zheng, Yang and Fu, Haobo and Zhang, Tong and Liu, Ji and Liu, Han},
file = {:Users/hikaruasano/Documents/mendeley/Xiong et al.{\_}2018{\_}Parametrized deep q-networks learning Reinforcement learning with discrete-continuous hybrid action space.pdf:pdf},
journal = {arXiv preprint arXiv:1810.06394},
title = {{Parametrized deep q-networks learning: Reinforcement learning with discrete-continuous hybrid action space}},
year = {2018}
}
@techreport{Sanghvi2020,
abstract = {Toward enabling next-generation robots capable of socially intelligent interaction with humans, we present a computational model of interactions in a social environment of multiple agents and multiple groups. The Multiagent Group Perception and Interaction (MGpi) network is a deep neural network that predicts the appropriate social action to execute in a group conversation (e.g., speak, listen, respond, leave), taking into account neighbors' observable features (e.g., location of people, gaze orientation, distraction, etc.). A central component of MGpi is the Kinesic-Proxemic-Message (KPM) gate, that performs social signal gating to extract important information from a group conversation. In particular, KPM gate filters incoming social cues from nearby agents by observing their body gestures (kinesics) and spatial behavior (proxemics). The MGpi network and its KPM gate are learned via imitation learning, using demonstrations from our designed social interaction sim-ulator. Further, we demonstrate the efficacy of the KPM gate as a social attention mechanism, achieving state-of-the-art performance on the task of group identification without using explicit group annotations, layout assumptions, or manually chosen parameters. KEYWORDS Social agent models; Socially interactive agents; Agent-based analysis of human interactions; Social group identification; Multiagent learning; Learning from demonstrations.},
author = {Sanghvi, Navyata and Yonetani, Ryo and Kitani, Kris},
booktitle = {ifaamas.org},
file = {:Users/hikaruasano/Documents/mendeley/Sanghvi, Yonetani, Kitani{\_}2020{\_}MGpi A Compu-tational Model of Multiagent Group Perception and Interaction.pdf:pdf},
keywords = {Agent-based analysis of human interactions,Learning from demonstrations.,Multiagent learning,Social agent models,Social group identification,Socially interactive agents},
title = {{MGpi: A Compu-tational Model of Multiagent Group Perception and Interaction}},
url = {www.ifaamas.org},
year = {2020}
}
@article{nair2015massively,
author = {Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and {De Maria}, Alessandro and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and Others},
file = {:Users/hikaruasano/Documents/mendeley/Nair et al.{\_}2015{\_}Massively parallel methods for deep reinforcement learning.pdf:pdf},
journal = {arXiv preprint arXiv:1507.04296},
title = {{Massively parallel methods for deep reinforcement learning}},
year = {2015}
}
@inproceedings{10.1145/3437963.3441824,
abstract = {As Recommender Systems (RS) influence more and more people in their daily life, the issue of fairness in recommendation is becoming more and more important. Most of the prior approaches to fairness-aware recommendation have been situated in a static or one-shot setting, where the protected groups of items are fixed, and the model provides a one-time fairness solution based on fairness-constrained optimization. This fails to consider the dynamic nature of the recommender systems, where attributes such as item popularity may change over time due to the recommendation policy and user engagement. For example, products that were once popular may become no longer popular, and vice versa. As a result, the system that aims to maintain long-term fairness on the item exposure in different popularity groups must accommodate this change in a timely fashion.Novel to this work, we explore the problem of long-term fairness in recommendation and accomplish the problem through dynamic fairness learning. We focus on the fairness of exposure of items in different groups, while the division of the groups is based on item popularity, which dynamically changes over time in the recommendation process. We tackle this problem by proposing a fairness-constrained reinforcement learning algorithm for recommendation, which models the recommendation problem as a Constrained Markov Decision Process (CMDP), so that the model can dynamically adjust its recommendation policy to make sure the fairness requirement is always satisfied when the environment changes. Experiments on several real-world datasets verify our framework's superiority in terms of recommendation performance, short-term fairness, and long-term fairness.},
address = {New York, NY, USA},
author = {Ge, Yingqiang and Liu, Shuchang and Gao, Ruoyuan and Xian, Yikun and Li, Yunqi and Zhao, Xiangyu and Pei, Changhua and Sun, Fei and Ge, Junfeng and Ou, Wenwu and Zhang, Yongfeng},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
doi = {10.1145/3437963.3441824},
file = {:Users/hikaruasano/Documents/mendeley/Ge et al.{\_}2021{\_}Towards Long-Term Fairness in Recommendation.pdf:pdf},
isbn = {9781450382977},
keywords = {constrained policy optimization,long-term fairness,recommender system,reinforcement learning,unbiased recommendation},
pages = {445--453},
publisher = {Association for Computing Machinery},
series = {WSDM '21},
title = {{Towards Long-Term Fairness in Recommendation}},
url = {https://doi.org/10.1145/3437963.3441824},
year = {2021}
}
@article{gu2021multi,
author = {Gu, Shangding and Kuba, Jakub Grudzien and Wen, Munning and Chen, Ruiqing and Wang, Ziyan and Tian, Zheng and Wang, Jun and Knoll, Alois and Yang, Yaodong},
file = {:Users/hikaruasano/Documents/mendeley/Gu et al.{\_}2021{\_}Multi-agent constrained policy optimisation.pdf:pdf},
journal = {arXiv preprint arXiv:2110.02793},
title = {{Multi-agent constrained policy optimisation}},
year = {2021}
}
@misc{洪水・高潮氾濫か52:online,
annote = {(Accessed on 06/28/2020)},
author = {中央防災会議 and 防災対策実行会議},
howpublished = {http://www.bousai.go.jp/fusuigai/kozuiworking/pdf/suigai/honbun.pdf},
title = {洪水・高潮氾濫からの大規模・広域避難に関する基本的な考え方（ 報 告 ）},
year = {2018}
}
@misc{norueene48:online,
annote = {(Accessed on 07/22/2020)},
author = {在ノルウェー日本国大使館},
howpublished = {https://www.no.emb-japan.go.jp/Japanese/Nikokukan/nikokukan{\_}files/norue{\_}enerugi{\_}jijou.pdf},
title = {ノルウェーのエネルギー事情},
year = {2012}
}
@article{2007,
author = {岡田 and 章},
issn = {18803164},
journal = {経済学史研究},
number = {1},
pages = {137--154},
publisher = {経済学史学会},
title = {ゲーム理論の歴史と現在--人間行動の解明を目指して},
url = {http://ci.nii.ac.jp/naid/110009498221/ja/},
volume = {49},
year = {2007}
}
@book{BN00151566,
author = {Kuhn, Thomas S and 茂, 中山},
publisher = {みすず書房},
title = {科学革命の構造},
url = {https://ci.nii.ac.jp/ncid/BN00151566},
year = {1971}
}
@article{110001137563,
author = {Tomita, Teruhiro},
issn = {03893367},
journal = {Information and communication studies},
number = {31},
pages = {141--158},
publisher = {Bunkyo University},
title = {{Competition Policy and Competitive Strategy of Japanese Beer Industry}},
url = {https://ci.nii.ac.jp/naid/110001137563/en/},
year = {2004}
}
@inproceedings{10.1145/3534678.3539481,
abstract = {Collaborative multi-agent reinforcement learning (MARL) has been widely used in many practical applications, where each agent makes a decision based on its own observation. Most mainstream methods treat each local observation as an entirety when modeling the decentralized local utility functions. However, they ignore the fact that local observation information can be further divided into several entities, and only part of the entities is helpful to model inference. Moreover, the importance of different entities may change over time. To improve the performance of decentralized policies, the attention mechanism is used to capture features of local information. Nevertheless, existing attention models rely on dense fully connected graphs and cannot better perceive important states. To this end, we propose a sparse state based MARL (S2RL) framework, which utilizes a sparse attention mechanism to discard irrelevant information in local observations. The local utility functions are estimated through the self-attention and sparse attention mechanisms separately, then are combined into a standard joint value function and auxiliary joint value function in the central critic. We design the S2RL framework as a plug-and-play module, making it general enough to be applied to various methods. Extensive experiments on StarCraft II show that S2RL can significantly improve the performance of many state-of-the-art methods.},
address = {New York, NY, USA},
author = {Luo, Shuang and Li, Yinchuan and Li, Jiahui and Kuang, Kun and Liu, Furui and Shao, Yunfeng and Wu, Chao},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3534678.3539481},
isbn = {9781450393850},
keywords = {deep learning,multi-agent reinforcement learning,sparse attention},
pages = {1183--1191},
publisher = {Association for Computing Machinery},
series = {KDD '22},
title = {{S2RL: Do We Really Need to Perceive All States in Deep Multi-Agent Reinforcement Learning?}},
url = {https://doi.org/10.1145/3534678.3539481},
year = {2022}
}
@article{Schulman2016HighDimensionalCC,
author = {Schulman, John and Moritz, P and Levine, Sergey and Jordan, Michael I and Abbeel, P},
file = {:Users/hikaruasano/Documents/mendeley/Schulman et al.{\_}2016{\_}High-Dimensional Continuous Control Using Generalized Advantage Estimation.pdf:pdf},
journal = {CoRR},
title = {{High-Dimensional Continuous Control Using Generalized Advantage Estimation}},
volume = {abs/1506.0},
year = {2016}
}
@inproceedings{jiang2018learning,
author = {Jiang, Jiechuan and Lu, Zongqing},
booktitle = {Advances in neural information processing systems},
file = {:Users/hikaruasano/Documents/mendeley/Jiang, Lu{\_}2018{\_}Learning attentional communication for multi-agent cooperation.pdf:pdf},
pages = {7254--7264},
title = {{Learning attentional communication for multi-agent cooperation}},
year = {2018}
}
@misc{令和元年交通安全55:online,
annote = {(Accessed on 07/15/2020)},
author = {内閣府},
howpublished = {https://www8.cao.go.jp/koutu/taisaku/r01kou{\_}haku/zenbun/index.html},
title = {令和元年交通安全白書},
year = {2019}
}
@book{BB16576592,
author = {円佳, 中野},
number = {713},
publisher = {光文社},
series = {光文社新書},
title = {「育休世代」のジレンマ : 女性活用はなぜ失敗するのか?},
url = {https://ci.nii.ac.jp/ncid/BB16576592},
year = {2014}
}
@article{yang2020overview,
author = {Yang, Yaodong and Wang, Jun},
file = {:Users/hikaruasano/Documents/mendeley/Yang, Wang{\_}2020{\_}An overview of multi-agent reinforcement learning from game theoretical perspective.pdf:pdf},
journal = {arXiv preprint arXiv:2011.00583},
title = {{An overview of multi-agent reinforcement learning from game theoretical perspective}},
year = {2020}
}
@article{中原崇2005携帯電話カーナビゲーションシステムにおける誘導案内方法の検討,
author = {奥出真理子 and 河村慶太郎 and 園田信幸},
journal = {土木計画学研究・講演集},
pages = {43},
title = {ロジスティクス向け経路探索アルゴリズムの検討},
volume = {124},
year = {2011}
}
@article{published_papers/29983698,
author = {裕貴, 瀧川},
doi = {10.11218/ojjams.34.47},
journal = {理論と方法},
number = {1},
pages = {47--64},
publisher = {数理社会学会},
title = {分析社会学と因果推論},
volume = {34},
year = {2019}
}
@article{brown2019extrapolating,
author = {Brown, Daniel S and Goo, Wonjoon and Nagarajan, Prabhat and Niekum, Scott},
file = {:Users/hikaruasano/Documents/mendeley/Brown et al.{\_}2019{\_}Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations.pdf:pdf},
journal = {arXiv preprint arXiv:1904.06387},
title = {{Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations}},
year = {2019}
}
@article{10.2307/2533849,
abstract = {Unlike traditional approaches, Bayesian methods enable formal combination of expert opinion and objective information into interim and final analyses of clinical trials data. However, most previous Bayesian approaches have based the stopping decision on the posterior probability content of one or more regions of the parameter space, thus implicitly determining a loss and decision structure. In this paper, we offer a fully Bayesian approach to this problem, specifying not only the likelihood and prior distributions but appropriate loss functions as well. At each data monitoring point, we enumerate the available decisions and investigate the use of backward induction, implemented via Monte Carlo methods, to choose the optimal course of action. We then present a forward sampling algorithm that substantially eases the analytic and computational burdens associated with backward induction, offering the possibility of fully Bayesian optimal sequential monitoring for previously untenable numbers of interim looks. We show that forward sampling can always identify the optimal sequential strategy in the case of a one-parameter exponential family with a conjugate prior and monotone loss functions as well as the best member of a certain class of strategies when backward induction is infeasible. Finally, we illustrate and compare the forward and backward approaches using data from a recent AIDS clinical trial.},
author = {Carlin, Bradley P and Kadane, Joseph B and Gelfand, Alan E},
issn = {0006341X, 15410420},
journal = {Biometrics},
number = {3},
pages = {964--975},
publisher = {[Wiley, International Biometric Society]},
title = {{Approaches for Optimal Sequential Decision Analysis in Clinical Trials}},
url = {http://www.jstor.org/stable/2533849},
volume = {54},
year = {1998}
}
@article{doi:10.1073/pnas.1510489113,
abstract = {In this paper we propose methods for estimating heterogeneity in causal effects in experimental and observational studies and for conducting hypothesis tests about the magnitude of differences in treatment effects across subsets of the population. We provide a data-driven approach to partition the data into subpopulations that differ in the magnitude of their treatment effects. The approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without “sparsity” assumptions. We propose an “honest” approach to estimation, whereby one sample is used to construct the partition and another to estimate treatment effects for each subpopulation. Our approach builds on regression tree methods, modified to optimize for goodness of fit in treatment effects and to account for honest estimation. Our model selection criterion anticipates that bias will be eliminated by honest estimation and also accounts for the effect of making additional splits on the variance of treatment effect estimates within each subpopulation. We address the challenge that the “ground truth” for a causal effect is not observed for any individual unit, so that standard approaches to cross-validation must be modified. Through a simulation study, we show that for our preferred method honest estimation results in nominal coverage for 90{\%} confidence intervals, whereas coverage ranges between 74{\%} and 84{\%} for nonhonest approaches. Honest estimation requires estimating the model with a smaller sample size; the cost in terms of mean squared error of treatment effects for our preferred method ranges between 7–22{\%}.},
author = {Athey, Susan and Imbens, Guido},
doi = {10.1073/pnas.1510489113},
file = {:Users/hikaruasano/Documents/mendeley/Athey, Imbens{\_}2016{\_}Recursive partitioning for heterogeneous causal effects.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
number = {27},
pages = {7353--7360},
title = {{Recursive partitioning for heterogeneous causal effects}},
url = {https://www.pnas.org/doi/abs/10.1073/pnas.1510489113},
volume = {113},
year = {2016}
}
@book{BA50694073,
author = {千鶴子編, 上野},
publisher = {勁草書房},
title = {構築主義とは何か},
url = {https://ci.nii.ac.jp/ncid/BA50694073},
year = {2001}
}
@misc{【独自】川崎重工86:online,
annote = {(Accessed on 06/15/2021)},
author = {読売新聞オンライン},
howpublished = {https://www.yomiuri.co.jp/economy/20210123-OYT1T50265/},
title = {【独自】川崎重工、水素が動力源の脱炭素大型船を建造へ{\ldots}世界初}
}
@inproceedings{gao2017intention,
author = {Gao, Wei and Hsu, David and Lee, Wee Sun and Shen, Shengmei and Subramanian, Karthikk},
booktitle = {Conference on robot learning},
organization = {PMLR},
pages = {185--194},
title = {{Intention-net: Integrating planning and deep learning for goal-directed autonomous navigation}},
year = {2017}
}
@article{doi:10.1137/19M1284981,
author = {Hironaka, Tomohiko and Giles, Michael B and Goda, Takashi and Thom, Howard},
doi = {10.1137/19M1284981},
file = {:Users/hikaruasano/Documents/mendeley/Hironaka et al.{\_}2020{\_}Multilevel Monte Carlo Estimation of the Expected Value of Sample Information.pdf:pdf},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
number = {3},
pages = {1236--1259},
title = {{Multilevel Monte Carlo Estimation of the Expected Value of Sample Information}},
url = {https://doi.org/10.1137/19M1284981},
volume = {8},
year = {2020}
}
@inproceedings{10.5555/3295222.3295322,
abstract = {Statistical performance bounds for reinforcement learning (RL) algorithms can be critical for high-stakes applications like healthcare. This paper introduces a new framework for theoretically measuring the performance of such algorithms called Uniform-PAC, which is a strengthening of the classical Probably Approximately Correct (PAC) framework. In contrast to the PAC framework, the uniform version may be used to derive high probability regret guarantees and so forms a bridge between the two setups that has been missing in the literature. We demonstrate the benefits of the new framework for finite-state episodic MDPs with a new algorithm that is Uniform-PAC and simultaneously achieves optimal regret and PAC guarantees except for a factor of the horizon.},
address = {Red Hook, NY, USA},
author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
file = {:Users/hikaruasano/Documents/mendeley/Dann, Lattimore, Brunskill{\_}2017{\_}Unifying PAC and Regret Uniform PAC Bounds for Episodic Reinforcement Learning(2).pdf:pdf},
isbn = {9781510860964},
keywords = {kozuno},
mendeley-tags = {kozuno},
pages = {5717--5727},
publisher = {Curran Associates Inc.},
series = {NIPS'17},
title = {{Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning}},
year = {2017}
}
@article{130007956202,
author = {佑斗, 大前 and 純, 豊谷 and 一之, 原 and 弘毅, 高橋},
doi = {10.3156/jsoft.32.6_998},
issn = {1347-7986},
journal = {知能と情報},
number = {6},
pages = {998--1007},
publisher = {日本知能情報ファジィ学会},
title = {感染症病床リソースと外出自粛を導入したマルチエージェント環境によるウィルス感染症の伝播予測手法},
url = {https://ci.nii.ac.jp/naid/130007956202/},
volume = {32},
year = {2020}
}
@article{Suresh2019AFF,
author = {Suresh, Harini and Guttag, John V},
file = {:Users/hikaruasano/Documents/mendeley/Suresh, Guttag{\_}2019{\_}A Framework for Understanding Unintended Consequences of Machine Learning.pdf:pdf},
journal = {ArXiv},
title = {{A Framework for Understanding Unintended Consequences of Machine Learning}},
volume = {abs/1901.1},
year = {2019}
}
@inproceedings{2017woodworthlearning,
author = {Woodworth, Blake and Gunasekar, Suriya and Ohannessian, Mesrob and Srebro, Nathan},
booktitle = {Proceedings of the 30th Conference on Learning Theory (COLT)},
file = {:Users/hikaruasano/Documents/mendeley/Woodworth et al.{\_}2017{\_}Learning Non-discriminatory Predictors.pdf:pdf},
pages = {1920--1953},
title = {{Learning Non-discriminatory Predictors}},
volume = {PMLR 65},
year = {2017}
}
@inproceedings{gilles2022thomas,
author = {Gilles, Thomas and Sabatini, Stefano and Tsishkou, Dzmitry and Stanciulescu, Bogdan and Moutarde, Fabien},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Gilles et al.{\_}2022{\_}{\{}THOMAS{\}} Trajectory Heatmap Output with learned Multi-Agent Sampling.pdf:pdf},
title = {{{\{}THOMAS{\}}: Trajectory Heatmap Output with learned Multi-Agent Sampling}},
url = {https://openreview.net/forum?id=QDdJhACYrlX},
year = {2022}
}
@inproceedings{chung2015recurrent,
author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron C and Bengio, Yoshua},
booktitle = {Advances in neural information processing systems},
file = {:Users/hikaruasano/Documents/mendeley/Chung et al.{\_}2015{\_}A recurrent latent variable model for sequential data.pdf:pdf},
pages = {2980--2988},
title = {{A recurrent latent variable model for sequential data}},
year = {2015}
}
@article{1390001206549130496,
author = {尾山, 大輔},
doi = {10.11499/sicejl.55.362},
file = {:Users/hikaruasano/Documents/mendeley/尾山{\_}2016{\_}社会ゲームにおけるナッシュ均衡の安定性：ポテンシャル・ゲームと完全予見動学(2).pdf:pdf},
issn = {04534662},
journal = {計測と制御},
number = {4},
pages = {362--367},
publisher = {公益社団法人 計測自動制御学会},
title = {社会ゲームにおけるナッシュ均衡の安定性：ポテンシャル・ゲームと完全予見動学},
url = {https://cir.nii.ac.jp/crid/1390001206549130496},
volume = {55},
year = {2016}
}
@article{2013,
author = {哲朗, 田中},
doi = {10.11429/sugaku.0651093},
file = {:Users/hikaruasano/Documents/mendeley/哲朗{\_}2013{\_}ゲームの解決.pdf:pdf},
issn = {0039-470X},
journal = {数学},
keywords = {complete information,game,puzzle,strongly solved,weakly solved,ゲーム,パズル,完全情報,弱解決,強解決},
month = {jan},
number = {1},
pages = {93--102find},
publisher = {一般社団法人 日本数学会},
title = {ゲームの解決},
volume = {65},
year = {2013}
}
@article{Azhar2008,
abstract = {Background: Observational epidemiologic data suggest that transmission of viral respiratory infection was significantly reduced during the SARS epidemic with the use of face masks as well as other infection control measures. However , there are no prospective randomised control trials on face masks in prevention of viral respiratory infections Aims: To determine the efficacy of surgical masks and P2 masks in households on the interruption of transmission of respiratory viruses. Methods: Prospective cluster randomized trial comparing surgical masks, non-fit-tested P2 (respirator) masks with no masks in interruption of viral transmission between household members. Families of children presenting to emergency department with influenza like illness (ILI) were randomised to one of the three groups and followed up for development of respiratory illness in other family members. Nasopharyngeal swabs of index patients and contacts that developed ILI were tested with a multiplex respiratory viral PCR for influenza A and B, parainfluenza, RSV, picornavirus, enterovirus, rhinovirus, adenovirus, coronaviruses human metapneumovirus. Results: We recruited 286 adults with exposure to respiratory infections in the Australian winters of 2006 and 2007-94 adults were randomized to surgical masks, 90 to P2 masks and 102 to the control group. Using intention to treat analysis , we found no significant difference in the relative risk of respiratory illness in the mask groups compared to control group. However, compliance with mask use was less than 50{\%}. In an adjusted analysis of compliant subjects, masks as a group had protective efficacy in excess of 80{\%} against clinical influenza-like illness. The efficacy against proven viral infection and between P2 masks (57{\%}) and surgical masks (33{\%}) was non-significant. Conclusions: This is the first RCT on mask use to be conducted and provides data to inform pandemic planning. We found compliance to be low, but compliance is affected by perception of risk. In a pandemic, we would expect compliance to improve. In compliant users, masks were highly efficacious. A larger study is required to enumerate the difference in efficacy (if any) between surgical and non-fit tested P2 masks.},
author = {Azhar, Esam I and Macintyre, C R and Dwyer, D and Seale, H and Fasher, M and Booy, R and Cheung, P and Ovdin, N and Browne, G and Ghamy, A B and Goorah, S S D and Caussy, B S and Cheeneebash, J and Ramchurn, S K},
doi = {10.1016/j.ijid.2008.05.879},
file = {:Users/hikaruasano/Documents/mendeley/Azhar et al.{\_}2008{\_}Dengue virus infection in the western region of Saudi Arabia The First Randomized, Controlled Clinical Trial of Mask U.pdf:pdf},
journal = {Article in International Journal of Infectious Diseases},
title = {{Dengue virus infection in the western region of Saudi Arabia The First Randomized, Controlled Clinical Trial of Mask Use in Households to Prevent Respiratory Virus Transmis-sion Clinical Complications of Chikungunya in Mauritius}},
url = {https://www.researchgate.net/publication/246311085},
year = {2008}
}
@inproceedings{mcmahan2003planning,
author = {McMahan, H Brendan and Gordon, Geoffrey J and Blum, Avrim},
booktitle = {Proceedings of the 20th International Conference on Machine Learning (ICML-03)},
file = {:Users/hikaruasano/Documents/mendeley/McMahan, Gordon, Blum{\_}2003{\_}Planning in the presence of cost functions controlled by an adversary.pdf:pdf},
pages = {536--543},
title = {{Planning in the presence of cost functions controlled by an adversary}},
year = {2003}
}
@article{zhang2021multi,
author = {Zhang, Kaiqing and Yang, Zhuoran and Ba$\backslash$csar, Tamer},
file = {:Users/hikaruasano/Documents/mendeley/Zhang, Yang, Bacsar{\_}2021{\_}Multi-agent reinforcement learning A selective overview of theories and algorithms.pdf:pdf},
journal = {Handbook of Reinforcement Learning and Control},
pages = {321--384},
publisher = {Springer},
title = {{Multi-agent reinforcement learning: A selective overview of theories and algorithms}},
year = {2021}
}
@book{2016世界市場で勝つルールメイキング戦略,
author = {國分俊史 and 福田峰之 and 角南篤},
isbn = {9784023315648},
publisher = {朝日新聞出版},
title = {世界市場で勝つルールメイキング戦略: 技術で勝る日本企業がなぜ負けるのか},
url = {https://books.google.co.jp/books?id=w70jMQAACAAJ},
year = {2016}
}
@article{Nakajima2020,
author = {Nakajima, Kohei},
doi = {10.35848/1347-4065/ab8d4f},
file = {:Users/hikaruasano/Documents/mendeley/Nakajima{\_}2020{\_}Physical reservoir computing—an introductory perspective.pdf:pdf},
issn = {0021-4922},
journal = {Japanese Journal of Applied Physics},
month = {jun},
number = {6},
pages = {060501},
title = {{Physical reservoir computing—an introductory perspective}},
url = {https://iopscience.iop.org/article/10.35848/1347-4065/ab8d4f},
volume = {59},
year = {2020}
}
@article{integ_hofman,
author = {Hofman, Jake and Watts, Duncan and Athey, Susan and Garip, Filiz and Griffiths, Thomas and Kleinberg, Jon and Margetts, Helen and Mullainathan, Sendhil and Salganik, Matthew and Vazire, Simine and Vespignani, Alessandro and Yarkoni, Tal},
doi = {10.1038/s41586-021-03659-0},
file = {:Users/hikaruasano/Documents/mendeley/Hofman et al.{\_}2021{\_}Integrating explanation and prediction in computational social science.pdf:pdf},
journal = {Nature},
pages = {1--8},
title = {{Integrating explanation and prediction in computational social science}},
volume = {595},
year = {2021}
}
@article{9112342,
author = {Chen, Yuxiao and Singletary, Andrew and Ames, Aaron D},
doi = {10.1109/LCSYS.2020.3000748},
file = {:Users/hikaruasano/Documents/mendeley/Chen, Singletary, Ames{\_}2021{\_}Guaranteed Obstacle Avoidance for Multi-Robot Operations With Limited Actuation A Control Barrier Function A.pdf:pdf},
journal = {IEEE Control Systems Letters},
number = {1},
pages = {127--132},
title = {{Guaranteed Obstacle Avoidance for Multi-Robot Operations With Limited Actuation: A Control Barrier Function Approach}},
volume = {5},
year = {2021}
}
@article{ChetanChudasama2014,
author = {{Chetan Chudasama}, Pramod Tripathi},
file = {:Users/hikaruasano/Documents/mendeley/Chetan Chudasama{\_}2014{\_}Optimizing Search Space of Othello Using Hybrid Approach.pdf:pdf},
journal = {International Journal of Morden Trends in Engineering and Research},
keywords = {-game playing,alpha- beta pruning,board game,genetic algorithm,othello},
number = {1},
title = {{Optimizing Search Space of Othello Using Hybrid Approach}},
volume = {1},
year = {2014}
}
@article{janssen2015truck,
author = {Janssen, Robbert and Zwijnenberg, Han and Blankers, Iris and de Kruijff, Janiek},
journal = {Driving the},
title = {{Truck platooning}},
year = {2015}
}
@misc{グレタ・トゥーン11:online,
annote = {(Accessed on 01/12/2021)},
author = {HUFFPOST},
howpublished = {https://www.huffingtonpost.jp/entry/story{\_}jp{\_}5ff40afec5b6e7974fd5e1f8?ncid=other{\_}twitter{\_}cooo9wqtham{\&}utm{\_}campaign=share{\_}twitter},
title = {グレタ・トゥーンベリさんら若者が、三菱商事などに抗議。公開質問状も},
year = {2021}
}
@techreport{Goodfellow,
abstract = {We propose a new framework for estimating generative models via an adversar-ial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
file = {:Users/hikaruasano/Documents/mendeley/Goodfellow et al.{\_}Unknown{\_}Generative Adversarial Nets.pdf:pdf},
title = {{Generative Adversarial Nets}},
url = {http://www.github.com/goodfeli/adversarial}
}
@techreport{Finn2016,
abstract = {Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.},
author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
booktitle = {jmlr.org},
file = {:Users/hikaruasano/Documents/mendeley/Finn, Levine, Abbeel{\_}2016{\_}Guided Cost Learning Deep Inverse Optimal Control via Policy Optimization(2).pdf:pdf},
title = {{Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization}},
url = {https://gateway.itc.u-tokyo.ac.jp/proceedings/papers/v48/finn16.pdf,DanaInfo=www.jmlr.org},
year = {2016}
}
@article{li2022online,
author = {Li, Maosen and Chen, Siheng and Shen, Yanning and Liu, Genjia and Tsang, Ivor W and Zhang, Ya},
file = {:Users/hikaruasano/Documents/mendeley/Li et al.{\_}2022{\_}Online Multi-Agent Forecasting With Interpretable Collaborative Graph Neural Networks.pdf:pdf},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
publisher = {IEEE},
title = {{Online Multi-Agent Forecasting With Interpretable Collaborative Graph Neural Networks}},
year = {2022}
}
@article{Ryan2003EstimatingEI,
author = {Ryan, K J},
journal = {Journal of Computational and Graphical Statistics},
pages = {585--603},
title = {{Estimating Expected Information Gains for Experimental Designs With Application to the Random Fatigue-Limit Model}},
volume = {12},
year = {2003}
}
@article{Wurman_D’Andrea_Mountz_2008,
author = {Wurman, Peter R and D'Andrea, Raffaello and Mountz, Mick},
doi = {10.1609/aimag.v29i1.2082},
file = {:Users/hikaruasano/Documents/mendeley/Wurman, D'Andrea, Mountz{\_}2008{\_}Coordinating Hundreds of Cooperative, Autonomous Vehicles in Warehouses.pdf:pdf},
journal = {AI Magazine},
number = {1},
pages = {9},
title = {{Coordinating Hundreds of Cooperative, Autonomous Vehicles in Warehouses}},
url = {https://ojs.aaai.org/index.php/aimagazine/article/view/2082},
volume = {29},
year = {2008}
}
@inproceedings{NEURIPS2019_10493aa8,
author = {Jiang, Jiechuan and Lu, Zongqing},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d$\backslash$textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
file = {:Users/hikaruasano/Documents/mendeley/Jiang, Lu{\_}2019{\_}Learning Fairness in Multi-Agent Systems.pdf:pdf},
publisher = {Curran Associates, Inc.},
title = {{Learning Fairness in Multi-Agent Systems}},
url = {https://proceedings.neurips.cc/paper/2019/file/10493aa88605cad5ab4752b04a63d172-Paper.pdf},
volume = {32},
year = {2019}
}
@inproceedings{mcmahan2017communication,
author = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
booktitle = {Artificial Intelligence and Statistics},
file = {:Users/hikaruasano/Documents/mendeley/McMahan et al.{\_}2017{\_}Communication-efficient learning of deep networks from decentralized data.pdf:pdf},
organization = {PMLR},
pages = {1273--1282},
title = {{Communication-efficient learning of deep networks from decentralized data}},
year = {2017}
}
@inproceedings{feng2018learning,
author = {Feng, Jun and Li, Heng and Huang, Minlie and Liu, Shichen and Ou, Wenwu and Wang, Zhirong and Zhu, Xiaoyan},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
file = {:Users/hikaruasano/Documents/mendeley/Feng et al.{\_}2018{\_}Learning to collaborate Multi-scenario ranking via multi-agent reinforcement learning.pdf:pdf},
pages = {1939--1948},
title = {{Learning to collaborate: Multi-scenario ranking via multi-agent reinforcement learning}},
year = {2018}
}
@misc{pp29gaiy54:online,
annote = {(Accessed on 07/20/2020)},
author = {国立社会保障・人口問題研究所},
howpublished = {http://www.ipss.go.jp/pp-zenkoku/j/zenkoku2017/pp29{\_}gaiyou.pdf},
title = {日本の将来推計人口},
year = {2017}
}
@book{BA50694073p140,
author = {千鶴子編, 上野},
pages = {pp.140--141},
publisher = {勁草書房},
title = {構築主義とは何か},
url = {https://ci.nii.ac.jp/ncid/BA50694073},
year = {2001}
}
@misc{【記事更新】私の18:online,
annote = {(Accessed on 01/31/2021)},
author = {人工知能学会},
howpublished = {https://www.ai-gakkai.or.jp/my-bookmark{\_}vol35-no4/},
title = {{私のブックマーク「反実仮想機械学習」（Counterfactual Machine Learning, CFML）}},
year = {2020}
}
@inproceedings{DBLP:journals/corr/SchaulQAS15,
author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
booktitle = {4th International Conference on Learning Representations, {\{}ICLR{\}} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
editor = {Bengio, Yoshua and LeCun, Yann},
title = {{Prioritized Experience Replay}},
url = {http://arxiv.org/abs/1511.05952},
year = {2016}
}
@book{1130282270713418240,
author = {佐藤, 俊樹},
publisher = {岩波書店},
title = {社会科学と因果分析 : ウェーバーの方法論から知の現在へ},
url = {https://cir.nii.ac.jp/crid/1130282270713418240},
year = {2019}
}
@misc{kulbvq0079:online,
annote = {(Accessed on 01/21/2021)},
author = {中央酪農会議},
howpublished = {https://www.dairy.co.jp/dairydata/jdc{\_}news/kulbvq000000nvzo-att/kulbvq000000nw4a.pdf},
title = {わが国のチーズ輸入をめぐる情勢},
year = {2019}
}
@article{Gerstner2021MultilevelMC,
author = {Gerstner, Thomas and Harrach, Bastian and Roth, Daniel and $\backslash$vSimon, Martin},
file = {:Users/hikaruasano/Documents/mendeley/Gerstner et al.{\_}2021{\_}Multilevel Monte Carlo learning.pdf:pdf},
journal = {ArXiv},
title = {{Multilevel Monte Carlo learning}},
volume = {abs/2102.0},
year = {2021}
}
@inproceedings{NIPS2016_dc4c44f6,
author = {Goh, Gabriel and Cotter, Andrew and Gupta, Maya and Friedlander, Michael P},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Lee, D and Sugiyama, M and Luxburg, U and Guyon, I and Garnett, R},
file = {:Users/hikaruasano/Documents/mendeley/Goh et al.{\_}2016{\_}Satisfying Real-world Goals with Dataset Constraints.pdf:pdf},
publisher = {Curran Associates, Inc.},
title = {{Satisfying Real-world Goals with Dataset Constraints}},
url = {https://proceedings.neurips.cc/paper/2016/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf},
volume = {29},
year = {2016}
}
@article{DBLP:journals/ml/Williams92,
author = {Williams, Ronald J},
doi = {10.1007/BF00992696},
journal = {Mach. Learn.},
pages = {229--256},
title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
url = {https://doi.org/10.1007/BF00992696},
volume = {8},
year = {1992}
}
@techreport{Ng,
author = {Ng, Andrew Y and Stuart, Russel},
file = {:Users/hikaruasano/Documents/mendeley/Ng, Stuart{\_}Unknown{\_}Algorithms for inverse reinforcement learning.pdf:pdf},
title = {{Algorithms for inverse reinforcement learning}}
}
@article{2006,
author = {溝口, 理一郎; 古崎, 晃司; 來村, 徳信; 笹島, 宗彦},
isbn = {4274202925},
publisher = {オーム社},
title = {オントロジー構築入門},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2002296339},
year = {2006}
}
@inproceedings{Kearns2000BiasVarianceEB,
author = {Kearns, M and Singh, Satinder},
booktitle = {COLT},
file = {:Users/hikaruasano/Documents/mendeley/Kearns, Singh{\_}2000{\_}Bias-Variance Error Bounds for Temporal Difference Updates.pdf:pdf},
title = {{Bias-Variance Error Bounds for Temporal Difference Updates}},
year = {2000}
}
@inproceedings{foerster2016learning,
author = {Foerster, Jakob and Assael, Ioannis Alexandros and {De Freitas}, Nando and Whiteson, Shimon},
booktitle = {Advances in neural information processing systems},
file = {:Users/hikaruasano/Documents/mendeley/Foerster et al.{\_}2016{\_}Learning to communicate with deep multi-agent reinforcement learning.pdf:pdf},
pages = {2137--2145},
title = {{Learning to communicate with deep multi-agent reinforcement learning}},
year = {2016}
}
@article{7857061,
author = {Wang, Li and Ames, Aaron D and Egerstedt, Magnus},
doi = {10.1109/TRO.2017.2659727},
file = {:Users/hikaruasano/Documents/mendeley/Wang, Ames, Egerstedt{\_}2017{\_}Safety Barrier Certificates for Collisions-Free Multirobot Systems.pdf:pdf},
journal = {IEEE Transactions on Robotics},
number = {3},
pages = {661--674},
title = {{Safety Barrier Certificates for Collisions-Free Multirobot Systems}},
volume = {33},
year = {2017}
}
@article{berk2017convex,
author = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Neel, Seth and Roth, Aaron},
file = {:Users/hikaruasano/Documents/mendeley/Berk et al.{\_}2017{\_}A convex framework for fair regression.pdf:pdf},
journal = {arXiv preprint arXiv:1706.02409},
title = {{A convex framework for fair regression}},
year = {2017}
}
@book{BB27883108,
author = {英敬, 石田 and 浩紀, 東},
number = {002},
pages = {p.30},
publisher = {ゲンロン},
series = {ゲンロン叢書},
title = {新記号論 : 脳とメディアが出会うとき},
url = {https://ci.nii.ac.jp/ncid/BB27883108},
year = {2019}
}
@article{UNESCO2020,
author = {UNESCO},
file = {:Users/hikaruasano/Documents/mendeley/UNESCO{\_}2020{\_}Artificial intelligence and gender equality key findings of UNESCO's Global Dialogue.pdf:pdf},
pages = {25},
title = {{Artificial intelligence and gender equality: key findings of UNESCO's Global Dialogue}},
year = {2020}
}
@misc{Emerging86:online,
annote = {(Accessed on 01/31/2022)},
author = {Association, World Nuclear},
howpublished = {http://www.world-nuclear.org/information-library/country-profiles/others/emerging-nuclear-energy-countries.aspx，1月31日閲覧},
title = {{Emerging Nuclear Energy Countries | New Nuclear Build Countries}},
year = {2022}
}
@techreport{Pinto2017,
abstract = {Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H ∞ control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced-that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper, Walker2d and Ant) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.},
author = {Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
booktitle = {dl.acm.org},
file = {:Users/hikaruasano/Documents/mendeley/Pinto et al.{\_}2017{\_}Robust Adversarial Reinforcement Learning.pdf:pdf},
title = {{Robust Adversarial Reinforcement Learning}},
url = {https://gateway.itc.u-tokyo.ac.jp/citation.cfm?id=3305972,DanaInfo=dl.acm.org},
year = {2017}
}
@inproceedings{10.5555/3398761.3399065,
abstract = {Humans collaborate in dynamic and flexible ways. Collaboration requires agents to coordinate their behavior on the fly, sometimes jointly solving a single task together and other times dividing it up into sub-tasks to work on in parallel. We develop Bayesian Delegation, a learning mechanism for decentralized multi-agent coordination that enables agents to rapidly infer the sub-tasks that other agents are working on by inverse planning. These inferences enable agents to determine, in the absence of communication, whether to plan jointly with others or work on complementary sub-tasks. We test this model in a suite of decentralized multi-agent environments inspired by cooking problems. To succeed, agents must coordinate both their high-level plans (sub-task) and their low-level actions (avoiding collisions). Including joint sub-tasks in the prior of Bayesian delegation enables agents to carry out sub-tasks that neither agent can finish independently. The full system outperforms lesioned systems that are missing one or more of these capabilities.},
address = {Richland, SC},
author = {Wang, Rose E and Wu, Sarah A and Evans, James A and Tenenbaum, Joshua B and Parkes, David C and Kleiman-Weiner, Max},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
file = {:Users/hikaruasano/Documents/mendeley/Wang et al.{\_}2020{\_}Too Many Cooks Coordinating Multi-Agent Collaboration Through Inverse Planning(2).pdf:pdf},
isbn = {9781450375184},
keywords = {bayesian inference,coordination,inverse planning,multi-agent,reinforcement learning},
pages = {2032--2034},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
series = {AAMAS '20},
title = {{Too Many Cooks: Coordinating Multi-Agent Collaboration Through Inverse Planning}},
year = {2020}
}
@article{ruadulescu2020multi,
author = {R$\backslash$uadulescu, Roxana and Mannion, Patrick and Roijers, Diederik M and Now{\'{e}}, Ann},
file = {:Users/hikaruasano/Documents/mendeley/Ruadulescu et al.{\_}2020{\_}Multi-objective multi-agent decision making a utility-based analysis and survey.pdf:pdf},
journal = {Autonomous Agents and Multi-Agent Systems},
number = {1},
pages = {1--52},
publisher = {Springer},
title = {{Multi-objective multi-agent decision making: a utility-based analysis and survey}},
volume = {34},
year = {2020}
}
@article{Plaat1997,
abstract = {We present two recursive state space search algorithms that are based on SSS* and Dual*. Both dominate Alpha-Beta on a node count basis and one of them is always faster in searching random trees. These results have been derived on artificial game trees with negligible leaf node evaluation time. For practical applications with more complex leaf evaluation functions we conjecture that our recursive state space search algorithms perform even better and might eventually supersede the popular directional search methods.},
author = {Plaat, Aske},
file = {:Users/hikaruasano/Documents/mendeley/Plaat{\_}1997{\_}A Minimax Algorithm faster than NegaScout.pdf:pdf},
isbn = {9062161014},
keywords = {MTD minimax game theory search algorithm},
number = {January},
pages = {1--6},
title = {{A Minimax Algorithm faster than NegaScout}},
year = {1997}
}
@inproceedings{ma2021distributed,
author = {Ma, Ziyuan and Luo, Yudong and Ma, Hang},
booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
file = {:Users/hikaruasano/Documents/mendeley/Ma, Luo, Ma{\_}2021{\_}Distributed heuristic multi-agent path finding with communication.pdf:pdf},
organization = {IEEE},
pages = {8699--8705},
title = {{Distributed heuristic multi-agent path finding with communication}},
year = {2021}
}
@article{duenez2021statistical,
author = {Du{\'{e}}{\~{n}}ez-Guzm{\'{a}}n, Edgar A and McKee, Kevin R and Mao, Yiran and Coppin, Ben and Chiappa, Silvia and Vezhnevets, Alexander Sasha and Bakker, Michiel A and Bachrach, Yoram and Sadedin, Suzanne and Isaac, William and Others},
file = {:Users/hikaruasano/Documents/mendeley/Du{\'{e}}{\~{n}}ez-Guzm{\'{a}}n et al.{\_}2021{\_}Statistical discrimination in learning agents.pdf:pdf},
journal = {arXiv preprint arXiv:2110.11404},
title = {{Statistical discrimination in learning agents}},
year = {2021}
}
@article{zhang2016data,
author = {Zhang, Huaguang and Jiang, He and Luo, Yanhong and Xiao, Geyang},
journal = {IEEE Transactions on Industrial Electronics},
number = {5},
pages = {4091--4100},
publisher = {IEEE},
title = {{Data-driven optimal consensus control for discrete-time multi-agent systems with unknown dynamics using reinforcement learning method}},
volume = {64},
year = {2016}
}
@article{published_papers/10142583,
author = {筒井淳也},
file = {:Users/hikaruasano/Documents/mendeley/筒井淳也{\_}2012{\_}マルチレベル分析を有効活用するには.pdf:pdf},
journal = {社会と調査},
pages = {102--106},
title = {マルチレベル分析を有効活用するには},
volume = {9},
year = {2012}
}
@inproceedings{NIPS2017_8cb22bdd,
author = {Xie, Qizhe and Dai, Zihang and Du, Yulun and Hovy, Eduard and Neubig, Graham},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
file = {:Users/hikaruasano/Documents/mendeley/Xie et al.{\_}2017{\_}Controllable Invariance through Adversarial Feature Learning.pdf:pdf},
publisher = {Curran Associates, Inc.},
title = {{Controllable Invariance through Adversarial Feature Learning}},
url = {https://proceedings.neurips.cc/paper/2017/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Paper.pdf},
volume = {30},
year = {2017}
}
@article{van2016pre,
author = {{Van't Veer}, Anna Elisabeth and Giner-Sorolla, Roger},
journal = {Journal of experimental social psychology},
pages = {2--12},
publisher = {Elsevier},
title = {{Pre-registration in social psychology—A discussion and suggested template}},
volume = {67},
year = {2016}
}
@misc{hinanjyo59:online,
annote = {(Accessed on 06/28/2020)},
author = {江戸川区防災危機管理室防災危機管理課},
howpublished = {https://www.city.edogawa.tokyo.jp/documents/519/hinanjyouhou.pdf},
title = {避難先について質問・回答},
year = {2019}
}
@inproceedings{HuSiyi2021UUmr,
abstract = {Recent advances in multi-agent reinforcement learning have been largely limited training one model from scratch for every new task. This limitation occurs due to the restriction of the model architecture related to fixed input and output dimensions, which hinder the experience accumulation and transfer of the learned agent over tasks across diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multiagent games). In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing a single architecture to fit tasks with different observation and action configuration requirements. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation, using an importance weight determined with the aid of the selfattention mechanism. Compared to a standard transformer block, the proposed model, which we name Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multiagent reinforcement learning pipeline and equip it with strong generalization abilities that enable multiple tasks to be handled at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant improvements relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).},
address = {United States},
author = {Hu, Siyi and Zhu, Fengda and Chang, Xiaojun and Liang, Xiaodan},
booktitle = {Proceedings of the 9th International Conference on Learning Representations (ICLR 2021)},
file = {:Users/hikaruasano/Documents/mendeley/Hu et al.{\_}2021{\_}UPDET Universal multi-agent reinforcement learning via policy decoupling with transformers.pdf:pdf},
keywords = {Multi-agent Reinforcement Learning,Transfer Learning},
pages = {15},
publisher = {International Conference on Representation Learning},
title = {{UPDET: Universal multi-agent reinforcement learning via policy decoupling with transformers}},
year = {2021}
}
@inproceedings{Goodfellow2014GenerativeAN,
author = {Goodfellow, I and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron C and Bengio, Yoshua},
booktitle = {NIPS},
title = {{Generative Adversarial Nets}},
year = {2014}
}
@inproceedings{reymond2019pareto,
author = {Reymond, Mathieu and Now{\'{e}}, Ann},
booktitle = {Proceedings of the adaptive and learning agents workshop (ALA-19) at AAMAS},
file = {:Users/hikaruasano/Documents/mendeley/Reymond, Now{\'{e}}{\_}2019{\_}Pareto-DQN Approximating the Pareto front in complex multi-objective decision problems.pdf:pdf},
title = {{Pareto-DQN: Approximating the Pareto front in complex multi-objective decision problems}},
year = {2019}
}
@book{Watanabe2008,
abstract = {本体価格: 3500円 文献あり},
author = {Watanabe, Takahiro},
isbn = {9784532133467},
publisher = {Nihon Keizai Shinbun Shuppansha},
title = {ゼミナールゲーム理論入門},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2002621639},
year = {2008}
}
@book{BA50694073p256-257,
author = {千鶴子編, 上野},
pages = {pp.256--257},
publisher = {勁草書房},
title = {構築主義とは何か},
url = {https://ci.nii.ac.jp/ncid/BA50694073},
year = {2001}
}
@article{宋裕姫2008米国における睡眠障害による社会損失を減らすための国家的な試みとその効果,
author = {宋裕姫 and 西野精治},
journal = {Journal of UOEH},
number = {3},
pages = {329--352},
publisher = {学校法人 産業医科大学},
title = {米国における睡眠障害による社会損失を減らすための国家的な試みとその効果},
volume = {30},
year = {2008}
}
@article{chang2003all,
author = {Chang, Yu-Han and Ho, Tracey and Kaelbling, Leslie},
file = {:Users/hikaruasano/Documents/mendeley/Chang, Ho, Kaelbling{\_}2003{\_}All learning is local Multi-agent learning in global reward games.pdf:pdf},
journal = {Advances in neural information processing systems},
title = {{All learning is local: Multi-agent learning in global reward games}},
volume = {16},
year = {2003}
}
@inproceedings{liu2020multi,
author = {Liu, Yong and Wang, Weixun and Hu, Yujing and Hao, Jianye and Chen, Xingguo and Gao, Yang},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
file = {:Users/hikaruasano/Documents/mendeley/Liu et al.{\_}2020{\_}Multi-agent game abstraction via graph attention neural network.pdf:pdf},
number = {05},
pages = {7211--7218},
title = {{Multi-agent game abstraction via graph attention neural network}},
volume = {34},
year = {2020}
}
@inbook{Winkle2016,
abstract = {In his meta-analysis, Thomas Winkle documents exemplary analyses of potential safety-enhancing vehicle systems with low degrees of automation. However, a safety prognosis of highly or fully automated vehicles depends on assumptions, as so far no series applications of such features exist. For testing methods to develop and validate safe automated vehicles with reasonable expenditure, the author recommends combining worldwide traffic accident-, weather-, vehicle operation data and traffic simulations. Based on these findings, a realistic evaluation of internationally prospective, and statistically relevant real world traffic scenarios as well as error processes and stochastic models can be analysed (in combination with virtual tests in laboratories and driving simulators) to control critical driving situations.},
address = {Berlin, Heidelberg},
author = {Winkle, Thomas},
booktitle = {Autonomous Driving: Technical, Legal and Social Aspects},
doi = {10.1007/978-3-662-48847-8_17},
editor = {Maurer, Markus and Gerdes, J Christian and Lenz, Barbara and Winner, Hermann},
isbn = {978-3-662-48847-8},
pages = {335--364},
publisher = {Springer Berlin Heidelberg},
title = {{Safety Benefits of Automated Vehicles: Extended Findings from Accident Research for Development, Validation and Testing}},
url = {https://doi.org/10.1007/978-3-662-48847-8{\_}17},
year = {2016}
}
@article{wang2021socially,
author = {Wang, Letian and Sun, Liting and Tomizuka, Masayoshi and Zhan, Wei},
journal = {IEEE Robotics and Automation Letters},
number = {2},
pages = {3421--3428},
publisher = {IEEE},
title = {{Socially-compatible behavior design of autonomous vehicles with verification on real human data}},
volume = {6},
year = {2021}
}
@inproceedings{chen2019crowd,
author = {Chen, Changan and Liu, Yuejiang and Kreiss, Sven and Alahi, Alexandre},
booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
file = {:Users/hikaruasano/Documents/mendeley/Chen et al.{\_}2019{\_}Crowd-robot interaction Crowd-aware robot navigation with attention-based deep reinforcement learning.pdf:pdf},
organization = {IEEE},
pages = {6015--6022},
title = {{Crowd-robot interaction: Crowd-aware robot navigation with attention-based deep reinforcement learning}},
year = {2019}
}
@inproceedings{pmlr-v119-schmitt20a,
abstract = {We investigate the combination of actor-critic reinforcement learning algorithms with a uniform large-scale experience replay and propose solutions for two ensuing challenges: (a) efficient actor-critic learning with experience replay (b) the stability of off-policy learning where agents learn from other agents behaviour. To this end we analyze the bias-variance tradeoffs in V-trace, a form of importance sampling for actor-critic methods. Based on our analysis, we then argue for mixing experience sampled from replay with on-policy experience, and propose a new trust region scheme that scales effectively to data distributions where V-trace becomes unstable. We provide extensive empirical validation of the proposed solutions on DMLab-30 and further show the benefits of this setup in two training regimes for Atari: (1) a single agent is trained up until 200M environment frames per game (2) a population of agents is trained up until 200M environment frames each and may share experience. We demonstrate state-of-the-art data efficiency among model-free agents in both regimes.},
author = {Schmitt, Simon and Hessel, Matteo and Simonyan, Karen},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
editor = {III, Hal Daum{\'{e}} and Singh, Aarti},
pages = {8545--8554},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Off-Policy Actor-Critic with Shared Experience Replay}},
url = {https://proceedings.mlr.press/v119/schmitt20a.html},
volume = {119},
year = {2020}
}
@inproceedings{pmlr-v119-kleinegesse20a,
abstract = {Implicit stochastic models, where the data-generation distribution is intractable but sampling is possible, are ubiquitous in the natural sciences. The models typically have free parameters that need to be inferred from data collected in scientific experiments. A fundamental question is how to design the experiments so that the collected data are most useful. The field of Bayesian experimental design advocates that, ideally, we should choose designs that maximise the mutual information (MI) between the data and the parameters. For implicit models, however, this approach is severely hampered by the high computational cost of computing posteriors and maximising MI, in particular when we have more than a handful of design variables to optimise. In this paper, we propose a new approach to Bayesian experimental design for implicit models that leverages recent advances in neural MI estimation to deal with these issues. We show that training a neural network to maximise a lower bound on MI allows us to jointly determine the optimal design and the posterior. Simulation studies illustrate that this gracefully extends Bayesian experimental design for implicit models to higher design dimensions.},
author = {Kleinegesse, Steven and Gutmann, Michael U},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
editor = {III, Hal Daum{\'{e}} and Singh, Aarti},
pages = {5316--5326},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{{\{}B{\}}ayesian Experimental Design for Implicit Models by Mutual Information Neural Estimation}},
url = {https://proceedings.mlr.press/v119/kleinegesse20a.html},
volume = {119},
year = {2020}
}
@misc{MachineB47:online,
annote = {(Accessed on 01/31/2021)},
author = {ProPublica},
howpublished = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
title = {{Machine Bias}},
year = {2016}
}
@article{ghosh2018learning,
author = {Ghosh, Dibya and Gupta, Abhishek and Levine, Sergey},
file = {:Users/hikaruasano/Documents/mendeley/Ghosh, Gupta, Levine{\_}2018{\_}Learning actionable representations with goal-conditioned policies.pdf:pdf},
journal = {arXiv preprint arXiv:1811.07819},
title = {{Learning actionable representations with goal-conditioned policies}},
year = {2018}
}
@unknown{unknown,
author = {Baumann, Tobias and Graepel, Thore and Shawe-Taylor, John},
file = {:Users/hikaruasano/Documents/mendeley/Baumann, Graepel, Shawe-Taylor{\_}2018{\_}Adaptive Mechanism Design Learning to Promote Cooperation.pdf:pdf},
title = {{Adaptive Mechanism Design: Learning to Promote Cooperation}},
year = {2018}
}
@inproceedings{Dai2018SBEEDCR,
author = {Dai, Bo and Shaw, Albert Eaton and Li, L and Xiao, Lin and He, Niao and Liu, Z and Chen, Jianshu and Song, L},
booktitle = {ICML},
file = {:Users/hikaruasano/Documents/mendeley/Dai et al.{\_}2018{\_}Sbeed Convergent reinforcement learning with nonlinear function approximation.pdf:pdf},
title = {{SBEED: Convergent Reinforcement Learning with Nonlinear Function Approximation}},
year = {2018}
}
@misc{pdf用indd16:online,
annote = {(Accessed on 07/18/2020)},
author = {Steel, NIppon},
howpublished = {https://www.nipponsteel.com/company/publications/quarterly-nssmc/pdf/2019{\_}25{\_}22{\_}25.pdf},
title = {水素時代の鉄づくり},
year = {2019}
}
@inproceedings{NEURIPS2018_85422afb,
author = {ZHENG, Y A N and Meng, Zhaopeng and Hao, Jianye and Zhang, Zongzhang and Yang, Tianpei and Fan, Changjie},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Bengio, S and Wallach, H and Larochelle, H and Grauman, K and Cesa-Bianchi, N and Garnett, R},
file = {:Users/hikaruasano/Documents/mendeley/ZHENG et al.{\_}2018{\_}A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents(2).pdf:pdf},
publisher = {Curran Associates, Inc.},
title = {{A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents}},
url = {https://proceedings.neurips.cc/paper/2018/file/85422afb467e9456013a2a51d4dff702-Paper.pdf},
volume = {31},
year = {2018}
}
@book{BC00323223,
author = {裕子編, 藤垣},
number = {1},
pages = {p.8},
publisher = {東京大学出版会},
series = {科学技術社会論の挑戦},
title = {科学技術社会論とは何か},
url = {https://ci.nii.ac.jp/ncid/BC00323223},
year = {2020}
}
@article{Adolphs2009,
abstract = {Social cognition in humans is distinguished by psychological processes that allow us to make inferences about what is going on inside other people—their intentions, feelings, and thoughts. Some of these processes likely account for aspects of human social behavior that are unique, such as our culture and civilization. Most schemes divide social information processing into those processes that are relatively automatic and driven by the stimuli, versus those that are more deliberative and controlled, and sensitive to context and strategy. These distinctions are reflected in the neural structures that underlie social cognition, where there is a recent wealth of data primarily from functional neuroimaging. Here I provide a broad survey of the key abilities, processes, and ways in which to relate these to data from cognitive neuroscience.},
author = {Adolphs, Ralph},
doi = {10.1146/annurev.psych.60.110707.163514.The},
file = {:Users/hikaruasano/Documents/mendeley/Adolphs{\_}2009{\_}NIH Public Access SOCIAL BRAIN MODEL.pdf:pdf},
isbn = {0066-4308},
issn = {0066-4308},
journal = {Annual Review of Psychology},
keywords = {amygdala,empathy,modularity,prefrontal,prefrontal cortex,simulation,social cognition,social neuroscience,theory of mind},
pages = {693--716},
pmid = {18771388},
title = {{NIH Public Access SOCIAL BRAIN MODEL}},
volume = {60},
year = {2009}
}
@inproceedings{NIPS2017_b8b9c74a,
author = {Pleiss, Geoff and Raghavan, Manish and Wu, Felix and Kleinberg, Jon and Weinberger, Kilian Q},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
file = {:Users/hikaruasano/Documents/mendeley/Pleiss et al.{\_}2017{\_}On Fairness and Calibration.pdf:pdf},
publisher = {Curran Associates, Inc.},
title = {{On Fairness and Calibration}},
url = {https://proceedings.neurips.cc/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf},
volume = {30},
year = {2017}
}
@misc{Unknown,
author = {環境省},
title = {平成29年版 環境・循環型社会・生物多様性白書},
url = {https://www.env.go.jp/policy/hakusyo/h29/index.html},
urldate = {2020-06-12}
}
@misc{2021,
author = {経済産業省},
file = {:Users/hikaruasano/Documents/mendeley/経済産業省{\_}2021{\_}エネルギー基本計画.pdf:pdf},
title = {エネルギー基本計画},
url = {https://www.meti.go.jp/press/2021/10/20211022005/20211022005-1.pdf},
urldate = {2020-03-27},
year = {2021}
}
@inproceedings{menon2020pulse,
author = {Menon, Sachit and Damian, Alexandru and Hu, Shijia and Ravi, Nikhil and Rudin, Cynthia},
booktitle = {Proceedings of the ieee/cvf conference on computer vision and pattern recognition},
pages = {2437--2445},
title = {{Pulse: Self-supervised photo upsampling via latent space exploration of generative models}},
year = {2020}
}
@book{2004a,
author = {和夫, 盛山},
publisher = {有斐閣社},
title = {社会調査法入門},
year = {2004}
}
@inproceedings{9922266,
author = {Chen, Siyuan and Wang, Meiling and Song, Wenjie and Yang, Yi and Fu, Mengyin},
booktitle = {2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)},
doi = {10.1109/ITSC55140.2022.9922266},
file = {:Users/hikaruasano/Documents/mendeley/Chen et al.{\_}2022{\_}Multi-agent Reinforcement Learning-based Twin-vehicle Fair Cooperative Driving in Dynamic Highway Scenarios.pdf:pdf},
pages = {730--736},
title = {{Multi-agent Reinforcement Learning-based Twin-vehicle Fair Cooperative Driving in Dynamic Highway Scenarios}},
year = {2022}
}
@phdthesis{thomas2017a,
author = {Rainforth, Thomas William Gamlen},
publisher = {University of Oxford},
school = {University of Oxford},
title = {{Automating inference, learning, and design using probabilistic programming}},
year = {2017}
}
@article{fan2020distributed,
author = {Fan, Tingxiang and Long, Pinxin and Liu, Wenxi and Pan, Jia},
file = {:Users/hikaruasano/Documents/mendeley/Fan et al.{\_}2020{\_}Distributed multi-robot collision avoidance via deep reinforcement learning for navigation in complex scenarios.pdf:pdf},
journal = {The International Journal of Robotics Research},
number = {7},
pages = {856--892},
publisher = {SAGE Publications Sage UK: London, England},
title = {{Distributed multi-robot collision avoidance via deep reinforcement learning for navigation in complex scenarios}},
volume = {39},
year = {2020}
}
@article{Rossell2013,
abstract = {In high-throughput experiments, the sample size is typically chosen informally. Most formal sample-size calculations depend critically on prior knowledge. We propose a sequential strategy that, by updating knowledge when new data are available, depends less critically on prior assumptions. Experiments are stopped or continued based on the potential benefits in obtaining additional data. The underlying decision-theoretic framework guarantees the design to proceed in a coherent fashion. We propose intuitively appealing, easy-to-implement utility functions. As in most sequential design problems, an exact solution is prohibitive. We propose a simulation-based approximation that uses decision boundaries. We apply the method to RNA-seq, microarray, and reverse-phase protein array studies and show its potential advantages. The approach has been added to the Bioconductor package gaga.},
author = {Rossell, David and Uller, Peter M ¨},
doi = {10.1093/biostatistics/kxs026},
file = {:Users/hikaruasano/Documents/mendeley/Rossell, Uller{\_}2013{\_}Sequential stopping for high-throughput experiments.pdf:pdf},
journal = {Biostatistics},
keywords = {Decision theory,Forward simulation,High-throughput experiments,Multiple testing,Optimal design,Sample size,Sequential design},
pages = {75--86},
title = {{Sequential stopping for high-throughput experiments}},
url = {https://academic.oup.com/biostatistics/article/14/1/75/250357},
year = {2013}
}
@inproceedings{10.5555/3504035.3504398,
abstract = {Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.},
author = {Foerster, Jakob N and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
file = {:Users/hikaruasano/Documents/mendeley/Foerster et al.{\_}2018{\_}Counterfactual Multi-Agent Policy Gradients(2).pdf:pdf},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
series = {AAAI'18/IAAI'18/EAAI'18},
title = {{Counterfactual Multi-Agent Policy Gradients}},
year = {2018}
}
@inproceedings{Hessel2018RainbowCI,
author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad Gheshlaghi and Silver, David},
booktitle = {AAAI},
title = {{Rainbow: Combining Improvements in Deep Reinforcement Learning}},
year = {2018}
}
@misc{•Nickelr43:online,
annote = {(Accessed on 01/31/2022)},
author = {Statista},
howpublished = {https://www.statista.com/statistics/273634/nickel-reserves-worldwide-by-country/，1月31日参照},
title = {{Nickel reserves worldwide by country 2020}}
}
@inproceedings{sheikh2020multi,
author = {Sheikh, Hassam Ullah and B{\"{o}}l{\"{o}}ni, Ladislau},
booktitle = {2020 International Joint Conference on Neural Networks (IJCNN)},
file = {:Users/hikaruasano/Documents/mendeley/Sheikh, B{\"{o}}l{\"{o}}ni{\_}2020{\_}Multi-agent reinforcement learning for problems with combined individual and team reward.pdf:pdf},
organization = {IEEE},
pages = {1--8},
title = {{Multi-agent reinforcement learning for problems with combined individual and team reward}},
year = {2020}
}
@inproceedings{li2019robust,
author = {Li, Shihui and Wu, Yi and Cui, Xinyue and Dong, Honghua and Fang, Fei and Russell, Stuart},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
number = {01},
pages = {4213--4220},
title = {{Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient}},
volume = {33},
year = {2019}
}
@article{perolat2022mastering,
author = {Perolat, Julien and de Vylder, Bart and Hennes, Daniel and Tarassov, Eugene and Strub, Florian and de Boer, Vincent and Muller, Paul and Connor, Jerome T and Burch, Neil and Anthony, Thomas and Others},
file = {:Users/hikaruasano/Documents/mendeley/Perolat et al.{\_}2022{\_}Mastering the Game of Stratego with Model-Free Multiagent Reinforcement Learning.pdf:pdf},
journal = {arXiv e-prints},
pages = {arXiv----2206},
title = {{Mastering the Game of Stratego with Model-Free Multiagent Reinforcement Learning}},
year = {2022}
}
@article{van2021model,
author = {{Van Der Vaart}, Pascal and Mahajan, Anuj and Whiteson, Shimon},
file = {:Users/hikaruasano/Documents/mendeley/Van Der Vaart, Mahajan, Whiteson{\_}2021{\_}Model based multi-agent reinforcement learning with tensor decompositions.pdf:pdf},
journal = {arXiv preprint arXiv:2110.14524},
title = {{Model based multi-agent reinforcement learning with tensor decompositions}},
year = {2021}
}
@article{singh2018learning,
author = {Singh, Amanpreet and Jain, Tushar and Sukhbaatar, Sainbayar},
file = {:Users/hikaruasano/Documents/mendeley/Singh, Jain, Sukhbaatar{\_}2018{\_}Learning when to communicate at scale in multiagent cooperative and competitive tasks.pdf:pdf},
journal = {arXiv preprint arXiv:1812.09755},
title = {{Learning when to communicate at scale in multiagent cooperative and competitive tasks}},
year = {2018}
}
@misc{ヒートポンプの利99:online,
annote = {(Accessed on 01/14/2022)},
author = {一般財団法人ヒートポンプ・蓄熱センター},
howpublished = {https://www.hptcj.or.jp/study/tabid/103/Default.aspx},
title = {ヒートポンプの利用範囲}
}
@inproceedings{henderson2018optiongan,
author = {Henderson, Peter and Chang, Wei-Di and Bacon, Pierre-Luc and Meger, David and Pineau, Joelle and Precup, Doina},
booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
file = {:Users/hikaruasano/Documents/mendeley/Krishnan et al.{\_}2016{\_}HIRL Hierarchical Inverse Reinforcement Learning for Long-Horizon Tasks with Delayed Rewards.pdf:pdf},
title = {{Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforcement learning}},
year = {2018}
}
@article{levine2018reinforcement,
author = {Levine, Sergey},
file = {:Users/hikaruasano/Documents/mendeley/Levine{\_}2018{\_}Reinforcement learning and control as probabilistic inference Tutorial and review(2).pdf:pdf},
journal = {arXiv preprint arXiv:1805.00909},
title = {{Reinforcement learning and control as probabilistic inference: Tutorial and review}},
year = {2018}
}
@misc{https://doi.org/10.17863/cam.41459,
author = {Dillon, Sarah and Collett, Clementine},
doi = {10.17863/CAM.41459},
keywords = {artificial intelligence,gender,research},
publisher = {Apollo - University of Cambridge Repository},
title = {{AI and Gender: Four Proposals for Future Research}},
url = {https://www.repository.cam.ac.uk/handle/1810/294360},
year = {2019}
}
@inproceedings{10.5555/3306127.3331967,
abstract = {Multi-agent reinforcement learning has gained lot of popularity primarily owing to the success of deep function approximation architectures. However, many real-life multi-agent applications often impose constraints on the joint action sequence that can be taken by the agents. In this work, we formulate such problems in the framework of constrained cooperative stochastic games. Under this setting, the goal of the agents is to obtain joint action sequence that minimizes a total cost objective criterion subject to total cost penalty/budget functional constraints. To this end, we utilize the Lagrangian formulation and propose actor-critic algorithms. Through experiments on a constrained multi-agent grid world task, we demonstrate that our algorithms converge to near-optimal joint action sequences satisfying the given constraints.},
address = {Richland, SC},
author = {Diddigi, Raghuram Bharadwaj and Reddy, D Sai Koti and K.J., Prabuchandran and Bhatnagar, Shalabh},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
file = {:Users/hikaruasano/Documents/mendeley/Diddigi et al.{\_}2019{\_}Actor-Critic Algorithms for Constrained Multi-Agent Reinforcement Learning.pdf:pdf},
isbn = {9781450363099},
keywords = {actor-critic algorithms,constrained reinforcement learning,cooperative stochastic game,multi-agent learning},
pages = {1931--1933},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
series = {AAMAS '19},
title = {{Actor-Critic Algorithms for Constrained Multi-Agent Reinforcement Learning}},
year = {2019}
}
@inproceedings{liu2021coach,
author = {Liu, Bo and Liu, Qiang and Stone, Peter and Garg, Animesh and Zhu, Yuke and Anandkumar, Anima},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Liu et al.{\_}2021{\_}Coach-Player Multi-Agent Reinforcement Learning for Dynamic Team Composition.pdf:pdf},
organization = {PMLR},
pages = {6860--6870},
title = {{Coach-Player Multi-Agent Reinforcement Learning for Dynamic Team Composition}},
year = {2021}
}
@article{黒川文子2018ev,
author = {黒川文子 and Others},
journal = {環境共生研究},
number = {11},
pages = {25--36},
title = {{EV へのシフトと CO2 排出量に関する考察}},
year = {2018}
}
@article{app12073439,
abstract = {Multi-agent reinforcement learning (MARL) algorithms have made great achievements in various scenarios, but there are still many problems in solving sequential social dilemmas (SSDs). In SSDs, the agent{\&}rsquo;s actions not only change the instantaneous state of the environment but also affect the latent state which will, in turn, affect all agents. However, most of the current reinforcement learning algorithms focus on analyzing the value of instantaneous environment state while ignoring the study of the latent state, which is very important for establishing cooperation. Therefore, we propose a novel counterfactual reasoning-based multi-agent reinforcement learning algorithm to evaluate the continuous contribution of agent actions on the latent state. We compute that using simulation reasoning and building an action evaluation network. Then through counterfactual reasoning, we can get a single agent{\&}rsquo;s influence on the environment. Using this continuous contribution as an intrinsic reward enables the agent to consider the collective, thereby promoting cooperation. We conduct experiments in the SSDs environment, and the results show that the collective reward is increased by at least 25{\%} which demonstrates the excellent performance of our proposed algorithm compared to the state-of-the-art algorithms.},
author = {Yuan, Yuyu and Zhao, Pengqian and Guo, Ting and Jiang, Hongpu},
doi = {10.3390/app12073439},
file = {:Users/hikaruasano/Documents/mendeley/Yuan et al.{\_}2022{\_}Counterfactual-Based Action Evaluation Algorithm in Multi-Agent Reinforcement Learning.pdf:pdf},
issn = {2076-3417},
journal = {Applied Sciences},
number = {7},
title = {{Counterfactual-Based Action Evaluation Algorithm in Multi-Agent Reinforcement Learning}},
url = {https://www.mdpi.com/2076-3417/12/7/3439},
volume = {12},
year = {2022}
}
@inproceedings{ijcai2019p316,
author = {Fan, Zhou and Su, Rui and Zhang, Weinan and Yu, Yong},
booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {\{}IJCAI-19{\}}},
doi = {10.24963/ijcai.2019/316},
file = {:Users/hikaruasano/Documents/mendeley/Fan et al.{\_}2019{\_}Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space.pdf:pdf},
pages = {2279--2285},
publisher = {International Joint Conferences on Artificial Intelligence Organization},
title = {{Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space}},
url = {https://doi.org/10.24963/ijcai.2019/316},
year = {2019}
}
@inproceedings{mnih2016asynchronous,
author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
booktitle = {International conference on machine learning},
file = {:Users/hikaruasano/Library/Application Support/Mendeley Desktop/Downloaded/A3C - 2013 - Asynchronous Methods for Deep Reinforcement Learning Volodymyr.pdf:pdf},
pages = {1928--1937},
title = {{Asynchronous methods for deep reinforcement learning}},
year = {2016}
}
@inproceedings{pmlr-v80-zhang18n,
abstract = {We consider the fully decentralized multi-agent reinforcement learning (MARL) problem, where the agents are connected via a time-varying and possibly sparse communication network. Specifically, we assume that the reward functions of the agents might correspond to different tasks, and are only known to the corresponding agent. Moreover, each agent makes individual decisions based on both the information observed locally and the messages received from its neighbors over the network. To maximize the globally averaged return over the network, we propose two fully decentralized actor-critic algorithms, which are applicable to large-scale MARL problems in an online fashion. Convergence guarantees are provided when the value functions are approximated within the class of linear functions. Our work appears to be the first theoretical study of fully decentralized MARL algorithms for networked agents that use function approximation.},
address = {Stockholmsm{\"{a}}ssan, Stockholm Sweden},
author = {Zhang, Kaiqing and Yang, Zhuoran and Liu, Han and Zhang, Tong and Basar, Tamer},
editor = {Dy, Jennifer and Krause, Andreas},
file = {:Users/hikaruasano/Documents/mendeley/Zhang et al.{\_}2018{\_}Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents(2).pdf:pdf},
pages = {5872--5881},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents}},
url = {http://proceedings.mlr.press/v80/zhang18n.html},
volume = {80},
year = {2018}
}
@book{2004,
author = {隆宏, 藤本},
isbn = {453231139X},
pages = {349},
publisher = {日本経済新聞社},
title = {日本のもの造り哲学},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2001892803},
year = {2004}
}
@book{BA7988910X,
author = {Hacking, Ian and 康夫, 出口 and 暁, 久米},
pages = {pp.143--230},
publisher = {岩波書店},
title = {何が社会的に構成されるのか},
url = {https://ci.nii.ac.jp/ncid/BA7988910X},
year = {2006}
}
@article{nachum2019multi,
author = {Nachum, Ofir and Ahn, Michael and Ponte, Hugo and Gu, Shixiang and Kumar, Vikash},
file = {:Users/hikaruasano/Documents/mendeley/Nachum et al.{\_}2019{\_}Multi-agent manipulation via locomotion using hierarchical sim2real.pdf:pdf},
journal = {arXiv preprint arXiv:1908.05224},
title = {{Multi-agent manipulation via locomotion using hierarchical sim2real}},
year = {2019}
}
@article{karimi2022relationship,
author = {Karimi, Amir-Hossein and Muandet, Krikamol and Kornblith, Simon and Sch{\"{o}}lkopf, Bernhard and Kim, Been},
file = {:Users/hikaruasano/Documents/mendeley/Karimi et al.{\_}2022{\_}On the Relationship Between Explanation and Prediction A Causal View.pdf:pdf},
journal = {arXiv preprint arXiv:2212.06925},
title = {{On the Relationship Between Explanation and Prediction: A Causal View}},
year = {2022}
}
@techreport{Fulton2017,
author = {Fulton, Lew and Mason, Davis Jacob and Meroux, Dominique and Davis, U C},
file = {:Users/hikaruasano/Documents/mendeley/Fulton et al.{\_}2017{\_}TRANSPORTATION Three Revolutions in Urban.pdf:pdf},
keywords = {Barr Foundation,ClimateWorks Foundation,William and Flora Hewlett Foundation},
title = {{TRANSPORTATION Three Revolutions in Urban}},
year = {2017}
}
@inproceedings{9635984,
author = {Zanger, Moritz A and Daaboul, Karam and {Marius Z{\"{o}}llner}, J},
booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS51168.2021.9635984},
file = {:Users/hikaruasano/Documents/mendeley/Zanger, Daaboul, Marius Z{\"{o}}llner{\_}2021{\_}Safe Continuous Control with Constrained Model-Based Policy Optimization.pdf:pdf},
pages = {3512--3519},
title = {{Safe Continuous Control with Constrained Model-Based Policy Optimization}},
year = {2021}
}
@article{10.2307/24538168,
abstract = {In this paper we present a methodology for designing experiments for efficiently estimating the parameters of models with computationally intractable likelihoods. The approach combines a commonly used methodology for robust experimental design, based on Markov chain Monte Carlo sampling, with approximate Bayesian computation (ABC) to ensure that no likelihood evaluations are required. The utility function considered for precise parameter estimation is based upon the precision of the ABC posterior distribution, which we form efficiently via the ABC rejection algorithm based on pre-computed model simulations. Our focus is on stochastic models and, in particular, we investigate the methodology for Markov process models of epidemics and macroparasite population evolution. The macroparasite example involves a multivariate process and we assess the loss of information from not observing all variables.},
author = {Drovandi, Christopher C and Pettitt, Anthony N},
issn = {0006341X, 15410420},
journal = {Biometrics},
number = {4},
pages = {937--948},
publisher = {[Wiley, International Biometric Society]},
title = {{Bayesian Experimental Design for Models with Intractable Likelihoods}},
url = {http://www.jstor.org/stable/24538168},
volume = {69},
year = {2013}
}
@article{130005480528,
author = {雄市, 服部 and 亮, 野津 and 克宏, 本多},
doi = {10.14864/fss.30.0_420},
journal = {日本知能情報ファジィ学会 ファジィ システム シンポジウム 講演論文集},
number = {0},
pages = {420--425},
publisher = {日本知能情報ファジィ学会},
title = {都市モデルに留意した感染症流行過程観測マルチエージェントシミュレータ},
url = {https://ci.nii.ac.jp/naid/130005480528/},
volume = {30},
year = {2014}
}
@article{doi:10.1126/science.abe2629,
abstract = {Theories of human decision-making have proliferated in recent years. However, these theories are often difficult to distinguish from each other and offer limited improvement in accounting for patterns in decision-making over earlier theories. Peterson et al. leverage machine learning to evaluate classical decision theories, increase their predictive power, and generate new theories of decision-making (see the Perspective by Bhatia and He). This method has implications for theory generation in other domains. Science, abe2629, this issue p. 1209; see also abi7668, p. 1150 A machine learning approach suggests that people make decisions in a way that violates the assumptions of classic decision-making theories. Predicting and understanding how people make decisions has been a long-standing goal in many fields, with quantitative models of human decision-making informing research in both the social sciences and engineering. We show how progress toward this goal can be accelerated by using large datasets to power machine-learning algorithms that are constrained to produce interpretable psychological theories. Conducting the largest experiment on risky choice to date and analyzing the results using gradient-based optimization of differentiable decision theories implemented through artificial neural networks, we were able to recapitulate historical discoveries, establish that there is room to improve on existing theories, and discover a new, more accurate model of human decision-making in a form that preserves the insights from centuries of research.},
author = {Peterson, Joshua C and Bourgin, David D and Agrawal, Mayank and Reichman, Daniel and Griffiths, Thomas L},
doi = {10.1126/science.abe2629},
journal = {Science},
number = {6547},
pages = {1209--1214},
title = {{Using large-scale experiments and machine learning to discover theories of human decision-making}},
url = {https://www.science.org/doi/abs/10.1126/science.abe2629},
volume = {372},
year = {2021}
}
@book{BB27195896,
author = {裕子, 藤垣},
publisher = {岩波書店},
series = {岩波科学ライブラリー},
title = {科学者の社会的責任},
url = {https://ci.nii.ac.jp/ncid/BB27195896},
year = {2018}
}
@inproceedings{conf/iclr/CaoLLLTC18,
author = {Cao, Kris and Lazaridou, Angeliki and Lanctot, Marc and Leibo, Joel Z and Tuyls, Karl and Clark, Stephen},
booktitle = {ICLR (Poster)},
file = {:Users/hikaruasano/Documents/mendeley/Cao et al.{\_}2018{\_}Emergent Communication through Negotiation.pdf:pdf},
keywords = {dblp},
publisher = {OpenReview.net},
title = {{Emergent Communication through Negotiation}},
url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2018.html{\#}CaoLLLTC18},
year = {2018}
}
@article{10.1145/3457607,
abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
address = {New York, NY, USA},
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
doi = {10.1145/3457607},
file = {:Users/hikaruasano/Documents/mendeley/Mehrabi et al.{\_}2021{\_}A Survey on Bias and Fairness in Machine Learning.pdf:pdf},
issn = {0360-0300},
journal = {ACM Comput. Surv.},
keywords = {Fairness and bias in artificial intelligence,deep learning,machine learning,natural language processing,representation learning},
month = {jul},
number = {6},
publisher = {Association for Computing Machinery},
title = {{A Survey on Bias and Fairness in Machine Learning}},
url = {https://doi.org/10.1145/3457607},
volume = {54},
year = {2021}
}
@dissertation{sbyy1003103,
address = {Mersin {\"{U}}niversitesi Eğitim Fak{\"{u}}ltesi Sosyal Bilimler Ve T{\"{u}}rk{\c{c}}e Eğitimi B{\"{o}}l{\"{u}}m{\"{u}} Sosyal Bilgiler Eğitimi Anabilim Dalı, MERSİN},
author = {Kalelioğlu, Uğur Berk},
booktitle = {Uluslararası Sosyal Bilgilerde Yeni Yaklaşımlar Dergisi},
doi = {10.38015/sbyy.1003103},
file = {:Users/hikaruasano/Documents/mendeley/Kalelioğlu{\_}2021{\_}Replicability 21st Century Crisis of the Positivist Social Sciences(2).pdf:pdf},
number = {2},
pages = {400--425},
publisher = {Hakan AKDAĞ},
title = {{Replicability: 21st Century Crisis of the Positivist Social Sciences}},
volume = {5},
year = {2021}
}
@inproceedings{raileanu2018modeling,
author = {Raileanu, Roberta and Denton, Emily and Szlam, Arthur and Fergus, Rob},
booktitle = {International conference on machine learning},
file = {:Users/hikaruasano/Documents/mendeley//Chu, Chinchali, Katti{\_}2020{\_}Multi-agent Reinforcement Learning for Networked System Control.pdf:pdf},
organization = {PMLR},
pages = {4257--4266},
title = {{Modeling others using oneself in multi-agent reinforcement learning}},
year = {2018}
}
@article{Kita2005,
author = {Kita, Hayato and Cincotti, Alessandro and Iida, Hiroyuki},
issn = {09196072},
journal = {IPSJ SIG Technical Report},
number = {87},
pages = {71--77},
publisher = {IPSJ},
title = {{TheoreticaI Value Prediction in Game-Playing}},
url = {http://ci.nii.ac.jp/naid/110002702349/ja/},
volume = {2005},
year = {2005}
}
@article{10.2307/2291016,
abstract = {Bayesian decision-theoretic designs for a clinical trial comparing two treatments for a disease with binary outcomes are developed and evaluated. The probability of successful outcome with treatment i is denoted by pi, i = 1, 2, and prior knowledge regarding each pi is assumed to follow a beta distribution. The pi are assumed to be independent. To facilitate comparison with frequentist clinical trial designs, we take a hypothesis-testing approach. The null hypothesis is {\$}\delta{\textless} \delta{\_}0{\$}, and the alternative hypothesis is {\$}\delta{\textgreater} 0{\$}, where 隆0 is the minimum treatment effect sought by the trial and 隆 = p2 - p1 is the true treatment difference. We use a simple terminal loss function reflecting the hypothesis-testing goal of the trial, and the total cost of the trial is the final sample size plus the terminal loss function. The stopping and decision rules that minimize the expectation of the total cost are determined by backward induction. Monte Carlo simulation is used to compare Bayesian and frequentist error rates and mean sample sizes of these Bayesian designs with one-tailed classical group-sequential designs of Pocock and O'Brien-Fleming. As expected, the Bayesian decision-theoretic designs have smaller mean costs than the classical designs. More surprising, when the magnitude of the terminal loss function is chosen to yield frequentist error rates similar to those for classical designs, the mean sample sizes of the Bayesian designs are usually smaller.},
author = {Lewis, Roger J and Berry, Donald A},
issn = {01621459},
journal = {Journal of the American Statistical Association},
number = {428},
pages = {1528--1534},
publisher = {[American Statistical Association, Taylor {\&} Francis, Ltd.]},
title = {{Group Sequential Clinical Trials: A Classical Evaluation of Bayesian Decision-Theoretic Designs}},
url = {http://www.jstor.org/stable/2291016},
volume = {89},
year = {1994}
}
@book{BC02964116,
author = {陽一郎, 三宅},
pages = {Kindle の位置No.462--466},
publisher = {ビー・エヌ・エヌ新社},
title = {{人工知能のための哲学塾 Kindle 版}},
url = {https://ci.nii.ac.jp/ncid/BC02964116},
year = {2018}
}
@inproceedings{pmlr-v162-ghai22a,
abstract = {We study the problem of multi-agent control of a dynamical system with known dynamics and adversarial disturbances. Our study focuses on optimal control without centralized precomputed policies, but rather with adaptive control policies for the different agents that are only equipped with a stabilizing controller. We give a reduction from any (standard) regret minimizing control method to a distributed algorithm. The reduction guarantees that the resulting distributed algorithm has low regret relative to the optimal precomputed joint policy. Our methodology involves generalizing online convex optimization to a multi-agent setting and applying recent tools from nonstochastic control derived for a single agent. We empirically evaluate our method on a model of an overactuated aircraft. We show that the distributed method is robust to failure and to adversarial perturbations in the dynamics.},
author = {Ghai, Udaya and Madhushani, Udari and Leonard, Naomi and Hazan, Elad},
booktitle = {Proceedings of the 39th International Conference on Machine Learning},
editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
file = {:Users/hikaruasano/Documents/mendeley/Ghai et al.{\_}2022{\_}A Regret Minimization Approach to Multi-Agent Control.pdf:pdf},
pages = {7422--7434},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{A Regret Minimization Approach to Multi-Agent Control}},
url = {https://proceedings.mlr.press/v162/ghai22a.html},
volume = {162},
year = {2022}
}
@book{WorldEconomicForum.Marsh&McLennanCompanies.2019,
author = {Companies, Mclennan and Group, Zurich Insurance},
file = {:Users/hikaruasano/Documents/mendeley/Companies, Group{\_}2019{\_}The Global Risks Report 2019 14th Edition.pdf:pdf},
isbn = {9781944835156},
title = {{The Global Risks Report 2019 14th Edition}},
year = {2019}
}
@misc{日本の新たな国際5:online,
annote = {(Accessed on 01/31/2022)},
author = {資源エネルギー庁},
howpublished = {https://www.enecho.meti.go.jp/about/special/johoteikyo/kokusaisigensenryaku{\_}03.html，1月31日参照},
title = {日本の新たな国際資源戦略 ③レアメタルを戦略的に確保するために},
year = {2020}
}
@misc{足立区の町丁別の91:online,
annote = {(Accessed on 06/28/2020)},
author = {足立区区民部戸籍住民課},
howpublished = {https://www.city.adachi.tokyo.jp/koseki/ku/aramashi/toke-machi-r020601.html},
title = {足立区の町丁別の世帯と人口令和2年6月1日現在｜足立区}
}
@inproceedings{girgis2022latent,
abstract = {Robust multi-agent trajectory prediction is essential for the safe control of robotic systems. A major challenge is to efficiently learn a representation that approxi- mates the true joint distribution of contextual, social, and temporal information to enable planning. We propose Latent Variable Sequential Set Transformers which are encoder-decoder architectures that generate scene-consistent multi-agent tra- jectories. We refer to these architectures as “AutoBots”. The encoder is a stack of interleaved temporal and social multi-head self-attention (MHSA) modules which alternately perform equivariant processing across the temporal and social dimensions. The decoder employs learnable seed parameters in combination with temporal and social MHSA modules allowing it to perform inference over the entire future scene in a single forward pass efficiently. AutoBots can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. For the single-agent prediction case, our model achieves top results on the global nuScenes vehicle motion prediction leaderboard, and produces strong results on the Argoverse vehicle prediction challenge. In the multi-agent setting, we evaluate on the synthetic partition of TrajNet++ dataset to showcase the model's socially-consistent predictions. We also demonstrate our model on general sequences of sets and provide illustrative experiments modelling the se- quential structure of the multiple strokes that make up symbols in the Omniglot data. A distinguishing feature of AutoBots is that all models are trainable on a single desktop GPU (1080 Ti) in under 48h.},
author = {Girgis, Roger and Golemo, Florian and Codevilla, Felipe and Weiss, Martin and D'Souza, Jim Aldon and Kahou, Samira Ebrahimi and Heide, Felix and Pal, Christopher},
booktitle = {International Conference on Learning Representations},
file = {:Users/hikaruasano/Documents/mendeley/Girgis et al.{\_}2022{\_}Latent Variable Sequential Set Transformers for Joint Multi-Agent Motion Prediction.pdf:pdf},
title = {{Latent Variable Sequential Set Transformers for Joint Multi-Agent Motion Prediction}},
url = {https://openreview.net/forum?id=Dup{\_}dDqkZC5},
year = {2022}
}
@misc{Auditing19:online,
annote = {(Accessed on 01/31/2021)},
author = {Review, Harvard Business},
howpublished = {https://hbr.org/2018/10/auditing-algorithms-for-bias},
title = {{Auditing Algorithms for Bias}},
year = {2018}
}
@inproceedings{moyer2018invariant,
author = {Moyer, Daniel and Gao, Shuyang and Brekelmans, Rob and Galstyan, Aram and {Ver Steeg}, Greg},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/hikaruasano/Documents/mendeley/Moyer et al.{\_}2018{\_}Invariant representations without adversarial training.pdf:pdf},
title = {{Invariant representations without adversarial training}},
url = {https://arxiv.org/abs/1805.09458},
year = {2018}
}
@inproceedings{10.5555/3524938.3525764,
abstract = {As the operations of autonomous systems generally affect simultaneously several users, it is crucial that their designs account for fairness considerations. In contrast to standard (deep) reinforcement learning (RL), we investigate the problem of learning a policy that treats its users equitably. In this paper, we formulate this novel RL problem, in which an objective function, which encodes a notion of fairness that we formally define, is optimized. For this problem, we provide a theoretical discussion where we examine the case of discounted rewards and that of average rewards. During this analysis, we notably derive a new result in the standard RL setting, which is of independent interest: it states a novel bound on the approximation error with respect to the optimal average reward of that of a policy optimal for the discounted reward. Since learning with discounted rewards is generally easier, this discussion further justifies finding a fair policy for the average reward by learning a fair policy for the discounted reward. Thus, we describe how several classic deep RL algorithms can be adapted to our fair optimization problem, and we validate our approach with extensive experiments in three different domains.},
author = {Siddique, Umer and Weng, Paul and Zimmer, Matthieu},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Siddique, Weng, Zimmer{\_}2020{\_}Learning Fair Policies in Multiobjective (Deep) Reinforcement Learning with Average and Discounted Rewards.pdf:pdf},
publisher = {JMLR.org},
series = {ICML'20},
title = {{Learning Fair Policies in Multiobjective (Deep) Reinforcement Learning with Average and Discounted Rewards}},
year = {2020}
}
@inproceedings{DBLP:conf/icml/IglZLWW18,
author = {Igl, Maximilian and Zintgraf, Luisa M and Le, Tuan Anh and Wood, Frank and Whiteson, Shimon},
booktitle = {ICML},
file = {:Users/hikaruasano/Documents/mendeley/Igl et al.{\_}2018{\_}Deep Variational Reinforcement Learning for POMDPs.pdf:pdf},
pages = {2122--2131},
title = {{Deep Variational Reinforcement Learning for POMDPs}},
url = {http://proceedings.mlr.press/v80/igl18a.html},
year = {2018}
}
@article{Jevons1981,
author = {Jevons, William Stanley; 寺尾, 琢磨; 小泉, 信三; 永田, 清},
isbn = {4818803472},
number = {4},
publisher = {日本経済評論社},
title = {経済学の理論},
url = {http://opac.dl.itc.u-tokyo.ac.jp/opac/opac{\_}link/bibid/2000018042},
year = {1981}
}
@article{1050564287755107584,
author = {慶田, 收},
file = {:Users/hikaruasano/Documents/mendeley/慶田{\_}2015{\_}完全ポテンシャルゲームとしての正規形ゲームと混雑ゲームの性質 (経済学部再編記念号).pdf:pdf},
issn = {1341-0202},
journal = {熊本学園大学経済論集 = Journal of Economics, Kumamoto Gakuen University},
number = {1-4},
pages = {231--260},
publisher = {熊本学園大学経済学会},
title = {完全ポテンシャルゲームとしての正規形ゲームと混雑ゲームの性質 (経済学部再編記念号)},
url = {https://cir.nii.ac.jp/crid/1050564287755107584},
volume = {21},
year = {2015}
}
@article{anderson2012multilevel,
author = {Anderson, David F and Higham, Desmond J},
file = {:Users/hikaruasano/Documents/mendeley/Anderson, Higham{\_}2012{\_}Multilevel Monte Carlo for continuous time Markov chains, with applications in biochemical kinetics.pdf:pdf},
journal = {Multiscale Modeling {\&} Simulation},
number = {1},
pages = {146--179},
publisher = {SIAM},
title = {{Multilevel Monte Carlo for continuous time Markov chains, with applications in biochemical kinetics}},
volume = {10},
year = {2012}
}
@book{BC04129608,
author = {Criado-Perez, Caroline and 朗子, 神崎},
publisher = {河出書房新社},
title = {存在しない女たち : 男性優位の世界にひそむ見せかけのファクトを暴く},
url = {https://ci.nii.ac.jp/ncid/BC04129608},
year = {2020}
}
@inproceedings{jeon2022maser,
author = {Jeon, Jeewon and Kim, Woojun and Jung, Whiyoung and Sung, Youngchul},
booktitle = {International Conference on Machine Learning},
file = {:Users/hikaruasano/Documents/mendeley/Jeon et al.{\_}2022{\_}Maser Multi-agent reinforcement learning with subgoals generated from experience replay buffer.pdf:pdf},
organization = {PMLR},
pages = {10041--10052},
title = {{Maser: Multi-agent reinforcement learning with subgoals generated from experience replay buffer}},
year = {2022}
}
@inproceedings{pmlr-v119-perdomo20a,
abstract = {When predictions support decisions they may influence the outcome they aim to predict. We call such predictions performative; the prediction influences the target. Performativity is a well-studied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining. We develop a risk minimization framework for performative prediction bringing together concepts from statistics, game theory, and causality. A conceptual novelty is an equilibrium notion we call performative stability. Performative stability implies that the predictions are calibrated not against past outcomes, but against the future outcomes that manifest from acting on the prediction. Our main results are necessary and sufficient conditions for the convergence of retraining to a performatively stable point of nearly minimal loss. In full generality, performative prediction strictly subsumes the setting known as strategic classification. We thus also give the first sufficient conditions for retraining to overcome strategic feedback effects.},
author = {Perdomo, Juan and Zrnic, Tijana and Mendler-D{\"{u}}nner, Celestine and Hardt, Moritz},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
editor = {III, Hal Daum{\'{e}} and Singh, Aarti},
file = {:Users/hikaruasano/Documents/mendeley/Perdomo et al.{\_}2020{\_}Performative Prediction.pdf:pdf},
pages = {7599--7609},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Performative Prediction}},
url = {https://proceedings.mlr.press/v119/perdomo20a.html},
volume = {119},
year = {2020}
}
@article{BORRMANN201568,
abstract = {Multi-agent robotics involves the coordination of large numbers of robots, which leads to significant challenges in terms of collision avoidance. This paper generates provably collision free swarm behaviours by constructing swarm safety control barrier certificates. The safety barrier, implemented via an optimization-based controller, serves as a low level safety controller formally ensuring the forward invariance of the safe operating set. In addition, the proposed method naturally combines the goals of collision avoidance and interference with the coordination laws in a unified and computationally efficient manner. The centralized version of safety barrier certificate is designed on double integrator dynamic model, and then a decentralized formulation is proposed as a less computationally intensive and more scalable solution. The safety barrier certificate is validated in simulation and implemented experimentally on multiple mobile robots; the proposed optimization-based controller successfully generates collision free control commands with minimal overall impact on the coordination control laws.},
annote = {Analysis and Design of Hybrid Systems ADHS},
author = {Borrmann, Urs and Wang, Li and Ames, Aaron D and Egerstedt, Magnus},
doi = {https://doi.org/10.1016/j.ifacol.2015.11.154},
file = {:Users/hikaruasano/Documents/mendeley/Borrmann et al.{\_}2015{\_}Control Barrier Certificates for Safe Swarm Behavior(3).pdf:pdf},
issn = {2405-8963},
journal = {IFAC-PapersOnLine},
number = {27},
pages = {68--73},
title = {{Control Barrier Certificates for Safe Swarm Behavior}},
url = {https://www.sciencedirect.com/science/article/pii/S240589631502412X},
volume = {48},
year = {2015}
}
@article{shoup2006cruising,
author = {Shoup, Donald C},
journal = {Transport Policy},
number = {6},
pages = {479--486},
publisher = {Elsevier},
title = {{Cruising for parking}},
volume = {13},
year = {2006}
}
@misc{翼の負圧を利用し36:online,
annote = {(Accessed on 06/15/2021)},
author = {NEDO},
howpublished = {https://www.nedo.go.jp/news/press/AA5{\_}0214A.html},
title = {翼の負圧を利用して船底で気泡を発生、摩擦抵抗を低減し、燃費を約10％向上},
year = {2009}
}
@inproceedings{9561195,
author = {Li, Jinning and Sun, Liting and Chen, Jianyu and Tomizuka, Masayoshi and Zhan, Wei},
booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA48506.2021.9561195},
file = {:Users/hikaruasano/Documents/mendeley/Li et al.{\_}2021{\_}A Safe Hierarchical Planning Framework for Complex Driving Scenarios based on Reinforcement Learning.pdf:pdf},
pages = {2660--2666},
title = {{A Safe Hierarchical Planning Framework for Complex Driving Scenarios based on Reinforcement Learning}},
year = {2021}
}
@article{Ades2004ExpectedVO,
author = {Ades, A and Lu, G and Claxton, K},
file = {:Users/hikaruasano/Documents/mendeley/Ades, Lu, Claxton{\_}2004{\_}Expected Value of Sample Information Calculations in Medical Decision Modeling.pdf:pdf},
journal = {Medical Decision Making},
pages = {207--227},
title = {{Expected Value of Sample Information Calculations in Medical Decision Modeling}},
volume = {24},
year = {2004}
}
@inproceedings{gupta2017cooperative,
author = {Gupta, Jayesh K and Egorov, Maxim and Kochenderfer, Mykel},
booktitle = {International Conference on Autonomous Agents and Multiagent Systems},
file = {:Users/hikaruasano/Documents/mendeley/Gupta, Egorov, Kochenderfer{\_}2017{\_}Cooperative multi-agent control using deep reinforcement learning.pdf:pdf},
organization = {Springer},
pages = {66--83},
title = {{Cooperative multi-agent control using deep reinforcement learning}},
year = {2017}
}
@techreport{Fu,
abstract = {Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly out-performs prior methods in these transfer settings.},
author = {Fu, Justin and Luo, Katie and Levine, Sergey},
file = {:Users/hikaruasano/Documents/mendeley/Fu, Luo, Levine{\_}Unknown{\_}LEARNING ROBUST REWARDS WITH ADVERSARIAL INVERSE REINFORCEMENT LEARNING.pdf:pdf},
title = {{LEARNING ROBUST REWARDS WITH ADVERSARIAL INVERSE REINFORCEMENT LEARNING}}
}
@article{Binois2019ReplicationOE,
author = {Binois, M and Huang, Jiangeng and Gramacy, R and Ludkovski, M},
journal = {Technometrics},
pages = {23 -- 7},
title = {{Replication or Exploration? Sequential Design for Stochastic Simulation Experiments}},
volume = {61},
year = {2019}
}
@inproceedings{wai2018multi,
author = {Wai, Hoi-To and Yang, Zhuoran and Wang, Zhaoran and Hong, Mingyi},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/hikaruasano/Documents/mendeley/Wai et al.{\_}2018{\_}Multi-agent reinforcement learning via double averaging primal-dual optimization.pdf:pdf},
pages = {9649--9660},
title = {{Multi-agent reinforcement learning via double averaging primal-dual optimization}},
year = {2018}
}
@article{efroni2020exploration,
author = {Efroni, Yonathan and Mannor, Shie and Pirotta, Matteo},
file = {:Users/hikaruasano/Documents/mendeley/Efroni, Mannor, Pirotta{\_}2020{\_}Exploration-exploitation in constrained mdps.pdf:pdf},
journal = {arXiv preprint arXiv:2003.02189},
title = {{Exploration-exploitation in constrained mdps}},
year = {2020}
}
@misc{Hydrogen84:online,
annote = {(Accessed on 07/18/2020)},
author = {Economist, The},
howpublished = {https://www.economist.com/science-and-technology/2020/07/04/after-many-false-starts-hydrogen-power-might-now-bear-fruit},
title = {{Hydrogen power - After many false starts, hydrogen power might now bear fruit | Science {\&} technology}},
year = {2020}
}
@inproceedings{9053534,
author = {Jin, Yue and Wei, Shuangqing and Yuan, Jian and Zhang, Xudong and Wang, Chao},
booktitle = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP40776.2020.9053534},
file = {:Users/hikaruasano/Documents/mendeley/Jin et al.{\_}2020{\_}Stabilizing Multi-Agent Deep Reinforcement Learning by Implicitly Estimating Other Agents' Behaviors.pdf:pdf},
pages = {3547--3551},
title = {{Stabilizing Multi-Agent Deep Reinforcement Learning by Implicitly Estimating Other Agents' Behaviors}},
year = {2020}
}
@book{BB10483692p172,
author = {千鶴子, 上野},
publisher = {岩波書店},
series = {岩波現代文庫},
title = {ナショナリズムとジェンダー},
url = {https://ci.nii.ac.jp/ncid/BB10483692}
}
@article{2012,
abstract = {• גרינבלט, י. (2012). ענף הקיווי: תמונת מצב 2012, עלון הנוטע שה“מ, מחוז צפון, משרד החקלאות.},
author = {גרינבלט, יעל},
file = {:Users/hikaruasano/Documents/mendeley/.pdf:pdf},
journal = {調査研究},
pages = {37--39},
title = {{ロジャーズによる「共感的理解」の記述の検討 一「知覚」から「感じる」、「内的照合枠Jから「私的な世界Jへ一 An investigation of Rogers' descriptions about “empathic understanding"一世om “perceive" to“sense"， and合om“intemalframe of reference" to“private world". 小林孝雄・ KOBAYASHI Takao キーワード共感的理解共感ロジャーズ内}},
volume = {66},
year = {2012}
}
@inproceedings{rana2020residual,
author = {Rana, Krishan and Talbot, Ben and Dasagi, Vibhavari and Milford, Michael and S{\"{u}}nderhauf, Niko},
booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
file = {:Users/hikaruasano/Documents/mendeley/Rana et al.{\_}2020{\_}Residual reactive navigation Combining classical and learned navigation strategies for deployment in unknown environmen.pdf:pdf},
organization = {IEEE},
pages = {11493--11499},
title = {{Residual reactive navigation: Combining classical and learned navigation strategies for deployment in unknown environments}},
year = {2020}
}
@book{/content/publication/9789264095298-en,
author = {OECD},
doi = {https://doi.org/https://doi.org/10.1787/9789264095298-en},
pages = {102},
title = {{PISA 2009 at a Glance}},
url = {https://www.oecd-ilibrary.org/content/publication/9789264095298-en},
year = {2010}
}
@book{1130282268914888576,
author = {久保川, 達也},
publisher = {共立出版},
series = {共立講座 数学の魅力},
title = {現代数理統計学の基礎},
url = {https://cir.nii.ac.jp/crid/1130282268914888576},
year = {2017}
}
@misc{keikakuh35:online,
annote = {(Accessed on 06/28/2020)},
author = {さいたま市環境局環境共生部},
howpublished = {https://www.city.saitama.jp/001/007/007/p058293{\_}d/fil/keikaku{\_}honpen.pdf},
title = {さいたま市空き家等対策計画},
year = {2015}
}
@article{pukelsheim1991optimal,
author = {Pukelsheim, Friedrich and Torsney, Ben},
journal = {The annals of Statistics},
pages = {1614--1625},
publisher = {JSTOR},
title = {{Optimal weights for experimental designs on linearly independent support points}},
year = {1991}
}
@article{woo2017well,
author = {Woo, JongRoul and Choi, Hyunhong and Ahn, Joongha},
journal = {Transportation Research Part D: Transport and Environment},
pages = {340--350},
publisher = {Elsevier},
title = {{Well-to-wheel analysis of greenhouse gas emissions for electric vehicles based on electricity generation mix: A global perspective}},
volume = {51},
year = {2017}
}
@inproceedings{pmlr-v80-yona18a,
abstract = {The seminal work of Dwork {\textless}em{\textgreater}et al.{\textless}/em{\textgreater} [ITCS 2012] introduced a metric-based notion of individual fairness: given a task-specific similarity metric, their notion required that every pair of similar individuals should be treated similarly. In the context of machine learning, however, individual fairness does not generalize from a training set to the underlying population. We show that this can lead to computational intractability even for simple fair-learning tasks. With this motivation in mind, we introduce and study a relaxed notion of {\textless}em{\textgreater}approximate metric-fairness{\textless}/em{\textgreater}: for a random pair of individuals sampled from the population, with all but a small probability of error, if they are similar then they should be treated similarly. We formalize the goal of achieving approximate metric-fairness simultaneously with best-possible accuracy as Probably Approximately Correct and Fair (PACF) Learning. We show that approximate metric-fairness {\textless}em{\textgreater}does{\textless}/em{\textgreater} generalize, and leverage these generalization guarantees to construct polynomial-time PACF learning algorithms for the classes of linear and logistic predictors.},
author = {Yona, Gal and Rothblum, Guy},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
editor = {Dy, Jennifer and Krause, Andreas},
pages = {5680--5688},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Probably Approximately Metric-Fair Learning}},
url = {https://proceedings.mlr.press/v80/yona18a.html},
volume = {80},
year = {2018}
}
